<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jiri Stodulka">

  
  
  
    
  
  <meta name="description" content="Canada recently joined few other countries that have completely decriminalized cannabis consumption, hence making it entirely legal. While many argued the Country has infringed a number of international agreements, others sang chorales. Whatever your attitude towards legalization of cannabis may be, there are one set of people that are definitely happy and they are the “Canadian Cannabis consumers”. However, only provincial, i.e. governmental, legal entities are allowed to sell and distribute marihuana products until April 2019.">

  
  <link rel="alternate" hreflang="en-us" href="/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/solarized-dark.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/solarized-dark.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.a898eb080fbf0112b89e87392c6ca457.css">

  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@jiristo">
  <meta property="twitter:creator" content="@jiristo">
  
  <meta property="og:site_name" content="Jiri Stodulka">
  <meta property="og:url" content="/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/">
  <meta property="og:title" content="Web Scraping Reddit: Text (Sentiment) Analysis | Jiri Stodulka">
  <meta property="og:description" content="Canada recently joined few other countries that have completely decriminalized cannabis consumption, hence making it entirely legal. While many argued the Country has infringed a number of international agreements, others sang chorales. Whatever your attitude towards legalization of cannabis may be, there are one set of people that are definitely happy and they are the “Canadian Cannabis consumers”. However, only provincial, i.e. governmental, legal entities are allowed to sell and distribute marihuana products until April 2019."><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-03-27T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-03-27T00:00:00&#43;00:00">
  

  


  





  <title>Web Scraping Reddit: Text (Sentiment) Analysis | Jiri Stodulka</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Jiri Stodulka</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#post_projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#project_posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#public"><span>Public</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>Resume</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Web Scraping Reddit: Text (Sentiment) Analysis</h1>

  

  
    



<meta content="2019-03-27 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2019-03-27 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Mar 27, 2019</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    18 min read
  </span>
  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/r/">R</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/&amp;text=Web%20Scraping%20Reddit:%20Text%20%28Sentiment%29%20Analysis" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/&amp;t=Web%20Scraping%20Reddit:%20Text%20%28Sentiment%29%20Analysis" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Web%20Scraping%20Reddit:%20Text%20%28Sentiment%29%20Analysis&amp;body=/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/&amp;title=Web%20Scraping%20Reddit:%20Text%20%28Sentiment%29%20Analysis" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Web%20Scraping%20Reddit:%20Text%20%28Sentiment%29%20Analysis%20/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/&amp;title=Web%20Scraping%20Reddit:%20Text%20%28Sentiment%29%20Analysis" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<link href="/rmarkdown-libs/wordcloud2/wordcloud.css" rel="stylesheet" />
<script src="/rmarkdown-libs/wordcloud2/wordcloud2-all.js"></script>
<script src="/rmarkdown-libs/wordcloud2/hover.js"></script>
<script src="/rmarkdown-libs/wordcloud2-binding/wordcloud2.js"></script>


<p>Canada recently joined few other countries that have completely decriminalized cannabis consumption, hence making it entirely legal. While many argued the Country has infringed a number of international agreements, others sang chorales. Whatever your attitude towards legalization of cannabis may be, there are one set of people that are definitely happy and they are the “Canadian Cannabis consumers”. However, only provincial, i.e. governmental, legal entities are allowed to sell and distribute marihuana products until April 2019. Marihuana consumers may not break the law while smoking or passing a joint any more. What were the sentiments tied to people in the early days of Cannabis legalization and when the consumers were allowed to make online orders? Did the new policies meet the users’ expectations? I am sure, some future and official studies will answer the question very soon. However, I attempted to find the answer on my own. In this post, I will explain how I scraped one of the biggest website forums - Reddit. Additionally, I will demonstrate how I performed a simple sentiment analysis on a tidy dataset I had created.</p>
<p>GitHub Repository:</p>
<pre><code>git clone https://github.com/jiristo/webscraping_inspectplot.git</code></pre>
<div id="reading-html-code" class="section level1">
<h1>Reading HTML Code</h1>
<p>Reddit is a static website. At the time, when I was scraping the forum, I did not know about its public API. However, the main purpose of my effort was to i.) learn web scraping and create my own data set, and ii.) understand web structure. To conduct the analysis, I was mostly interested in pure text, i.e. reviews, and their authors. Any contributor can also assign points (in the form of likes or dislikes) to the most appealing comments posted by other users. Each post also displays a time frame when the comment was posted. <strong>rvest</strong> library, developed by Hadley Wickham, is powerful enough to secure any accessible data within an HTML. Moreover, the package supports the syntax of tidyverse.</p>
<p>First of all, I connected to the webpage.</p>
<pre class="r"><code>url &lt;- (&quot;https://old.reddit.com/r/canadients/comments/9ovapz/legal_cannabis_in_canada_megathread/?limit=500&quot;)</code></pre>
<p>I struggled a little while selecting all the nodes with comments and relevant information (my variables). However, you need to read HTML code, <code>call read_html()</code>, only once. Basically, every comment on Reddit is displayed in a “bubble”. And most importantly, the “bubbles” contain all the required information! Being new to HTML and web scraping, I experienced difficulties to specify the correct arguments inside html_nodes(). Examining HTML code and conducting some research, I realized I had to use CSS selectors to style the elements in HTML. To my understanding <code>.</code> as the argument in quotation marks means: “select classes so <code>.entry</code> selector selects all the objects with the entry class. The output assigned to reviews is a list (of nodes) of length 484. Again, every node contains relevant information about each comment. Therefore, I called <code>read_html()</code> and <code>html_nodes()</code> only once.</p>
<pre class="r"><code># library(rvest)

reviews &lt;- url %&gt;%
  read_html() %&gt;%
  html_nodes(&#39;.entry&#39;)</code></pre>
<div id="generating-the-variables" class="section level6">
<h6>Generating the Variables</h6>
<p>I started with the authors. <code>html_node(&quot;.author&quot;)</code> selects all the objects within the author class from <code>reviews</code>. The output is a vector of a length of the <code>reviews</code>. Each element in the list is a node with the class <code>author</code>. For example: <code>&lt;a href=&quot;https://old.reddit.com/user/Hendrix194&quot; class=&quot;author may-blank id-t2_n4hdv&quot;&gt;Hendrix194&lt;/a&gt;</code>. This would be useless unless you call other functions from <strong>rvest</strong>, i.e. <code>html_text()</code>. It extracts the selector content that is, in this case, the author’s name: <strong>Hendrix194</strong>!</p>
</div>
<div id="author" class="section level6">
<h6>Author</h6>
<pre class="r"><code>author &lt;- reviews %&gt;%
  html_node(&quot;.author&quot;) %&gt;%
  html_text()%&gt;%
  str_trim() 

author &lt;- as.factor(author)</code></pre>
<p>After obtaining and examining the value <strong>author</strong>, I found several <code>[deleted]</code> values. Initially, I thought these authors and their comments were deleted. However, for whatever reason, the comments were still visible. Therefore, I decided to not dispose them. Finally, I stored the vector as a factor because each author is an individual entity.</p>
</div>
<div id="comment" class="section level6">
<h6>Comment</h6>
<p>Naturally, each comment is the most important variable for my analysis. The approach was identical to scraping <code>author</code>. Once again, the function selects all the objects with the class. In addition to solely extracting the selector content by <code>html_text()</code>, I specified an additional argument <code>trim = TRUE</code>. <code>trim</code> eliminates any space character (which is invisible to human eye) before and after a string. Additionally, I dispose of newline separators <code>\n</code> by calling the <code>gsub()</code> function and specifying the pattern as the function’s argument.</p>
<pre class="r"><code>comment &lt;- reviews %&gt;%
  html_node(&quot;.md&quot;) %&gt;%
  html_text(trim = TRUE ) %&gt;%
  gsub(&quot;\n&quot;,&quot;&quot;,.)</code></pre>
<p>After I extracted the content of <code>.score</code>, I ended up with a string vector where each value was composed of two words. Additionally, the first word was supposed to be numeric, e.g. <code>&quot;3 points&quot;</code>. To get it, I simply supplied 1; i.e. specified the position of my word of interest, as the argument inside <code>stringr::word()</code> and piped the result into <code>as.integer()</code>.</p>
<pre class="r"><code># library(stringr)

likes &lt;- reviews %&gt;%
  html_node(&quot;.score&quot;) %&gt;%
  html_text() %&gt;%
  word(1) %&gt;%
  as.integer() </code></pre>
<p>Initially, I thought I would only be able to scrape the time variable as it is displayed on Reddit, e.g. <code>&quot;3 months ago&quot;</code>. Fortunately, such format is the output of <code>java</code>, which together with the <code>HTML</code> and <code>CSS</code>, makes any website’s content readable to a human. Note, that I also did not extract selector’s content as I did with the previous variables.</p>
<p>Finally, I formatted <code>time</code> with the base <code>striptime()</code> function and transformed it by <code>ymd_hms()</code> into POSIXct. Such formatting may have major benefits when analyzing sentiments in units of minutes, seconds but also at lower frequencies!</p>
<pre class="r"><code># lubridate
date &lt;- reviews %&gt;%
  html_node(&quot;time&quot;)%&gt;%
  html_attr(&quot;title&quot;)%&gt;%
  strptime(format = &quot;%a %b %d %H:%M:%S %Y&quot;,tz = &quot;UTC&quot;)%&gt;%
  ymd_hms()</code></pre>
</div>
<div id="data-frames" class="section level6">
<h6>Data Frames</h6>
<p>I ended up with 4 vectors. Firstly, I merged all of them into one data frame <code>dataset</code>. I also called <code>filter</code> and <code>mutate</code> to simultaneously clean the data a little bit and create <code>id</code> - a unique value to every comment. <code>dataset</code> is my entry level data frame for two additional tidy datasets. Additionally, it is easy to search for any <code>id</code> and examine the associated comment.</p>
<pre class="r"><code># Data frame from vectors
dataset &lt;- data.frame(author, likes, comment, date, stringsAsFactors = FALSE) %&gt;%
  filter(!is.na(author))%&gt;%
  # Filtering comments of few words
  filter(str_count(comment)&gt;=5) %&gt;%
  # creating ID for each comment so I can refer to it later
  mutate(id = row_number())</code></pre>
<p>Secondly, it was necessary to clean <code>dataset$comment</code>. Specifically, any stop words, single and meaningless characters, and numbers that are redundant for the majority of text analysis. My goal was to create a new <strong>tidy data frame</strong> where each observation would be a single word from the comment it appears in. I achieved that through <code>tidytext::unnest_tokens()</code>, <code>tidytext::stop_words</code>, <code>dplyr::anti_join()</code>. <code>unnest_token()</code> takes three arguments: i.) dataset, ii.) new variable (<code>word</code>), and iii.) name of a string column (<code>comment</code>). The output is the tidy data frame where number of rows per entity (<code>comment</code> or <code>id</code>) is equal to number of words in <code>comment</code>. It is also worthy of mention that the function sets all the letters to lower case by default. I also called <code>anti_join()</code> three times to filter: i.) stop words (<code>stop_words</code> is a vector of the stop words from <code>tidytext</code>), ii.) URL links (<code>url_words</code>), iii.) and numeric strings. Note that the argument in <code>anti_join()</code> must not be a vector! Finally, I filtered any words of lower length or equal to 3 characters.</p>
<pre class="r"><code>#tidytext
url_words &lt;- tibble(
  word = c(&quot;https&quot;,&quot;http&quot;)) #filternig weblinks

tidy_data &lt;- dataset %&gt;% unnest_tokens(word, comment) %&gt;% anti_join(stop_words, by=&quot;word&quot;) %&gt;%  anti_join(url_words, by=&quot;word&quot;) %&gt;% 
# filtering &quot;numeric&quot; words  
anti_join(tibble(word = as.character (c(1:10000000))), by=&quot;word&quot;) %&gt;%
# stop word already filter but there were still redundent words, e.g. nchar(word) &lt; 3
  filter(nchar(word)&gt;=3) 

rm(url_words)</code></pre>
<p>Lastly, I created <code>data_sentiment</code> data frame from <code>tidy_data.</code> I achieved that by filtering and joining the data from <code>nrc</code> lexicon. I had chosen <code>nrc</code> because it contains a variety of emotions associated with the word. I also created <code>author_total</code>, i.e. how many words in total has each author posted to the forum. <code>inner_join(get_sentiments(&quot;nrc&quot;), by=&quot;word&quot;)</code> joins <code>nrc</code> lexicon with sentiments to <code>tidy_data</code>. Note that <code>inner_join()</code> filters out any words out of the intersection of any two data sets.</p>
<pre class="r"><code>data_sentiment &lt;- tidy_data %&gt;% 
    # Group by author
    group_by(author) %&gt;% 
    # Define a new column author_total; i.e. how many words an author has posted
    mutate(author_total=n()) %&gt;%
    ungroup() %&gt;%
    # Implement sentiment analysis with the NRC lexicon
    inner_join(get_sentiments(&quot;nrc&quot;), by=&quot;word&quot;)</code></pre>
<p>These steps above summarize my preprocessing and data cleaning strategies. Having three different data sets, there was nothing else impeding the analysis. From this step, it was easy to explore and analyze the data more precisely.</p>
</div>
<div id="exploratory-data-analysis-eda" class="section level6">
<h6>Exploratory Data Analysis (EDA)</h6>
<p>Collecting any time varying variable, one should always know the time span.</p>
<pre class="r"><code>range(dataset$date) #time span</code></pre>
<pre><code>## [1] &quot;2018-10-17 04:17:53 UTC&quot; &quot;2019-03-10 04:45:13 UTC&quot;</code></pre>
<pre class="r"><code>range(dataset$date)[2]-range(dataset$date)[1]</code></pre>
<pre><code>## Time difference of 144.019 days</code></pre>
<p>Interesting! The very first comment on the forum was posted on the day when legalization happened, i.e. <code>2018-10-17</code>. On the other hand, the last comment was submitted (in time of writing this post) in March 2019. This tells us the discussion has been alive for 145 days, i.e. almost 5 months. Initially, I thought that the time frame would be sufficient for interesting observations and conclusions. However, the frequency of new comments matters as well. Is there a representative number of submitted comments for each month? One way to answer the question would be a visual examination.</p>
<p>The following plot displays the count of new comments in time.</p>
<pre class="r"><code>dataset %&gt;% select(date,comment) %&gt;% 
  mutate(date = round_date(date, &quot;1 day&quot;))  %&gt;% group_by(date) %&gt;% mutate(n_comments = n()) %&gt;% 
  # filter(date &lt; ymd(&quot;2018-10-25&quot;)) %&gt;%
  # Had to round  up the date object into &quot;week&quot; units, otherwise grouping a mutating would not work   (too narrow interval)

  ggplot(aes(date,n_comments)) +
  geom_line(linetype = 1)+
  ggtitle(&quot;Number of Comments in Time&quot;)</code></pre>
<p><img src="/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Numeber%20of%20Comments%20in%20Time%20and%20PLOT-1.png" width="672" /> Unfortunately, the discussion was truly “alive” only a few weeks after October 17th. After the end of October 2018, new comments per day or even month were marginal.</p>
<p>Even though I could not answer my initial questions in the full extent, I could focus on the cannabis consumers’ initial attitude. So how many comments were posted there?</p>
<pre class="r"><code>nrow(dataset) # n comments</code></pre>
<pre><code>## [1] 460</code></pre>
<p>Besides the number of comments, and the frequency of new ones, the total of contributors to the discussion should be represented as well.</p>
<pre class="r"><code>nlevels(dataset$author) #n levels</code></pre>
<pre><code>## [1] 166</code></pre>
<p>Relative to the total number of comments, I maintain that enough authors have contributed into the discussion.</p>
</div>
<div id="text-analysis" class="section level6">
<h6>Text Analysis</h6>
<p>Did you already examine this post’s cover picture? It is the word cloud from <code>wordcloud2</code> package and it displays the most frequent words in the discussion. By default, words with the highest frequency are centered in the middle of the plot. I like word clouds because they are great tools for very first inspection of text data. You can easily infer the subject being discussed.</p>
<pre class="r"><code># library(wordcloud2)

tidy_data %&gt;% select(word) %&gt;% count(word,sort=T) %&gt;%
wordcloud2(backgroundColor = &quot;black&quot;, color = &quot;green&quot;)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="wordcloud2 html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"word":["cannabis","legal","ocs","people","time","day","weed","shipping","visa","card","market","buy","alberta","lol","ontario","strains","government","online","debit","black","credit","oil","pretty","update","canada","store","website","yeah","it’s","morning","prices","bought","hours","line","seeds","data","days","don’t","illegal","street","border","flower","home","medical","post","smoke","stuff","trade","allowed","guys","i’m","lot","shipped","wait","bank","canadian","laws","night","price","smoking","sold","stock","that’s","thinking","week","cbd","guess","happy","legally","mine","policy","pre","retail","stores","strain","address","bud","call","called","can’t","company","delivery","drive","edibles","feel","found","information","issue","locations","mastercard","ocs.ca","packaging","prepaid","province","public","purchase","read","sell","selling","ship","smoked","specifically","start","super","you’re","amount","bit","buds","cost","edit","fee","google","grey","hope","kush","law","legalization","marijuana","medreleaf","money","postal","question","rolling","sativa","service","statement","vanilla","waiting","worth","american","aurora","based","coming","considered","dont","fine","flowr","half","haze","industry","info","live","middle","montreal","news","nice","option","paid","park","pink","plants","pot","products","reason","search","security","site","thc","trading","vape","weedmd","weekend","access","actual","ago","avenue","business","buying","cards","coffee","concentrate","concentrates","cool","country","customer","doubt","edison","eigth","experience","free","fuck","ghost","gift","gram","green","grow","growing","guy","hash","helps","hotel","ill","illicit","makes","oils","page","past","phone","picture","pictures","power","pricing","processing","provinces","purchased","quality","questions","roll","saturday","seed","selection","shatter","shop","similar","smell","sort","sounds","specific","surprised","taxes","thread","times","told","tomorrow","train","travel","type","user","wondering","yesterday","3.5g","act","aged","ahead","amazing","amex","anxiety","assume","bad","ban","banking","broken","burn","calgary","canopy","changed","check","chill","code","complain","completely","container","declined","doesn’t","door","drinking","email","entry","gonna","grabbed","growth","hack","haha","hear","hey","house","indica","issues","l'acadie","left","licenced","limited","local","location","love","luck","main","major","means","minutes","oct","officers","organigram","pay","paying","person","pick","plain","police","privacy","product","quarter","quick","rafael","ready","recently","reddit","redecan","safe","san","sense","shit","sorted","sour","south","started","statements","sticking","tide","tobacco","top","trouble","ultra","vertical","walk","websites","2gs","3.5","admits","advice","age","albertacannabis.org","americans","answer","aphria","apparently","apply","arrive","banned","billing","bong","brands","bulk","campaign","cannafarms","catherine","cbp","centre","charge","christmas","cities","comment","comments","concern","concerned","continue","cops","correct","crazy","criminal","current","damn","deal","dealer","deliver","demographic","destroy","dry","dude","due","earlier","easier","eat","expect","fairly","fear","federal","figured","fixed","food","forgot","fresh","friday","frustrating","fucked","grams","grown","guards","guessing","happen","happening","hard","heads","heard","heavy","hit","hold","holds","hoping","impressed","included","isn’t","job","keeping","kids","laid","lcbo","leafs","level","license","licenses","light","linked","literally","lp's","marketing","matter","meant","medicine","mind","missed","monday","month","mortar","notification","nova","nugs","office","outrageous","overpriced","packaged","password","patriot","pen","permit","personally","picked","pics","pipe","plane","plant","posts","probable","producer","psychedelic","quebec","real","reasons","received","recommend","recommendations","records","reddit's","registered","related","rolled","royal","run","sale","scale","servers","shadows","shot","sit","slip","snoop","social","solei","spam","steal","stick","supply","supposed","talking","telling","terrible","there’s","they’re","tilray","toronto","tracking","trip","u.s","updated","version","white","won’t","word","workers","wouldn’t","yep","18th","20am","25th","31.99","4th","9.24","9.95","abcann","absurd","account","add","added","adding","afraid","aglc.ca","airports","alta","amazed","amounts","amsterdam","anymore","apps","article","ass","avoid","aware","awesome","balance","basically","basis","beans","bed","beleave","birthday","block","blvd","boat","booze","bottle","boutique","breaks","brick","bright","bring","bro","broadway","btw","buddy","bunch","burns","busy","bylaw","cancel","canna","canntrust","cap","capsules","car","carry","cash","casual","casuals","caught","chance","charges","cheaper","checked","checkout","cheers","choices","chopping","claim","clamping","clones","closer","closing","coast","commerce","companies","competition","competitive","confident","confirm","congratulations","consume","consumption","couch","countryside","couple","court.happy","cove","crack","crashed","created","cronos","crossing","dads","danksgiving","date","deals","decent","demand","didn’t","digits","disappear","dna","dollar","dollars","dream","drink","dumb","dunno","east","easy","eaton","edible","edmonton","effect","eighth","eighths","emblem","empty","encrypted","ents","error","evening","exchange","excited","exciting","existing","explicitly","export","eye","facilitate","failed","fair","family","fast","fees","feet","field","figure","fire","fireside","flexdelivery","footer","foreign","fort","front","funny","fyi","game","genetics","giving","goal","god","gotta","govt","grabbing","grove","gst","hahaha","hands","harvest","haven","haven’t","hexo","highly","hits","honest","household","huge","hydropothecary","i’ve","ice","ideally","idiots","images","imagine","imgur.com","import","individual","insulted","interim","internet","involved","irisa","jars","joint","joints","kinda","knowing","label","landed","landing","lands","larger","lazy","lbs","lead","learn","leave","legalized","legislation","licensed","life","lights","liiv","lined","lines","lineup","list","load","loaded","logo","lower","lowest","lps","lungs","macleod","markham","massive","meantime","mention","midnight","mix","model","mom","mynslc.com","nb.com","nervous","newstrike","nope","normal","notice","notices","nslc","oaks","october","offense","offer","offering","opportunity","ounce","oven","packed","pardoning","patrol","personal","physical","planned","planning","plastic","poor","possession","pound","pour","press","priced","prime","private","produce","proper","protected","purchases","purchasing","purulator","putting","quantities","quarters","québec","queue","railway","ran","randylaheyjr","reasonable","receiving","rediquette","refuse","registration","regulations","report","reporting","reserve","response","rest","restrictions","retailer","ripped","road","rolls","rosin","rue","rule","sask","school","searching","sells","separate","server","shake","shark","shelf","shift","ships","shitty","shock","shoppers","shopping","shops","shrink","shut","sign","sites","sitting","slightly","smoker","smokes","soil","source","sovereignty","spend","sprays","spruce","sqdc","starseed","sticky","stomach","stoned","stop","stored","story","strategy","strawberry","strong","subs","substance","subtle","successfully","suddenly","suffering","suppose","symbl","system","taking","target","tax","term","terms","terrascend","that'd","ton","tons","tourist","town","trades","traffic","trail","transporting","true","tuesday","united","users","usual","valley","vaping","vapor","verification","vie","violates","visited","visiting","wake","warehouse","warning","warrant","watch","we’re","wednesday","weird","west","who’ve","wide","wipe","woodstock","world","worry","www.cannabis","yorkton","youre","0.01","0.5g","01am","0a2moosomin","0e0estevan","0v2martensville","1.4136368","1.4867420","100,000","11pm","15am","17some","17th","195c","1ec5c87b","1st","200mg","20celebrate","20pm","21st","262.36","27am","27th","2pm","3.44g","3.50","3.5gs","30,000","30am","30g","39.99","3n0wiid","4.20","4.5","4.85","421a","48.95","49.95","5.50","5.5g","50,000","50k","56.50","56amontariohttps","5mg","6.95","62.99","6am","6pm","7.5g","7.95","77.63","78.70","8th","8ths","9p9ens","aaaa","aah","about.they","abroad","absolutely","absorbed","abuser","accept","acceptable","accepted","access.and","accessories","ace","acmpr","acquire","acquired","act.pot","activation","activities","acts","adam","addict","addressed","adds","admissible","admission","adult","advance","advertising","afaik","agents","aglc","agree","agreed","ahhh","airline","airplane","airspace","airtight","albertacannabis","albertacannabis.orghere","albertahttps","albertfire","alien","alive","amazing.both","amazing.i","amazon","america","america's","analysis","angles","angyfox13","announced","annoying","answered","apologies","appeal","applies","appreciated","approach","approved","area.edit","aren’t","arrange","arriving","arrogant","ash","assessment","asshats","assumed","assuming","assure","athabasca","atleast","atm","attempt","attitude","attracting","august","aussie","australian","authorized","ave","average","awake","aws","b8015","babies","backlogged","backwards","bag","bake","baked","banks","barter","base","bastards","batteries","battleford","bcuz","beatles","beer","beginner.weedmd","bellerose","bench","benefit","benson","bet","bias","bickel","biohazard","biosector","bit.i","bits","blaze","blend","board","booo","booooo","boost","bootleggers","bore","bored","born","bother","boulevard","box","boxes","brain","brand","brandace","branding","brandno","breaking","breeders","bringing","brings","brother","browse","browser","brunswick","brunswickhttps","brutal","bubbler","bucks","buisness","bullshit","bummed","bureaucrat","buried","businesses","butter","buyer","bylaws","c45","cache","cake","calgarynova","calgarysmall","caller","calling","calls","canabis","canadagrows","canadas","canadianmoms","canadians","canadients","cancellation","cancelled","cancelling","cannabisocs","cannabutter","cannaflower","caps","card.edit","card.so","cardholder","care","cart","catching","catsa","cause.you","caused","centennial","cents","cerebral","challenging","chamber","charged","charging","charlottetown","chatted","cheap","checking","checks","child","childproof","children","china","choice","chose","cigarette","circulated","citation","cite","citizen","citizen.i","citizens","city","clandestine","clarify","clearing","click","clinic","clone","close","closet","clue","coconut","coffeeshops","coils","cold","colors","columbia","columbiaonline","comanies","comfortable","comments.hope","comments.thanks","committed","committing","common","communication","community","compare","competitively","competitors.the","complaining","complicated","complies","complies.no","concierge","concrete","condo","confirmation","confusing","congrats","connect","connection","conspiracy","constitute","construction","consultandgrow.cathey","consumers","contact","containers","continent","continuing","controlled","convenient","convert","convicted","cop","corner","corp","corporations","couldn’t","countdowns","countries.any","courtroom","covering","covers","crappy","crash","create","creative","crept","criminally","crippling","critical","cross","crosses","crossing.jeez","crossings","crotch","crowded","crown","curious","customers","customs","d'octobre","d290","dab","dad","daily","dates","day.everyone","day.i","daysdidn't","dead","decarb","decarbed","deceiving","decide","decided","decides","declining","deemed","deep","deeper","def","default.here","defined","defitnely","dehydrate","delahaze","delay","delays","delivered","deliveries","delivers","density","dent","deny","depending","depts","describe","deserve","desired.all","desk","detailed","details","determine","determined","device","devonwaldo's","dick","didnt","difficult","dig","digest","diligent","dipping","direct","directly","disappeared","disappears","disappointed","disappointing","disapproving","discord","discount","discovered","discrete","discuss","discussion","dislike","dispensaries","dispensary","dispensery","disproportionately","dissapointed","distalite.the","distillate","doable","doe","dogged","dominant","doob","dosing","doug","downtown","downvote","downvoted","downvotes","downvoting","dozen","drag","draw","dreamed","dreams","dried","driven","driver","drop","drops","drove","drug","drugs","dum","dynavap","eastnewfoundland","editor","edmonton420","edmontonalternative","edmontoncannabis","edmontonfire","edmontonnova","edmontonwestside","educated","education","elements","emails","employee","energetic","enforceable","enforcing","enjoyable","enter","enters","epic","essential","est","eventually","evidence","excuse","exist","expectations","expensive","expensive.money","experience:order","experience:packaging","experience.website:great","experienced","experiences","experienceunable","expert","expired","explain","explained","extra","extremely","facilitating","factors","fall","familiar","faq","farmers","fat","favorites","favourite","favourites","federally","feds","feedback","feeling","feels","fields","fight","figr","fill","filling","filtered","finagling","finally","financial","finished","firecrackers","flagging","flat","flawed","flex","flight","floods","flowers","flu","fluffy","fly","flying","follow","fool","football","forced","ford","fortunate","forward","fredericton","freedom.i","fri","frostiness","fucked.shipping","fuckers","fucking","fuss","future","gagner","games","generalize","giant","girlfriend","glad","glass","glut","god’s","good.otherwise","got.also","gov’t","government's","government’s","grab","grade","grail","gramflat","grand","grandchildren","granted","grants","grass","grateful","greenhouses","greens","grind","grocery","ground","grovenova","grower","grower.wasn't","guaranteed","guide","gwill","habit","hah","hairs","halifax","hammer","hand","handed","handle","handling","handy","happened","hardcore","harper","harriman","hassle","hassles","hatmanitobahttps","hatnova","hatnumo","head","headache","headband","headed","hearing","hearsay","heavier","hectic","hell","helloocs.ca","helping","here.either","here’s","heroin","history","holders","holding","holes","homework","honestly","honey","hops","hotels","hotter","hour","hubert","hung","hybrid","hybridliiv","hydro","hydroponics","i’d","i’ll","id'd","idea","ideal","idiot","idiotsweaksauce","illegally","imgur","immediately","improve","improved","inadmissible","inadmissible.link","incentive","include","incognito","income","increase","increases","indicas","info:canada","information.edit","informationhttps","informed","ingest","ingested","insane","inside","insight","inspection","inspired","intense","intention","intentions","interacting","interchangeably","interior","international","interview","investigated","investigation","invisible","iron","island","issue.i","issued","issuing","it.as","jack","jar","jean","jesus","jet","jill","jive","jnkrbwdk","jobs","joke","joking","journey","judge","jurisdiction","justly","kamloops","kensington","kick","kidding","kiefy","kills","kinky","label.liiv","labelling","labour","labradorhttp","lack","lady","lagged.i","land","landrace","largest","late","laughable","laughter","launch","laying","layout","leading","leaves","leaving","led","leftthe","legalities","legalize","legit","lethbridge","licence","licensee","licensing","lifetime","lift.co","likes","liking","lil","link","links","lipid","liquor","lists","lmao","lmfao","loading","locally","located","locking","logic","login","lol.like","lol’d","lolol","lolwtf","longtime","looked","lots","lounges","lucky","lug","luggage","lumped","lying","macro","magazine","magento","magneto","mail","majour","mangled","mango","manitoba","manitobans.saskatchewanjimmy's","map","mappeel","maprosemont","mapville","marie","maritimes","marked","marketconvenient","marketplace","master","mastercards","match","match.edit","mate","material","materials","math","me.all","me.oh","media","medically","memory","mentioned","mentioning","meridian","mess","message","messing","metal","method","milled","million","min","mines","minimal","minimum","mins","minute","mirabel","miss","missing","misunderstood","mixed","mmpr","moc","mode","modest","moist","mom's.they","moment.throw","moms","monopoly","months","months.thanks","moochers","more.edit","mornings","moron","morons","move","movies","multiple","munchies","municipal","muted","namao","nationwide","naturals","nb's","neat","needed.edit","negative","neighbourhood","nelson","net","network","newfoundland","newsroom","nexus","niagara","nicer","noobs","noon","nose","note","noticing","novelty","now.canada","nudes","nug","numo","nunavut","nurse","nutella","nuts","obtain","of.i'll","off.edit","office.money","oil‘s","omg","ontarios","oof","optimistic","option.some","original","orwch9u","ouest","out.edit","outcome","outsell","outstanding","outta","owners","packages","paids","pain","paranoid","parent","part.all","partake","partakeoh","parties","pass","pass.going","passed","passenger","passes","passwordbrick","path","patience","patient","paused","peace","pei","pennies","penny","percentage","perfect","permission","permits","permitting","pervasive","phemonia","photos","pic","pieces","pill","pills","pinene","pizza","placing","plaingreen","plan","planes","plant.guess","platform","playing","plenty","plz","pm.no","political","pooping","pop","portalocs","posed","possibility","posted","posting","pot.students","pot's","potbyprovince.ca","potent","potential","potentially","poti","potpriced","pounds","preclearance","premier","premium","prepaids","prepared","prerolls","president's","pressing","price.the","pricing:i","pricing.https","primo","prize","process","processed","producers","prohibition","proliferation","promise","promised","prongs","proof","properly","prov","provide","provided","provider","providing","provincefeel","provincially","pumped","punch","purely","purolator","quality.people","quantity","quebecers","queues","quickly","rambling","random","rate","rates","ratio","reasoned","reasoning","rebuy","rec","receipt","reckon","recommending","redditors","reduce","referring","refrain","register","regular","regulation","relating","release","released","relevant","remain","remember","remembered","remove","replies","reply","reporter","represented","represents","requests","residences","resistance.obviously","resource","restaurant","restock","restocked","restocking","restocks","restricting","retail.battleford","ridiculous","rioted","rips","risk","road.i","robbed","rough","route","rqjjbcr","rugles","rules","rules.the","running","runs","s.provide","s0g","s0k","s0m","s4a","sad","saint","sainte","sake","same.maybe","sample","sampling","saskatchewan","saskatchewanfire","saskatchewannova","sat","sativas","save","savings","scan","scandal","scanning","scarab138","scare","schedule","scheer","scotia","scotiahttps","screen","screw","screwing","seal","sealed","searchable","searches","season","security.product","sedative","sellare","sensitive","seriously.shipping","servers.it","session","sessions.all","set","setting","shame","share","shared","shipping.catching","shippingpre","shishkaberry","shocked","shop.if","shopcannabisnl.com","shopify","short","shorted","shots","should've","shown","significant","silly","situations","size","sizes","slowly","slows","smallish","smart","smashing","smelled","smith","smokable","smoke.the","smoked.good","smooth","snapped","soften","solar","solei's","solid","solve","solventless","someday","sooooooooooooooo","sound","southfort","southpointe","space","speeches","spending","spoiled","spoke","spoken","spoonful","spouse","stability","staff","stag","standards","starting","starts","stash","stated","states.i","stay","step","stepping","stern","stigma","stinky","stocked","stoked","stone","stoners","stony","storefront","stories","storz","straightened","streams","streets","strict","strikes","struggling","stuck","stuffing","subject","subreddit","substantially","success.was","successful","sucks","suggest","suggesting.in","suggestion","suggestions","suh","sunday","sunflower","sunk","suppliers","support","support_the_breeders_lp_strain_overview","supporting","surely","surreal","suspicion","swallow","swap","sweat","sweedy","sweeps","swim","swimming","system.i","tactic","tag","talked","talks","taste","tasty","taxi","tea","tempered","ten","terpenes","terpinolene","terrorism","that.no","that.the","thc:cbd","them.police","themed","there.the","thermometer","they’ll","they’ve","thier","this.make","thread.british","thrive","thriving","thrown","thurs","thursday","thx","ticket","tight","tilray's","timer","timezones","tinctures","tire","today.fyi","today's","tolerance","tonight","tool","toss","total","touch","tough","tourism","tourists","traceable","tradeitforweed.club","traveler","tray","trend","trichomes","tried.i","tub","turnaround","tweed","tweeds","types","uhm","unable","unavailable","unbelievable","uncertainty","understand","understanding","understood","unexpected.i","unfold.can't","unfortunately.edit","unique","unlicensed","unrelated","up.i'm","updating","uplifting","upload","ups.personally","upset","us.they","usa","usabilitypoor","uscbp","vancouver","vancouver.holding","vaporized","vegging","vehicle","vehicle.out","viable","vicinity","video","view","violate","violation","viscous","visit","voila","volume","volumes","wack","waited","waking","walking","walmart","war","waste","wax","we’ll","weaknesses","weary","web","website:a","weds","weed.police","weed.then","weed.you","weedless","weedmd's","weekly","weigh","weighed","whats","whelp","whiff","whim","white.the","widow","wife","willow","winnipeg","witb","wits","woman","wont","worker","worried","worse","would've","wow","wrong","wrote","wth","wut","www.albertacannabis.org","www.bccannabisstores.com","www.cbc.ca","www.cbp.gov","www.cp24","www.delta9","www.reddit.com","www.shopcannabisnl.comnew","www.sqdc.ca","yah","yea","yesterday's","yet.edit","yetquebecmontreal","yikes","you.i","you.take","you’ll","yup","zip","zombiejesus1987no","zone","zoom","zzz","ಠ_ಠ"],"freq":[52,42,40,38,35,30,29,26,25,24,24,22,20,20,20,20,19,19,18,17,17,17,17,17,16,16,16,16,15,15,15,14,14,14,14,13,13,13,13,13,12,12,12,12,12,12,12,12,11,11,11,11,11,11,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"fontFamily":"Segoe UI","fontWeight":"bold","color":"green","minSize":0,"weightFactor":3.46153846153846,"backgroundColor":"black","gridSize":0,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":true,"rotateRatio":0.4,"shape":"circle","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":[]}</script>
<p>While words like cannabis, legal, weed, etc. would be expected to appear in the discussion, there are other frequent words which may arouse your interest. For example, there are a lot of words associated with online shopping. Among them, ocs (Ontario Cannabis Store), shipping, buy, visa, credit, debit, and card appear most frequently. Such observations suggest that a substantial number of comments in the discussion is about the cannabis business.</p>
<p>Let’s examine some of the comments with those - cannabis business related - words!</p>
<pre class="r"><code>tidy_data %&gt;%
  filter(!is.na(likes),word %in% c(&quot;ocs&quot;, &quot;shipping&quot;, &quot;credit&quot;, &quot;store&quot;, &quot;visa&quot;, &quot;debit&quot;,&quot;buy&quot;))%&gt;%
  group_by(id)%&gt;%
  summarize(n=n())%&gt;%
  arrange(desc(n))%&gt;%
  head(5)</code></pre>
<pre><code>## # A tibble: 5 x 2
##      id     n
##   &lt;int&gt; &lt;int&gt;
## 1   457     4
## 2   147     3
## 3   163     3
## 4   211     3
## 5   450     3</code></pre>
<div class="figure">
<img src="/post/figure/458_iLoveyoumissmary.png" />

</div>
<div class="figure">
<img src="/post/figure/147_EatPastaSkateFasta.png" />

</div>
<div class="figure">
<img src="/post/figure/163_thedommer.png" />

</div>
<p>These comments confirm my hypothesis that those words are closely related to the cannabis business. In addition, the author of these comments are concerned about their privacy while making a purchase. It seems like, they are not certain about the privacy involved in buying cannabis with their credit cards.</p>
<p>Who were the most frequent contributors to the discussion? In addition to a “tibble”, as the output of <code>dplyr</code> functions, <code>ggplot2</code> allows another visual inspection. The following bar plot exhibits the count of comments that the five most frequent contributors to this discussion have posted. However, <code>[deleted</code>] is rather for every unknown author than a specific one.</p>
<pre class="r"><code>dataset %&gt;%
  group_by(author) %&gt;%
  summarise(Comments=n())%&gt;%
  arrange(desc(Comments))%&gt;%
  mutate(author = reorder(author, Comments)) %&gt;%
  head(5) %&gt;%
  
  ggplot(aes(author,Comments))+
  geom_col(show.legend = F)+
  coord_flip()+
  geom_text(aes(label = Comments))+
  ggtitle(&quot;The Most Frequently Contributing Authors&quot;)</code></pre>
<p><img src="/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Most%20Frequent%20contributors%20PLOT-1.png" width="672" /></p>
<p>Because I was assuming these contributors have a substantial impact on positive and negative sentiments, I have decided to focus on the words they posted.</p>
<p>The plot below outlines the specific and most frequently used words by their authors. Bear in mind these words come from <code>nrc</code> lexicon. Therefore, the words that were not contained in the lexicon were filtered out. However, the remaining words create the sentiments.</p>
<pre class="r"><code>data_sentiment %&gt;%
    # Count by word and author
    count(word,author) %&gt;%
    # Group by author
    group_by(author) %&gt;%
    # Take the top 10 words for each author
    top_n(10) %&gt;%
    ungroup() %&gt;%
    mutate(word = reorder(paste(word, author, sep = &quot;__&quot;), n)) %&gt;%
    filter(author %in% c(&quot;BioSector&quot;, &quot;ruglescdn&quot;,  &quot;terrencemckenna&quot;, &quot;frowawe&quot; )) %&gt;%
    # Set up the plot with aes()
    ggplot(aes(word,n)) +
    geom_col(show.legend = FALSE) +
    scale_x_discrete(labels = function(x) gsub(&quot;__.+$&quot;, &quot;&quot;, x)) +
    facet_wrap(~ author, nrow = 2, scales = &quot;free&quot;) +
    coord_flip()</code></pre>
<p><img src="/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Word_Choice-1.png" width="672" /> The output of the most frequently used words posted by individuals may be interesting. But to what extent is it insightful? One should know the specific sentiment these words annotate. The following chart displays counts of all the words from the previous scheme. Most importantly, these words are categorized within the sentiments from <code>nrc</code> lexicon.</p>
<pre class="r"><code>data_sentiment %&gt;%
    # Count by word and author
    group_by(word,author)%&gt;%
    mutate(n=n())%&gt;%
    # Group by author
    filter(author %in% c(&quot;BioSector&quot;, &quot;ruglescdn&quot;,  &quot;terrencemckenna&quot;, &quot;frowawe&quot; ))%&gt;%
    group_by(author) %&gt;%
    top_n(30) %&gt;%
    ungroup() %&gt;% 

    # Set up the plot with aes()
    ggplot(aes(word,n)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ sentiment, scales = &quot;free&quot;) +
    coord_flip()+
    ylab(&quot;Count&quot;)</code></pre>
<p><img src="/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Which_words_contribute_to_scores-1.png" width="672" /></p>
<p>You can see that BioSector, ruglescdn, terrencemckenna, and frowawe express in a number of ways. Even though they do not express disgust or surprise too often, I could not determine what is their most common sentiment. Since a table is not as space demanding as any rigorous plot, the one mentioned below summarizes and counts sentiment related words by their authors.</p>
<pre class="r"><code>data_sentiment %&gt;%
    filter(author %in% c(&quot;BioSector&quot;, &quot;ruglescdn&quot;,  &quot;terrencemckenna&quot;, &quot;frowawe&quot; ))%&gt;%
    group_by(sentiment) %&gt;%
    summarise(count=n())%&gt;%
    arrange(desc(count))</code></pre>
<pre><code>## # A tibble: 10 x 2
##    sentiment    count
##    &lt;chr&gt;        &lt;int&gt;
##  1 positive       122
##  2 trust           83
##  3 negative        77
##  4 anticipation    70
##  5 fear            45
##  6 sadness         35
##  7 anger           32
##  8 joy             30
##  9 surprise        26
## 10 disgust         20</code></pre>
<p>Amazing! BioSector, ruglescdn, terrencemckenna, and frowawe create mostly positive sentiment!</p>
<p>My next goal was to allocate the most positive and negative authors. For this purpose, I created a new variable <code>percent</code>. <code>count()</code> groups author, sentiment, and author_total . Then, it creates a new variable <code>n</code>, and calls <code>ungroup()</code> afterwards. Consequently, <code>n</code> is nothing but the number of words by author within a particular sentiment.</p>
<pre class="r"><code># Which authors use the most negative words?
data_sentiment %&gt;% 
    count(author, sentiment, author_total) %&gt;%
    # Define a new column percent
    mutate(percent=n/author_total) %&gt;%
    # Filter only for negative words
    filter(sentiment %in% c(&quot;negative&quot;,&quot;positive&quot;)) %&gt;%
    # Arrange by percent
    arrange(desc(percent))%&gt;%
    group_by(sentiment)%&gt;%
    top_n(5) %&gt;%
    ungroup()%&gt;%
  
  ggplot(aes(author,percent)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ sentiment, nrow = 2, scales = &quot;free&quot;) +
    coord_flip()</code></pre>
<p><img src="/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The plot above discloses the information of authors, responsible for the greater portion of positive and negative comments. There are two things worthy of mention: i.) the two sets do not intersect so these authors are not ambiguous. ii.) there appears to be “no author” from the group of most frequent contributors.</p>
<p>I also assumed that BioSector, ruglescdn, terrencemckenna, and frowawe are the most popular ones or at least have posted the most popular comments.</p>
<pre class="r"><code>#the Most Popular Comments
dataset %&gt;%
  select(id,author,likes)%&gt;%
  arrange(desc(likes))%&gt;%
  head(5)</code></pre>
<pre><code>##    id           author likes
## 1 436        [deleted]    20
## 2 444 captain_deadfoot    19
## 3 265   AgentChimendez    18
## 4 172        [deleted]    16
## 5 184          jamagut    15</code></pre>
<p>To my surprise, no one appears in these two groups simultaneously.</p>
<p>To have an idea about what drives the participants to like someone’s comment, one should read them! Below are a few screenshots of the top 3 ranked comments: <img src="/post/figure/437_SirWinnipegger.png" alt="center" /> SirWinnipegge complains about new prices in the local dispensary. Has he been buying weed on the black market for lower prices? If so, are the other cannabis users having the same experience? Then is legalizing marihuana an effective strategy to uproot black market cannabis trade? <img src="/post/figure/265_AgentChimendez.png" alt="center" /> On the other hand, AgentChimendez celebrates the date: 10/17/2018 as the new milestone in Canadian history. He points out to the 20th of April, the World Cannabis Day, mostly referred to as 420. <img src="/post/figure/445_captain_deadfoot.png" alt="center" /> Lastly, captain_deadfoot’s comment was liked because he reacted to someone else outside of Canada. He emphasizes how friendly Canadian black market prices are in contrast to other countries. Again, it seems like the black market has already been supplying cannabis for a friendly price. Should the government introduce price ceiling to effectively compete with the black market?</p>
<pre class="r"><code>#Least Popular Comments
dataset %&gt;%
  select(id, author,likes) %&gt;%
  arrange(likes) %&gt;%
  head(5)</code></pre>
<pre><code>##    id         author likes
## 1 297       dubs2112   -10
## 2 351         Justos    -5
## 3  25     Hendrix194    -2
## 4  60      trekrlife    -2
## 5 348 Happyradish532    -2</code></pre>
<p><img src="/post/figure/297_dubs2112.png" alt="center" /> I particularly enjoyed dubs2112’s comments! He/she was constantly bullied (disliked) by others in the discussion because he/she basically calls people stupid for buying legal cannabis products.</p>
<p><img src="/post/figure/352_Justos.png" alt="center" /> The other posts with dislikes appear in the same thread. Happyradish532 and Justos celebrate the shock that the black market might incur. They seem comfortable with paying more for legal cannabis since it is supposed to weaken the black market.</p>
<p>Finally, let’s see how the sentiments - positive and negative - were developing during the first few weeks. This step was the most difficult one. First of all, I created a new data set <code>sentiment_by_time</code> from <code>tidy_data</code>. Since the time span for which the discussion was truly alive is really short, I rounded <code>date</code> to the unit of days and called the new variable <code>date_floor</code>. After grouping by <code>date_floor</code>, I created another new variable <code>total_words</code>, i.e. the number of total words per day. Again, I merged the data with <code>nrc</code> lexicon.</p>
<p>In the next step, I filtered the positive and negative sentiments in <code>sentiment_by_time</code>. Most importantly, I counted entries of the group: <code>date_floor</code>, <code>sentiment</code>, and <code>total_words</code>, and created new variable <code>percent</code> which is the ratio of counts per group and total words submitted in a day.</p>
<p>The purpose of this plot is to demonstrate the long-term development of positive vs. negative sentiment. Additionally, <code>method = &quot;lm&quot;</code> regresses percent on time and plots slopes of the development curves. Unfortunately, the time span in my model is very short and I do not yet mak any conlusions.</p>
<pre class="r"><code>sentiment_by_time &lt;- tidy_data %&gt;%
    # Define a new column using floor_date()
    mutate(date_floor = floor_date(date, unit = &quot;1 day&quot;)) %&gt;%
    # Group by date_floor
    group_by(date_floor) %&gt;%
    mutate(total_words = n()) %&gt;%
    ungroup() %&gt;%
    # Implement sentiment analysis using the NRC lexicon
    inner_join(get_sentiments(&quot;nrc&quot;), by=&quot;word&quot;)

sentiment_by_time %&gt;%
    # Filter for positive and negative words
    filter(sentiment %in% c(&quot;positive&quot;,&quot;negative&quot;)) %&gt;%
    filter(date_floor &lt; ymd(&quot;2018-11-10&quot;)) %&gt;%
    # Count by date, sentiment, and total_words
    count(date_floor, sentiment, total_words) %&gt;%
    ungroup() %&gt;%
    mutate(percent = n / total_words) %&gt;%
    
    # Set up the plot with aes()
    ggplot(aes(date_floor,percent,col=sentiment)) +
    geom_line(size = 1.5) +
    geom_smooth(method = &quot;lm&quot;, se = FALSE, lty = 2) +
    expand_limits(y = 0)+
    ggtitle(&quot;Sentiment Over Time&quot;)</code></pre>
<p><img src="/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Sentiment%20over%20time%20Visual-1.png" width="672" /> Nevertheless, it seems like positive and negative sentiments exhibit the same mild but positive slopes. Secondly, positive sentiment has much greater variation. Finally, both sentiments achieve their peaks shortly after October 22nd and decline later.</p>
<p>To see what stands behind the steep rise of positive sentiment, I found a few explanatory comments.</p>
<pre class="r"><code>sentiment_by_time %&gt;%
  filter(sentiment %in% c(&quot;positive&quot;,&quot;negative&quot;))%&gt;%
  filter( ymd(&quot;2018-10-22&quot;) &lt; date_floor &amp; date_floor &lt; ymd(&quot;2018-10-29&quot;)) %&gt;%
  group_by(date_floor, sentiment, total_words) %&gt;%
  mutate(n=n()) %&gt;%
  ungroup() %&gt;%
  mutate(percent = n / total_words) %&gt;%
  group_by(id)%&gt;%
  mutate(max=max(percent)) %&gt;%
  arrange(desc(max))</code></pre>
<pre><code>## # A tibble: 9 x 11
## # Groups:   id [5]
##   author likes date                   id word  date_floor         
##   &lt;fct&gt;  &lt;int&gt; &lt;dttm&gt;              &lt;int&gt; &lt;chr&gt; &lt;dttm&gt;             
## 1 Inter…     0 2018-10-25 04:40:29    13 ques… 2018-10-25 00:00:00
## 2 Inter…     0 2018-10-25 04:40:29    13 word  2018-10-25 00:00:00
## 3 Inter…     0 2018-10-25 04:40:29    13 legal 2018-10-25 00:00:00
## 4 Inter…     0 2018-10-25 04:40:29    13 ille… 2018-10-25 00:00:00
## 5 Diabl…     1 2018-10-27 23:24:02     6 child 2018-10-27 00:00:00
## 6 h3xad…     2 2018-10-23 04:59:38     7 legal 2018-10-23 00:00:00
## 7 h3xad…     2 2018-10-23 04:59:38     7 money 2018-10-23 00:00:00
## 8 Alypi…     0 2018-10-23 12:32:16    14 legal 2018-10-23 00:00:00
## 9 Hendr…     0 2018-10-23 00:01:04    20 eat   2018-10-23 00:00:00
## # … with 5 more variables: total_words &lt;int&gt;, sentiment &lt;chr&gt;, n &lt;int&gt;,
## #   percent &lt;dbl&gt;, max &lt;dbl&gt;</code></pre>
<p>First of all, we can see that the positive sentiment achieved its peak on October 25th and was caused by a post written by IntermolecularButter (comment id 13). Another important observation is the fact that even though the time span was 1 week, there were only 7 new comments during that time! Less comments stand for less number words, therefore <code>mutate(percent = n / total_words)</code> may fluctuate a lot. Here, the peak is caused by the fact that the denominator was higher before October 22nd, resulting in a lower value of <code>percent</code>.</p>
<p>Nonetheless, let’s see what IntermolecularButter posted. <img src="/post/figure/13_IntermolecularButter.png" alt="center" /></p>
<p>It is not very insightful, is it? The main reason why this post creates a positive sentiment, in addition to the explanation of high variation, is because the twelve words from the comment are associated with positive sentiment! Only one word - illegal - is considered negative. On top of that, <code>sentiment</code> was joined to the data set by <code>inner_join()</code>. By default, the result is that the data frame filtered words that were not present in the lexicon. For example, I noticed the word “thanks”&quot; is not in <code>nrc</code> lexicon even though its connotation is definitely positive.</p>
</div>
<div id="conclusion" class="section level6">
<h6>Conclusion</h6>
<p>The analysis covered three topics; i.) web scraping using <code>rvest</code> package, ii.) cannabis legalization in Canada, and iii.) sentiment analysis. First of all, <code>rvest</code> provides enough flexibility to extract an HTML code from static web page and is therefore ideal for web scraping with R. However, I would suggest using API’s whenever possible to ease your job. Secondly, the discussion on cannabis legalization was not alive as one would expect. It seems that concerns about cannabis users’ privacy were not clearly communicated from the government as is evident from the comments. Additionally, during the time of this discussion posting, the consumers seem to be concerned about the price of legal cannabis being higher than the black market pricing. Lastly, sentiment analysis may not be a reliable approach when it comes to analyzing new policies, especially when something was decriminalized. One can see that more than legalization itself, the cannabis community was rather concerned about prices and privacy.</p>
</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/nlp/">NLP</a>
  
  <a class="badge badge-light" href="/tags/r/">R</a>
  
  <a class="badge badge-light" href="/tags/rvest/">rvest</a>
  
  <a class="badge badge-light" href="/tags/sentimentanalysis/">sentimentanalysis</a>
  
  <a class="badge badge-light" href="/tags/textmining/">textmining</a>
  
  <a class="badge badge-light" href="/tags/webscraping/">webscraping</a>
  
</div>



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hufb83aa85fc46043dba6c25a9ac125096_949376_250x250_fill_lanczos_center_2.png" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/">Jiri Stodulka</a></h5>
      <h6 class="card-subtitle">Data Scientist</h6>
      <p class="card-text" itemprop="description">Former athlete passionate about Artificial Inteligence, Machine Learning, Economics, and next-generation business solutions.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/jiristo" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://www.linkedin.com/in/jiristodulka/" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/jiristo" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://medium.com/@jiristo" target="_blank" rel="noopener">
              <i class="fab fa-medium"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r,python,go.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3394a224b26ce58ff36f44c54743e0ab.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2021 Jiri Stodulka &middot; 
All opinions and views are my own and do not represent my employer &middot;
 Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
