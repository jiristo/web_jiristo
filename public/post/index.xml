<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jiri Stodulka</title>
    <link>/post/</link>
    <description>Recent content in Posts on Jiri Stodulka</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Jiri Stodulka</copyright>
    <lastBuildDate>Sun, 23 Aug 2020 19:12:37 -0400</lastBuildDate>
    
	    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>End-to-End Project: Emotionally Aware Movie Recommender</title>
      <link>/post/emotionally_aware_recommender/</link>
      <pubDate>Sun, 23 Aug 2020 19:12:37 -0400</pubDate>
      
      <guid>/post/emotionally_aware_recommender/</guid>
      <description>&lt;p&gt;&amp;quot;We want to enable the user to utilize their  own emotional state to browse their personalized movie recommendations faster and without external research.&amp;quot;&amp;quot;
The work and deployed model was introduced as the capstone project for the Product Workshop facilitated by the Aggregate Intelecet Socratic Circles &lt;a href=&#34;#AISC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aisc.ai.science/&#34;&gt;https://aisc.ai.science/&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;a href=&#34;https://github.com/jiristo/fast_film_movie_recommender/&#34; target=&#34;_blank&#34;&gt;GittHub Repository&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;iframe width=&#34;1020&#34; height=&#34;500&#34; src=&#34;https://www.youtube.com/embed/h_Xn0TBIRdc&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Video Presentation: Multi-variate (15,000) Time-Series Forecasting for Inventory Items</title>
      <link>/post/time-series-bagging/</link>
      <pubDate>Tue, 07 Apr 2020 19:12:37 -0400</pubDate>
      
      <guid>/post/time-series-bagging/</guid>
      <description>&lt;p&gt;Recently, I was working on a project while using bagging tree-based algorithms for low-frequency time-series data. Originally, the data set contained only three variables for each series. However, given the nature of time-series data stored in the wide formar, I succeeded in engineeing 300 more features.
The presentation was first introduced during an online session organized by the Aggregate Intelect Socratic Circles &lt;a href=&#34;#AISC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aisc.ai.science/&#34;&gt;https://aisc.ai.science/&lt;/a&gt;&lt;/p&gt;

&lt;iframe width=&#34;1020&#34; height=&#34;500&#34; src=&#34;https://www.youtube.com/embed/9inNZbsAhfg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Collaborative Filtering: Matrix Factorization Recommender System </title>
      <link>/post/recsys_cf/</link>
      <pubDate>Sun, 13 Oct 2019 19:12:37 -0400</pubDate>
      
      <guid>/post/recsys_cf/</guid>
      <description>&lt;p&gt;This article targets anyone with previous exposure to machine learning but with little to no knowledge of the recommendation systems. However, it is highly probable that anyone interested in this work interacts with a recommender system regularly. Anyone who listens to Spotify or watches movies on Netflix benefits from the rigorous algorithms (recommendation systems) developed by teams of data scientists and software engineers. The theoretical part of the article explains the fundamentals of various recommendation systems. The practical section emphasizes practical usage of collaborative filtering while utilizing Surprise package and movie-lense data set. Specifically, the author creates two recommender systems utilizing i.) Singular Value Decomposition (SVD), ii.) Non-negative Matrix Factorization (NMF). Both models evaluate/determine users&#39; preferences based on principles of linear algebra.&lt;/p&gt;

&lt;h1 id=&#34;theoretical-part&#34;&gt;Theoretical Part&lt;/h1&gt;

&lt;h2 id=&#34;basics-of-the-recommender-system&#34;&gt;Basics of The Recommender System&lt;/h2&gt;

&lt;p&gt;The reason why consumers need reliable recommendations is straightforward: Given the availability of almost unlimited number of choices (e.g. different movie genres of various quality),  and the natural tendency of human is to maximize his/her utility, the user needs guidance to the next best item that accommodates his/her needs or preferences. Specifically, Netflix has become so popular because of its capability to suggest the user a next movie (from thousands of other options) that matches to the best of user&#39;s taste.&lt;/p&gt;

&lt;h2 id=&#34;types-of-the-systems&#34;&gt;Types of the Systems&lt;/h2&gt;

&lt;p&gt;There are many ways and complex algorithms used to build a recommender system. The following are fundamental approaches. While reading, the reader should think which one may be &lt;strong&gt;the most effective method&lt;/strong&gt; when it comes to a movie recommendation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The Most Popular Item&lt;/strong&gt;: It is the simplest strategy and requires no coding skills. It works based on the assumption that the most popular item attracts most consumers or most users. For example, any consumer shopping on Amazon would see the most frequently bought items. Conversely, Netflix would recommend every user the most popular movie in its list.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Association &amp;amp; Market Based Model:&lt;/strong&gt; The system makes recommendations based on the items in the consumer&#39;s basket. For instance, if the system detected that the buyer is purchasing ground coffee it would also suggest her to buy filters as well (observed association coffee - filters).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Content Filtering:&lt;/strong&gt; Uses metadata to determine the user&#39;s taste. For example, the system recommends the user movies based on their preferences of genres, actors, themes, etc. Such a system matches the user and the item based on similarity. For example, if the user watched and liked Terminator and Predator (both action movies with Arnold Schwarzenegger in the main role), it would probably recommend them to watch Commando.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Collaborative Filtering (CF):&lt;/strong&gt; It is an algorithmic architecture that recommends consumers items based on their observed behavior. There are two types of Collaborative Filtering frameworks: &lt;strong&gt;Model-Based Approach&lt;/strong&gt; and &lt;strong&gt;Memory-Based Approach&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User-based (UBCF):&lt;/strong&gt; It is a predecessor of Item-based CF. UBCF makes recommendations based on the user&#39;s characteristics that are similar to other users in the system. For example, if the end-user positively rates a movie, the algorithm finds other users who have previously rated the movie too, i.e. these users are similar to one another. In the next step, the system recommends the user an unseen movie but highly rated by other - referenced - users. See Figure 1.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Item-based (IBCF):&lt;/strong&gt; IBCF was originally developed by Amazon and is currently adopted by most online corporations (e.g. Netflix, YouTube, etc.).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hybrid Models:&lt;/strong&gt; As the name suggests, the Hybrid Models combine two or more recommendation strategies. For instance, a Hybrid Content-Collaborative System can recommend the user a movie based on their gender but still focuses on the movie features the user exhibits to prefer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While &lt;strong&gt;Hybrid Models&lt;/strong&gt; logically appear to be the most effective ones, &lt;strong&gt;Netflix&#39;s recommendation engine is based on the assumption that similar users like and dislike similar items&lt;/strong&gt;; i.e. &lt;strong&gt;Collaborative Filtering&lt;/strong&gt; is the key to Netflix&#39;s success.&lt;/p&gt;

&lt;p&gt;Both the SVD and NMF models trained in the article are classified as IBCF. &lt;strong&gt;Note that the performance of these models do not meet industry standards. With recent advances in deep learning, online users currently encounter recommendations trained with various types of (hybrid) neural networks (e.g. MLP, CN, RNN, etc.)&lt;/strong&gt;. If the reader is interested in a new perspective and the most advanced models used by the giant online corporations, &lt;a href=&#34;https://arxiv.org/abs/1707.07435&#34;&gt;&lt;em&gt;Deep Learning based Recommender System: A Survey and New Perspectives&lt;/em&gt;&lt;/a&gt; by Zhang et al. provides a complex overview of up-to-date development in this field.&lt;/p&gt;

&lt;h2 id=&#34;userbased-ubcf&#34;&gt;User-based (UBCF)&lt;/h2&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./Untitled_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1&lt;/em&gt; demonstrates how the UBCF works when the system identifies similar users (the reference group) to the end-user. Again, to explain exactly how the recommender system evaluates/determines the user&#39;s taste, one should recall the assumption that similar users exhibit similar preferences. Precisely, that is how the reference group is determined; i.e. both the user and group share a history of rating similar items. The author of this article describes the similarity as &amp;quot;empirical&amp;quot; because the similarity is observable in data. In the next step, the system allocates the only items the reference group had previously been exposed to and recommends the end-user items with the highest predicted rating.&lt;/p&gt;

&lt;h3 id=&#34;issues-with-ubcf&#34;&gt;Issues with UBCF&lt;/h3&gt;

&lt;p&gt;Even though CF is powerful, there are few challenges. Like other researchers, Sarwar et al. (2001) state that scalability and sparsity are the primary issues.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Scalability: In a system where there are many new users, i.e. users with low records of ratings, it is computationally expensive to train the model. A typical example would be a web-based application with a recommender system incorporating millions of users and items. Such problems arise with Nearest Neighbour algorithms in UBCF where such algorithms require computations that grow simultaneously with the increasing numbers of users and items.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sparsity arises in a system when even the most active users have rated or purchased only a marginal number of available items. For example, these users may have experience with even less than 1% of available items.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If a recommender system experiences either one or both of the described issues, the algorithm&#39;s performance decreases; i.e. the system does not recommend the user relevant items (Sarwar et al.,2001). Consequently, the user does not trust the recommendations.&lt;/p&gt;

&lt;h2 id=&#34;itembased-ibcf&#34;&gt;Item-based (IBCF)&lt;/h2&gt;

&lt;p&gt;To tackle the issues with UBCF, item-based collaborative techniques analyze the user-item matrix and identify relationships between different items (Sarwar et al.,2001). The item-based recommendation system then makes recommendations based on the discovered linear relationships (similarities) amongst the items.&lt;/p&gt;

&lt;h1 id=&#34;collaborative-filtering-modelbased-approach&#34;&gt;Collaborative Filtering: Model-Based Approach&lt;/h1&gt;

&lt;p&gt;Once again, this article discusses Collaborative Item-based Filtering and focuses on the Model-Based Approach which tackles the two challenges imposed by CF. Unlike Memory-Based Approach, Model-Based procedure facilitates machine learning techniques such as Singular Value Decomposition (SVD) and Matrix Factorization models to predict the end user&#39;s rating on unrated items. In the context of a movie-to-movie recommender, a collaborative filter answers the question: “What movies have a similar user-rating profile?&amp;quot;(Lineberry &amp;amp; Longo, 2018).&lt;/p&gt;

&lt;h2 id=&#34;importing-packages-and-data&#34;&gt;Importing Packages and Data&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


from collections import defaultdict #data colector

#Surprise: https://surprise.readthedocs.io/en/stable/
import surprise

from surprise.reader import Reader
from surprise import Dataset
from surprise.model_selection import GridSearchCV

  ##CrossValidation
from surprise.model_selection import cross_validate


  ##Matrix Factorization Algorithms
from surprise import SVD
from surprise import NMF

np.random.seed(42) # replicating results
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;importing-online-data&#34;&gt;Importing Online Data&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://grouplens.org/datasets/movielens/&#34;&gt;MovieLens&lt;/a&gt; provides available rating datasets from the &lt;a href=&#34;http://movielens.org&#34;&gt;MovieLens&lt;/a&gt; web site (F. M. Harper and J. A. Konstan, 2015). Any machine learning practitioner may use several different rating files with a number of rated movies and the time of release. For demonstrative purposes and limited computation power, the author worked with 100,836 ratings and 3,683 tag applications across 9,742 movies. The full description of the particular dataset can be found &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html&#34;&gt;here&lt;/a&gt;. According to the documentation, &lt;strong&gt;the selected users in data rated at least 20 movies on the scale from 0.5 to 5&lt;/strong&gt;. The dataset was last updated on 9/2018&lt;/p&gt;

&lt;p&gt;The work considers only tidy data in &lt;code&gt;ratings.csv&lt;/code&gt; and &lt;code&gt;movies.csv&lt;/code&gt;.
Specifically, &lt;code&gt;ratings_df&lt;/code&gt; records &lt;code&gt;userId&lt;/code&gt;, &lt;code&gt;movieId&lt;/code&gt;, and &lt;code&gt;rating&lt;/code&gt; consecutively.
On the other hand, &lt;code&gt;movies_df&lt;/code&gt; stores values in &lt;code&gt;movieId&lt;/code&gt; and &lt;code&gt;genres&lt;/code&gt;. &lt;code&gt;movieId&lt;/code&gt; is, therefore, the mutual variable.&lt;/p&gt;

&lt;p&gt;Note that &lt;code&gt;Surprise&lt;/code&gt; enables one to upload data, e.g. csv files, for predictions through its own methods. On the other hand, as it is discussed below, &lt;code&gt;Surprise&lt;/code&gt; also allows the user to use pandas&#39; DataFrames. The author works with &lt;code&gt;pd.DataFrame&lt;/code&gt; objects for convenience.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from io import BytesIO
from zipfile import ZipFile
from urllib.request import urlopen

r = urlopen(&amp;quot;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&amp;quot;)
zipfile = ZipFile(BytesIO(r.read()))

#print the content of zipfile
zipfile.namelist()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&#39;ml-latest-small/&#39;,
 &#39;ml-latest-small/links.csv&#39;,
 &#39;ml-latest-small/tags.csv&#39;,
 &#39;ml-latest-small/ratings.csv&#39;,
 &#39;ml-latest-small/README.txt&#39;,
 &#39;ml-latest-small/movies.csv&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# tidy df ratings (movieId,)
ratings_df = pd.read_csv(zipfile.open(&#39;ml-latest-small/ratings.csv&#39;))
print(&#39;Columns of ratings_df: {0}&#39;.format(ratings_df.columns))

#movies df (tidy data)
movies_df = pd.read_csv(zipfile.open(&#39;ml-latest-small/movies.csv&#39;))
print(&#39;Columns of movies_df: {0}&#39;.format(movies_df.columns))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Columns of ratings_df: Index([&#39;userId&#39;, &#39;movieId&#39;, &#39;rating&#39;, &#39;timestamp&#39;], dtype=&#39;object&#39;)
Columns of movies_df: Index([&#39;movieId&#39;, &#39;title&#39;, &#39;genres&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;inspecting-the-data&#34;&gt;Inspecting the Data&lt;/h2&gt;

&lt;p&gt;One of the advantages of training on the selected dataset is its purity. Unlike in the real world, one does not need to spend extra time on data cleansing. The following chunk&#39;s output demonstrates how the data is stored.
The results are in line with the disclosed data description.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#ratings
print(ratings_df.head())

print(ratings_df.info())

print(ratings_df.describe())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   userId  movieId  rating  timestamp
0       1        1     4.0  964982703
1       1        3     4.0  964981247
2       1        6     4.0  964982224
3       1       47     5.0  964983815
4       1       50     5.0  964982931
&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 100836 entries, 0 to 100835
Data columns (total 4 columns):
userId       100836 non-null int64
movieId      100836 non-null int64
rating       100836 non-null float64
timestamp    100836 non-null int64
dtypes: float64(1), int64(3)
memory usage: 3.1 MB
None
              userId        movieId         rating     timestamp
count  100836.000000  100836.000000  100836.000000  1.008360e+05
mean      326.127564   19435.295718       3.501557  1.205946e+09
std       182.618491   35530.987199       1.042529  2.162610e+08
min         1.000000       1.000000       0.500000  8.281246e+08
25%       177.000000    1199.000000       3.000000  1.019124e+09
50%       325.000000    2991.000000       3.500000  1.186087e+09
75%       477.000000    8122.000000       4.000000  1.435994e+09
max       610.000000  193609.000000       5.000000  1.537799e+09
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#movies
print(movies_df.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;   movieId  ...                                       genres
0        1  ...  Adventure|Animation|Children|Comedy|Fantasy
1        2  ...                   Adventure|Children|Fantasy
2        3  ...                               Comedy|Romance
3        4  ...                         Comedy|Drama|Romance
4        5  ...                                       Comedy

[5 rows x 3 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that &lt;code&gt;movies_df&lt;/code&gt; contains only  &lt;code&gt;movieId&lt;/code&gt; and &lt;code&gt;genres&lt;/code&gt; variables which store even multiple genres separated by the vertical bar in one cell.&lt;/p&gt;

&lt;h2 id=&#34;data-preprocessing&#34;&gt;Data Pre-Processing&lt;/h2&gt;

&lt;h4 id=&#34;filtering-data-set&#34;&gt;Filtering Data Set&lt;/h4&gt;

&lt;p&gt;Firstly, it is essential to filter out movies and users with low exposure to remove some of the noise from outliers. According to the official MovieLens documentation, all selected users have rated at least 20 movies in the data set. However, the following code filters out the movies and users based on an arbitrary threshold and creates a new data frame &lt;code&gt;ratings_flrd_df&lt;/code&gt;. Moreover, the chunk also prints the value of deleted movies with new and old dimensions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;min_movie_ratings = 2 #a movie has was rated at least 
min_user_ratings =  5 #a user rated movies at least


ratings_flrd_df = ratings_df.groupby(&amp;quot;movieId&amp;quot;).filter(lambda x: x[&#39;movieId&#39;].count() &amp;gt;= min_movie_ratings)
ratings_flrd_df = ratings_flrd_df.groupby(&amp;quot;userId&amp;quot;).filter(lambda x: x[&#39;userId&#39;].count() &amp;gt;= min_user_ratings)



&amp;quot;{0} movies deleted; all movies are now rated at least: {1} times. Old dimensions: {2}; New dimensions: {3}&amp;quot;\
.format(len(ratings_df.movieId.value_counts()) - len(ratings_flrd_df.movieId.value_counts())\
        ,min_movie_ratings,ratings_df.shape, ratings_flrd_df.shape )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;    &#39;3446 movies deleted; all movies are now rated at least: 2 times. Old dimensions: (100836, 4); New dimensions: (97390, 4)&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;data-loading&#34;&gt;Data Loading&lt;/h2&gt;

&lt;p&gt;While using &lt;code&gt;Surprise&lt;/code&gt;, one can use a bunch of built-in datasets (e.g. Jeseter or even the movielens) parsed by &lt;code&gt;Dataset&lt;/code&gt; module. However, it is usually required to build a customized recommender system. In a case as such, it is necessary to upload your own rating dataset either from a file (e.g. csv) or from a pandas&#39; dataframe. In both cases, you need to define a &lt;code&gt;Reader&lt;/code&gt; object to parse the file or the dataframe by &lt;code&gt;Surprise&lt;/code&gt;. See the reference &lt;a href=&#34;https://surprise.readthedocs.io/en/stable/getting_started.html#use-a-custom-dataset&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the next step, one must load the data set through the call of a particular method of &lt;code&gt;surprise.Dataset&lt;/code&gt;. Specifically, &lt;code&gt;load_from_file()&lt;/code&gt; loads a csv file. Surprise also allows to upload pandas&#39; &lt;code&gt;DataFrame&lt;/code&gt;. This time, it is required to upload the data frame with ratings by user per movie (i.e. in the tidy format) with &lt;code&gt;Dataset.load_from_df&lt;/code&gt; and specify &lt;code&gt;reader&lt;/code&gt; as the argument.&lt;/p&gt;

&lt;p&gt;Lastly, &lt;code&gt;build_full_trainset()&lt;/code&gt; method builds the training set from the entire data set. As demonstrated later, training on the whole data while using the best hyper tuning parameters is useful for the prediction of top arbitrary number of movies for each &lt;code&gt;userId&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader = Reader(rating_scale=(0.5, 5)) #line_format by default order of the fields
data = Dataset.load_from_df(ratings_flrd_df[[&amp;quot;userId&amp;quot;,	&amp;quot;movieId&amp;quot;,	&amp;quot;rating&amp;quot;]], reader=reader)

trainset = data.build_full_trainset()

testset = trainset.build_anti_testset()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following sections aim to explain particular methods of matrix factorization. Since SVD is the first model to be examined,  the scope differs a little. To avoid copy-pasting long chunks of code, the author defines and explains two useful generic functions for a.) performance evaluation and b.) predictions.&lt;/p&gt;

&lt;h2 id=&#34;matrix-factorization&#34;&gt;Matrix Factorization&lt;/h2&gt;

&lt;p&gt;Hopcroft and Kannan (2012), explains the whole concept of &lt;strong&gt;matrix factorization&lt;/strong&gt; on customer data where &lt;em&gt;m&lt;/em&gt; customers buy &lt;em&gt;n&lt;/em&gt; products. The authors explain collaborative filtering in a comprehensive language. For demonstrative purposes, the author of this article demonstrates the concept on a specific case.&lt;/p&gt;

&lt;p&gt;Let matrix &lt;span  class=&#34;math&#34;&gt;\(R_{m*n}\)&lt;/span&gt; represent the ratings on movies assigned by each user, also called the utility matrix. Specifically, the value $r_{ij}=5$ represents the rating of user &lt;em&gt;i&lt;/em&gt; assigned to movie &lt;em&gt;j&lt;/em&gt;. However, the individual&#39;s preference is determined by &lt;em&gt;k&lt;/em&gt; factors. For example, the user&#39;s age, sex, income, education, etc. are likely to affect the user&#39;s behavior. Accordingly, the individual&#39;s rating of a movie ($r_{ij}$) is determined by some weighted combinations of the hidden factors. &lt;strong&gt;In practice, customer&#39;s behavior can be characterized by a k-dimensional vector with much lower dimensions than the original matrix $R$ with $m * n$ dimensions&lt;/strong&gt;. The vector&#39;s components, also called the latent factors, represent the weight of each factor. For example, given a vector $v_2 = [0.2 , 0.8]$ it can be hypothesized that there are only two (unknown) latent factors with subsequent weights describing the rating (behavior).&lt;/p&gt;

&lt;p&gt;Matrix factorization is an effective CF technique because it benefits from the properties of linear algebra. Specifically, consider matrix $R$ as a record of various elements. As it is possible to decompose any integer into the product of its prime factor, matrix factorization also enables humans to explore information about matrices and their functional properties an array of elements (Goodfellow, Bengio, 2016)&lt;/p&gt;

&lt;h2 id=&#34;singular-value-decomposition-svd&#34;&gt;Singular Value Decomposition (SVD)&lt;/h2&gt;

&lt;p&gt;SVD decomposes any matrix into &lt;strong&gt;singular vectors&lt;/strong&gt; and &lt;strong&gt;singular values&lt;/strong&gt;. If the reader has previous experience with machine learning, particularly with dimensionality reduction, they would find traditional use of SVD in Principal Component Analysis (PCA).
Simply put, SVD is equivalent to PCA after mean centering, i.e. shifting all data points so that their mean is on the origin (Gillis, 2014).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Formally, SVD is decomposition of a matrix R into the product of three matrices:&lt;/strong&gt; &lt;span  class=&#34;math&#34;&gt;\(R_{m*n} = U_{m*m} D_{m*n} V_{n*n}^{t}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Where &lt;span  class=&#34;math&#34;&gt;\(R_{m*n}\)&lt;/span&gt; denotes the utility matrix with &lt;em&gt;n&lt;/em&gt; equal to the number of e.g. users and &lt;em&gt;m&lt;/em&gt; number exposed items (movies). $U_{m*m}$ is a left singular orthogonal matrix, representing the relationship between users and &lt;strong&gt;latent factors&lt;/strong&gt; (Hopcroft &amp;amp; Kannan, 2012). &lt;span  class=&#34;math&#34;&gt;\(D_{m*n}\)&lt;/span&gt; is a diagonal matrix (with positive real values) describing the strength of each latent factor. &lt;span  class=&#34;math&#34;&gt;\(V_{n*n}^{t}\)&lt;/span&gt; (transpose) is a right singular orthogonal matrix, indicating the &lt;strong&gt;similarity between items and latent factors&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The general goal of SVD (and other matrix factorization methods) is to decompose the matrix R with all missing $r_{ij}$ and multiply its components &lt;span  class=&#34;math&#34;&gt;\(U_{m*m} D_{m*n} V_{n*n}^{t}\)&lt;/span&gt; once again. &lt;strong&gt;As a result, there are no missing values $r_{ij}$ and it is possible to recommend each user movies (items) they have not seen or purchased yet&lt;/strong&gt;. To better understand linear algebra behind SVD, one can watch Gilbert Strang&#39;s lecture on SVD for MIT OpenCourseWare on YouTube channel or visit refer to NIT &lt;a href=&#34;http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm&#34;&gt;Singular Value Decomposition (SVD) tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/mBcLRGuAFUk&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;number-of-factors-and-rmse&#34;&gt;Number of Factors and RMSE&lt;/h3&gt;

&lt;p&gt;For the demonstrative purpose, let&#39;s examine the effect of number of latent factors &lt;em&gt;k&lt;/em&gt; on the model&#39;s performance. Specifically, it is possible to visually observe the effect of multiple factors on error measurement. As in supervised machine learning, &lt;code&gt;cross_validate&lt;/code&gt; computes the error rate for each fold. The following function computes the average of RMSE given by the five folds and append the empty list &lt;code&gt;rmse_svd&lt;/code&gt;. Consequently, the list contains 100 measures of min RMSE given 100 consecutive values of &lt;em&gt;k&lt;/em&gt; in each test set, and by five folds in every iteration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def rmse_vs_factors(algorithm, data):
  &amp;quot;&amp;quot;&amp;quot;Returns: rmse_algorithm i.e. a list of mean RMSE of CV = 5 in cross_validate() for each  factor k in range(1, 101, 1)
  100 values 
  Arg:  i.) algorithm = Matrix factoization algorithm, e.g SVD/NMF/PMF, ii.)  data = surprise.dataset.DatasetAutoFolds
  &amp;quot;&amp;quot;&amp;quot;
  
  rmse_algorithm = []
  
  for k in range(1, 101, 1):
    algo = algorithm(n_factors = k)
    
    #[&amp;quot;test_rmse&amp;quot;] is a numpy array with min accuracy value for each testset
    loss_fce = cross_validate(algo, data, measures=[&#39;RMSE&#39;], cv=5, verbose=False)[&amp;quot;test_rmse&amp;quot;].mean() 
    rmse_algorithm.append(loss_fce)
  
  return rmse_algorithm
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;    0.0015829191458780407
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;rmse_svd = rmse_vs_factors(SVD,data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To replicate the plot of performance for each subsequent model, the following chunk defines the function &lt;code&gt;plot_rmse()&lt;/code&gt; with two arguments where &lt;code&gt;rmse&lt;/code&gt; is a list of float values and &lt;code&gt;algorithm&lt;/code&gt; is an instantiated matrix factorization model. The function returns a plot with two line subplots that display performance vs. numbers of factors. The second subplot only zooms in and marks &lt;em&gt;k&lt;/em&gt; with the best performance, i.e. the minimum RMSE.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def plot_rmse(rmse, algorithm):
  &amp;quot;&amp;quot;&amp;quot;Returns: sub plots (2x1) of rmse against number of factors. 
     Vertical line in the second subplot identifies the arg for minimum RMSE
    
     Arg: i.) rmse = list of mean RMSE returned by rmse_vs_factors(), ii.) algorithm = STRING! of algo 
  &amp;quot;&amp;quot;&amp;quot;
  
  plt.figure(num=None, figsize=(11, 5), dpi=80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;)

  plt.subplot(2,1,1)
  plt.plot(rmse)
  plt.xlim(0,100)
  plt.title(&amp;quot;{0} Performance: RMSE Against Number of Factors&amp;quot;.format(algorithm), size = 20 )
  plt.ylabel(&amp;quot;Mean RMSE (cv=5)&amp;quot;)

  plt.subplot(2,1,2)
  plt.plot(rmse)
  plt.xlim(0,50)
  plt.xticks(np.arange(0, 52, step=2))

  plt.xlabel(&amp;quot;{0}(n_factor = k)&amp;quot;.format(algorithm))
  plt.ylabel(&amp;quot;Mean RMSE (cv=5)&amp;quot;)
  plt.axvline(np.argmin(rmse), color = &amp;quot;r&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;plot_rmse(rmse_svd,&amp;quot;SVD&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./Untitled_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;According to the figure, there is an increasing trend of worse performance with higher &lt;em&gt;k&lt;/em&gt;. The lowest RMSE is achieved when $k=4$. However, it is worth mentioning that $k=14$ is also very close to the  RMSE achieved with only 4 latent factors. Besides, the author argues that it is not probable that the user&#39;s taste (rating) is determined by such a low number of factors. On the other hand, the result suggests a range of values which can be used in &lt;code&gt;GridSearchCV()&lt;/code&gt;for  parameter tunning.&lt;/p&gt;

&lt;h3 id=&#34;gridsearchcv-sample&#34;&gt;GridsearchCV (Sample)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;param_grid = {&#39;n_factors&#39;: [4,6,9,11,14,18,29]}
gs = GridSearchCV(SVD, param_grid, measures=[&#39;rmse&#39;], cv=5)
gs.fit(data)


# best RMSE score
print(gs.best_score[&#39;rmse&#39;])

# combination of parameters that gave the best RMSE score
print(gs.best_params[&#39;rmse&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.8639552777419859
{&#39;n_factors&#39;: 11}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To make the model generalizable, i.e. avoid over and underfitting, the grid algorithm finds  &lt;code&gt;n_factors = 11&lt;/code&gt;  optimal.&lt;/p&gt;

&lt;h3 id=&#34;training-svd-algorithm-and-predictions&#34;&gt;Training SVD Algorithm and Predictions&lt;/h3&gt;

&lt;p&gt;Next, &lt;code&gt;SVD(n_factors = 11)&lt;/code&gt; fits the model on &lt;code&gt;trainset&lt;/code&gt;. To predict values, i.e. ratings, for each empty element $a_{ij}$ in the utility matrix, it is essential to specify: a.) the users and b.) particular movies that &lt;strong&gt;are not&lt;/strong&gt; in the trainset. &lt;code&gt;build_anti_testset()&lt;/code&gt; method of &lt;code&gt;trainset&lt;/code&gt; accomplishes the goal. It returns a list of ratings (&lt;code&gt;testset&lt;/code&gt;) that are &lt;strong&gt;not&lt;/strong&gt; in the trainset or in the entire utility matrix $R$. Consequently, it is possible to use the fitted model and predict ratings for movies in &lt;code&gt;testset&lt;/code&gt;. &lt;code&gt;algo_SVD.test(testset)&lt;/code&gt;returns the list with predictions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;algo_SVD = SVD(n_factors = 11)
algo_SVD.fit(trainset)


# Predict ratings for all pairs (i,j) that are NOT in the training set.
testset = trainset.build_anti_testset()

predictions = algo_SVD.test(testset)

# subset of the list  predictions
predictions[0:2]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[Prediction(uid=1, iid=318, r_ui=3.5110432282575212, est=5, details={&#39;was_impossible&#39;: False}),
 Prediction(uid=1, iid=1704, r_ui=3.5110432282575212, est=4.953822490465707, details={&#39;was_impossible&#39;: False})]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prediction-and-historical-reference&#34;&gt;Prediction and Historical Reference&lt;/h3&gt;

&lt;p&gt;As is mentioned above, the raw predictions are stored in a list. It would also be difficult to search for any &lt;code&gt;userId&lt;/code&gt; and predictions with meaningful information. For example, let&#39;s assume there are two goals: i.) suggest any &lt;code&gt;userId&lt;/code&gt; the top 10 unseen movies the person is likely to enjoy and ii.) recommend the user movies with titles and genres. Overall, it was the author&#39;s objective  to define a function with the following properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Map the predictions to each user.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Return: i.) recommendations for any given &lt;code&gt;userId&lt;/code&gt; and ii.) the user&#39;s historical ratings&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Return the above objects with specific reference to the movie and its genre in a readable format (i.e. tidy DataFrame)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The below function &lt;code&gt;get_top_n()&lt;/code&gt; accomplishes the goals. The function takes five arguments.  Specifically, &lt;code&gt;predictions&lt;/code&gt; is the list with predictions (&lt;code&gt;predictions = algo_SVD.test(testset)&lt;/code&gt;
) , &lt;code&gt;userId&lt;/code&gt; is an arbitrary user&#39;s id, &lt;code&gt;movies_df&lt;/code&gt; is DataFrame with title and genre to each &lt;code&gt;movieId&lt;/code&gt;, &lt;code&gt;ratings_df&lt;/code&gt; contains historical ratings, and &lt;code&gt;n&lt;/code&gt; specifies how many movies should be recommended to the user. By default, &lt;code&gt;n&lt;/code&gt; is set to 10.
&lt;code&gt;get_top_n()&lt;/code&gt; consists of two main parts:
&lt;br&gt;
&lt;br&gt;
Part I. comes from the official Surprise documentation. It maps the prediction to each user, sorts them in descending order, and returns the top n (by default 10) recommended movies for the &lt;code&gt;userId&lt;/code&gt; specified as the argument of the function.
&lt;br&gt;
&lt;br&gt;
Part II. was inspired by an &lt;a href=&#34;https://beckernick.github.io/matrix-factorization-recommender/&#34;&gt;article&lt;/a&gt; published by IVIDIA data scientist Nick Becker on his blog. It prints the total number of movies rated by the user. Then, it merges the DataFrame objects, i.e. history and predictions, on &lt;code&gt;movieID&lt;/code&gt; in &lt;code&gt;movies_df&lt;/code&gt;. Therefore, besides &lt;code&gt;movieId&lt;/code&gt; both objects consequently contain &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;genres&lt;/code&gt;. One can then &lt;em&gt;holistically&lt;/em&gt; evaluate the model&#39;s performance on an individual level because the function&#39;s output (data frames) allows to observing both the highest rated predictions and the highest rated movies in the past. It is assumed that high rated genres should correspond to the user&#39;s taste and are therefore expected to appear in the recommended movies as well.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_top_n(predictions, userId, movies_df, ratings_df, n = 10):
    &#39;&#39;&#39;Return the top N (default) movieId for a user,.i.e. userID and history for comparisom
    Args:
    Returns: 
  
    &#39;&#39;&#39;
    #Peart I.: Surprise docomuntation
    
    #1. First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    #2. Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key = lambda x: x[1], reverse = True)
        top_n[uid] = user_ratings[: n ]
    
    #Part II.: inspired by: https://beckernick.github.io/matrix-factorization-recommender/
    
    #3. Tells how many movies the user has already rated
    user_data = ratings_df[ratings_df.userId == (userId)]
    print(&#39;User {0} has already rated {1} movies.&#39;.format(userId, user_data.shape[0]))

    
    #4. Data Frame with predictions. 
    preds_df = pd.DataFrame([(id, pair[0],pair[1]) for id, row in top_n.items() for pair in row],
                        columns=[&amp;quot;userId&amp;quot; ,&amp;quot;movieId&amp;quot;,&amp;quot;rat_pred&amp;quot;])
    
    
    #5. Return pred_usr, i.e. top N recommended movies with (merged) titles and genres. 
    pred_usr = preds_df[preds_df[&amp;quot;userId&amp;quot;] == (userId)].merge(movies_df, how = &#39;left&#39;, left_on = &#39;movieId&#39;, right_on = &#39;movieId&#39;)
            
    #6. Return hist_usr, i.e. top N historically rated movies with (merged) titles and genres for holistic evaluation
    hist_usr = ratings_df[ratings_df.userId == (userId) ].sort_values(&amp;quot;rating&amp;quot;, ascending = False).merge\
    (movies_df, how = &#39;left&#39;, left_on = &#39;movieId&#39;, right_on = &#39;movieId&#39;)
    
    
    return hist_usr, pred_usr
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;svd-recommendations&#34;&gt;SVD Recommendations&lt;/h3&gt;

&lt;p&gt;Since the model was properly trained, it is already possible to suggest any &lt;code&gt;userID&lt;/code&gt; &lt;code&gt;n&lt;/code&gt; movies at this stage. Additionally, based on the acquired predictions and defined &lt;code&gt;get_top_n()&lt;/code&gt; function it is reasonable to visually inspect the recommended movies and the user&#39;s highest rated movies in the past. For instance, let&#39;s assume the &lt;code&gt;userId&lt;/code&gt; 124 wants to watch a movie at their earliest convenience but has no specific title in mind. On top of that, the movie database contains over 6,000 titles across multiple genres so the user would spend a lot of time researching for what movies are in line with respect to their &lt;strong&gt;specific preferences&lt;/strong&gt;. The following code makes the user&#39;s choice much easier. After calling &lt;code&gt;get_top_n()&lt;/code&gt; function, it is immediately obvious the user 124 has already rated 50 movies.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hist_SVD_124, pred_SVD_124 = get_top_n(predictions, movies_df = movies_df, userId = 124, ratings_df = ratings_df)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To see the user&#39;s history, let&#39;s examine their top 15 highest rated movies. The ratings are in the range from 5 to 4.5. As the table below shows, the user 124 enjoys a wide range of genres. Specifically, the highest rated movies (rating 5) are mostly dramas. Additionally, the user has mostly rated comedies, thrillers, and action/adventure movies.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hist_SVD_124.head(15)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;movieId&lt;/th&gt;
      &lt;th&gt;rating&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;genres&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1358&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;Sling Blade (1996)&lt;/td&gt;
      &lt;td&gt;Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;3949&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;Requiem for a Dream (2000)&lt;/td&gt;
      &lt;td&gt;Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;7361&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;Eternal Sunshine of the Spotless Mind (2004)&lt;/td&gt;
      &lt;td&gt;Drama|Romance|Sci-Fi&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;6377&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;Finding Nemo (2003)&lt;/td&gt;
      &lt;td&gt;Adventure|Animation|Children|Comedy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;2858&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;American Beauty (1999)&lt;/td&gt;
      &lt;td&gt;Drama|Romance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;356&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;Forrest Gump (1994)&lt;/td&gt;
      &lt;td&gt;Comedy|Drama|Romance|War&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;608&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;Fargo (1996)&lt;/td&gt;
      &lt;td&gt;Comedy|Crime|Drama|Thriller&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;3252&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Scent of a Woman (1992)&lt;/td&gt;
      &lt;td&gt;Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1210&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Star Wars: Episode VI - Return of the Jedi (1983)&lt;/td&gt;
      &lt;td&gt;Action|Adventure|Sci-Fi&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1196&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Star Wars: Episode V - The Empire Strikes Back...&lt;/td&gt;
      &lt;td&gt;Action|Adventure|Sci-Fi&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;3328&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Ghost Dog: The Way of the Samurai (1999)&lt;/td&gt;
      &lt;td&gt;Crime|Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1884&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Fear and Loathing in Las Vegas (1998)&lt;/td&gt;
      &lt;td&gt;Adventure|Comedy|Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;4226&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Memento (2000)&lt;/td&gt;
      &lt;td&gt;Mystery|Thriller&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;3147&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Green Mile, The (1999)&lt;/td&gt;
      &lt;td&gt;Crime|Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;5608&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
      &lt;td&gt;Das Experiment (Experiment, The) (2001)&lt;/td&gt;
      &lt;td&gt;Drama|Thriller&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;When it comes to predictions, the outcome is stored in the same order and format as the user&#39;s history.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pred_SVD_124
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;movieId&lt;/th&gt;
      &lt;th&gt;rat_pred&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;genres&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;750&lt;/td&gt;
      &lt;td&gt;4.610126&lt;/td&gt;
      &lt;td&gt;Dr. Strangelove or: How I Learned to Stop Worr...&lt;/td&gt;
      &lt;td&gt;Comedy|War&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1136&lt;/td&gt;
      &lt;td&gt;4.545477&lt;/td&gt;
      &lt;td&gt;Monty Python and the Holy Grail (1975)&lt;/td&gt;
      &lt;td&gt;Adventure|Comedy|Fantasy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;904&lt;/td&gt;
      &lt;td&gt;4.541237&lt;/td&gt;
      &lt;td&gt;Rear Window (1954)&lt;/td&gt;
      &lt;td&gt;Mystery|Thriller&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1208&lt;/td&gt;
      &lt;td&gt;4.537614&lt;/td&gt;
      &lt;td&gt;Apocalypse Now (1979)&lt;/td&gt;
      &lt;td&gt;Action|Drama|War&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1204&lt;/td&gt;
      &lt;td&gt;4.535593&lt;/td&gt;
      &lt;td&gt;Lawrence of Arabia (1962)&lt;/td&gt;
      &lt;td&gt;Adventure|Drama|War&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;56782&lt;/td&gt;
      &lt;td&gt;4.517568&lt;/td&gt;
      &lt;td&gt;There Will Be Blood (2007)&lt;/td&gt;
      &lt;td&gt;Drama|Western&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1225&lt;/td&gt;
      &lt;td&gt;4.513478&lt;/td&gt;
      &lt;td&gt;Amadeus (1984)&lt;/td&gt;
      &lt;td&gt;Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;898&lt;/td&gt;
      &lt;td&gt;4.512611&lt;/td&gt;
      &lt;td&gt;Philadelphia Story, The (1940)&lt;/td&gt;
      &lt;td&gt;Comedy|Drama|Romance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;2959&lt;/td&gt;
      &lt;td&gt;4.510696&lt;/td&gt;
      &lt;td&gt;Fight Club (1999)&lt;/td&gt;
      &lt;td&gt;Action|Crime|Drama|Thriller&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;2160&lt;/td&gt;
      &lt;td&gt;4.499703&lt;/td&gt;
      &lt;td&gt;Rosemary&#39;s Baby (1968)&lt;/td&gt;
      &lt;td&gt;Drama|Horror|Thriller&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Comparing the predictions with history, one can observe that the genres are in line with the user&#39;s taste.&lt;/p&gt;

&lt;h2 id=&#34;nonnegative-matrix-factorization-nmf&#34;&gt;Non-Negative Matrix Factorization (NMF)&lt;/h2&gt;

&lt;p&gt;NMF is another method used for matrix factorization. Contrary to SVD, NMF decomposes the &lt;strong&gt;non-negative&lt;/strong&gt; utility matrix R into the product of matrices &lt;em&gt;W&lt;/em&gt; and &lt;em&gt;H&lt;/em&gt;: &lt;span  class=&#34;math&#34;&gt;\(R_{n*d} = W_{n*r} H_{r*d}\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Where columns in matrix &lt;span  class=&#34;math&#34;&gt;\(W_{n*r}\)&lt;/span&gt; represent components, while matrix &lt;span  class=&#34;math&#34;&gt;\(H_{r*d}\)&lt;/span&gt; stores the corresponding weights. More importantly,  NMF introduces constraints under which: $W \geq 0$ and &lt;span  class=&#34;math&#34;&gt;\(H \geq 0\)&lt;/span&gt;. The component-wise nonnegativity is a substantial difference from SVD (Gillis, 2017). Additionally to collaborative filtering, one can find use cases of NMF in clustering, image processing,  or music analysis.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmse_nmf = rmse_vs_factors(NMF, data)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;plot_rmse(rmse_nmf, &amp;quot;NMF&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./Untitled_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;param_grid = {&#39;n_factors&#39;: [11,14,15,16,17,18,20]}
gs = GridSearchCV(NMF, param_grid, measures=[&#39;rmse&#39;], cv=5)
gs.fit(data)



# best RMSE score
print(gs.best_score[&#39;rmse&#39;])

# combination of parameters that gave the best RMSE score
print(gs.best_params[&#39;rmse&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.8861525979842921
{&#39;n_factors&#39;: 17}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;algo_NMF = NMF(n_factors = 16)
algo_NMF.fit(trainset)


# Predict ratings for all pairs (u, i) that are NOT in the training set.
testset = trainset.build_anti_testset()
predictions = algo_NMF.test(testset)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;hist_NMF_124, pred_NMF_124 = get_top_n(predictions, movies_df = movies_df, userId = 124, original_ratings_df = ratings_df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;User 124 has already rated 50 movies.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;pred_NMF_124
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;movieId&lt;/th&gt;
      &lt;th&gt;rat_pred&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;genres&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;2324&lt;/td&gt;
      &lt;td&gt;4.604147&lt;/td&gt;
      &lt;td&gt;Life Is Beautiful (La Vita è bella) (1997)&lt;/td&gt;
      &lt;td&gt;Comedy|Drama|Romance|War&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1089&lt;/td&gt;
      &lt;td&gt;4.558471&lt;/td&gt;
      &lt;td&gt;Reservoir Dogs (1992)&lt;/td&gt;
      &lt;td&gt;Crime|Mystery|Thriller&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1213&lt;/td&gt;
      &lt;td&gt;4.548465&lt;/td&gt;
      &lt;td&gt;Goodfellas (1990)&lt;/td&gt;
      &lt;td&gt;Crime|Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1248&lt;/td&gt;
      &lt;td&gt;4.544143&lt;/td&gt;
      &lt;td&gt;Touch of Evil (1958)&lt;/td&gt;
      &lt;td&gt;Crime|Film-Noir|Thriller&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1104&lt;/td&gt;
      &lt;td&gt;4.541697&lt;/td&gt;
      &lt;td&gt;Streetcar Named Desire, A (1951)&lt;/td&gt;
      &lt;td&gt;Drama&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;750&lt;/td&gt;
      &lt;td&gt;4.536357&lt;/td&gt;
      &lt;td&gt;Dr. Strangelove or: How I Learned to Stop Worr...&lt;/td&gt;
      &lt;td&gt;Comedy|War&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;904&lt;/td&gt;
      &lt;td&gt;4.530793&lt;/td&gt;
      &lt;td&gt;Rear Window (1954)&lt;/td&gt;
      &lt;td&gt;Mystery|Thriller&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1235&lt;/td&gt;
      &lt;td&gt;4.526960&lt;/td&gt;
      &lt;td&gt;Harold and Maude (1971)&lt;/td&gt;
      &lt;td&gt;Comedy|Drama|Romance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;1242&lt;/td&gt;
      &lt;td&gt;4.517884&lt;/td&gt;
      &lt;td&gt;Glory (1989)&lt;/td&gt;
      &lt;td&gt;Drama|War&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;898&lt;/td&gt;
      &lt;td&gt;4.512130&lt;/td&gt;
      &lt;td&gt;Philadelphia Story, The (1940)&lt;/td&gt;
      &lt;td&gt;Comedy|Drama|Romance&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The article discussed the “fundamentals of recommender systems and their classification”. Moreover, the author showed how to use Surprise package for two matrix factorization approaches. Among them, SVD achieved slightly lower RMSE (0.864), and therefore performed better, compared to measured RMSE (0.886) by NMF. Both models were hyper parametrized for several latent factors used in training of the algorithms. In addition, the authors showed how to evaluate the models on an individual level by observing the predicted movies and the user&#39;s historical rating.&lt;/p&gt;

&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;

&lt;p&gt;Lineberry, A., &amp;amp; Longo, C. (2018, September 11). Creating a hybrid content-collaborative movie recommender using deep learning. Retrieved from &lt;a href=&#34;https://towardsdatascience.com/creating-a-hybrid-content-collaborative-movie-recommender-using-deep-learning-cc8b431618af&#34;&gt;https://towardsdatascience.com/creating-a-hybrid-content-collaborative-movie-recommender-using-deep-learning-cc8b431618af&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=&lt;a href=&#34;http://dx.doi.org/10.1145/2827872&#34;&gt;http://dx.doi.org/10.1145/2827872&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Gillis, N. (2014). The why and how of nonnegative matrix factorization (2). Retrieved from &lt;a href=&#34;https://arxiv.org/abs/1401.5226v2&#34;&gt;https://arxiv.org/abs/1401.5226v2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Guruswami, V., &amp;amp; Kannan, R. (2012). Singular value decomposition (SVD). In Computer science theory for the information age (pp. 111-135).&lt;/p&gt;

&lt;p&gt;Sarwar, Badrul &amp;amp; Badrul, &amp;amp; Karypis, George &amp;amp; Cybenko, George &amp;amp; Konstan, &amp;amp; Joseph, &amp;amp; Reidl, &amp;amp; Tsibouklis, John. (2001). Item-based collaborative filtering recommendation algorithmus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Toronto Crime and Folium</title>
      <link>/post/toronto-crime/</link>
      <pubDate>Fri, 28 Jun 2019 19:26:32 -0400</pubDate>
      
      <guid>/post/toronto-crime/</guid>
      <description>

&lt;p&gt;In this post, I will analyze major Crime Indicators in Toronto in years from 2014 to 2018. I obtained the publicly available data set from the Toronto Police Service. First, I will visually inspect the crime scene in the City. Specifically, I will use the Matplot library to demonstrate the composition of assaults. Additionally, I will mark the most criminal neighborhoods on the map while utilizing both the &lt;code&gt;MarkerCluster&lt;/code&gt; and &lt;code&gt;HeatMap&lt;/code&gt; as &lt;code&gt;plugins&lt;/code&gt; of folium package. Examining the criminal behavior on the map, shows the downtown as the area with the highest concentration of crime. On top of that, criminal neighborhoods can be easily clustered. Then, one can see that areas alongside the major routes exhibit high criminal activity as well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#GitHub Repository
git clone https://github.com/jiristo/toronto-crime-folium.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
&lt;div style=&#34;width:100%;&#34;&gt;&lt;div style=&#34;position:relative;width:100%;height:0;padding-bottom:60%;&#34;&gt;&lt;iframe src=&#34;data:text/html;charset=utf-8;base64,<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <script>L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;</script>
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
    <meta name="viewport" content="width=device-width,
        initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <style>#map_848f32327bb547c2ba242552f380edf9 {
        position: relative;
        width: 100.0%;
        height: 100.0%;
        left: 0.0%;
        top: 0.0%;
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/leaflet.markercluster.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.Default.css"/>
    <script src="https://leaflet.github.io/Leaflet.heat/dist/leaflet-heat.js"></script>
</head>
<body>    
    
    <div class="folium-map" id="map_848f32327bb547c2ba242552f380edf9" ></div>
</body>
<script>    
    
    
        var bounds = null;
    

    var map_848f32327bb547c2ba242552f380edf9 = L.map(
        'map_848f32327bb547c2ba242552f380edf9', {
        center: [43.70227, -79.366074],
        zoom: 11,
        maxBounds: bounds,
        layers: [],
        worldCopyJump: false,
        crs: L.CRS.EPSG3857,
        zoomControl: true,
        });


    
    var tile_layer_4776b9b1e2d04911b8896178f97c7257 = L.tileLayer(
        'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',
        {
        "attribution": null,
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_848f32327bb547c2ba242552f380edf9);
    
            var marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb = L.markerClusterGroup({});
            map_848f32327bb547c2ba242552f380edf9.addLayer(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
            
    
        var marker_84ef23bb01584a539a32a8716704574a = L.marker(
            [43.6684494, -79.3430939],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b75ed076e608447693888046b5ea2225 = L.marker(
            [43.759285, -79.50792690000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_525087834a3c45db844d22ab541ec0a1 = L.marker(
            [43.69755170000001, -79.5016632],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_15d29874cf22467b8f4674e062f77128 = L.marker(
            [43.7217026, -79.5715103],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_a3adc02c2b094109a4b7ad6fadbdd19b = L.marker(
            [43.6638908, -79.50348659999999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_989875e27ae54703bcc4b147f7909e76 = L.marker(
            [43.65730670000001, -79.3734589],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_6bd48a75e75e46ff8a23aa3afea240ca = L.marker(
            [43.6663628, -79.31660459999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_0efc82d8db49427b875ce86ac29f1fa5 = L.marker(
            [43.65811160000001, -79.4020233],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_3c974c2a31704272937ad17eb5127f40 = L.marker(
            [43.768856, -79.4669266],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_964e15e298fc430880c80ccb04cf038f = L.marker(
            [43.80780410000001, -79.21559909999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_bcd268ac5635419684868072d274bdff = L.marker(
            [43.641552, -79.4745407],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_83a1f673488e46739dcc45e28453a3c9 = L.marker(
            [43.69513320000001, -79.3034821],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_cfdb2cd0e9f1472b9afb47c263a2f71a = L.marker(
            [43.65588, -79.3632202],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_bbd353e609fa4565816343137c69a29e = L.marker(
            [43.6639061, -79.38415529999997],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_0ce437fcb6bc4ba4a90bad2e497f8ddb = L.marker(
            [43.6500702, -79.3968811],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_7f7259b7d50a498884244f9833cc3a26 = L.marker(
            [43.699131, -79.2543335],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_07665d3bbbf64cd3875021bee1fedd74 = L.marker(
            [43.7494888, -79.5327911],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_08b7574888654079b6fb19cdd5706990 = L.marker(
            [43.6724319, -79.33419040000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_bf028436cc99479b9939513297a531ce = L.marker(
            [43.77361679999999, -79.2611313],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_58379e73db1f492faa47af30ad1079a1 = L.marker(
            [43.6878204, -79.43493649999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_248eeda2adc84504946714ffb6ff98df = L.marker(
            [43.65807720000001, -79.3847122],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_3889b6efcc894ca7afe982e17f1e8ae7 = L.marker(
            [43.6449547, -79.397644],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_ec3a6192d0de4eae9b23a3ad8f1cd6e8 = L.marker(
            [43.7564392, -79.3607712],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_47f308696b3a457ea3be7af7984fff5e = L.marker(
            [43.663723, -79.3705215],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_7ced8d53a58e4d7c8a27e958b5608d11 = L.marker(
            [43.7666092, -79.1856613],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_37df0936c23f4674b755ee87c2993567 = L.marker(
            [43.6943665, -79.2736511],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_4b5ddc3aa92e47238c463377e24fccd0 = L.marker(
            [43.7439919, -79.5988693],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_2a19dcb154054611bffc88ff0ffa2e19 = L.marker(
            [43.7630234, -79.3175278],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_86bdb742a9a74c349baec746a87b966f = L.marker(
            [43.768837, -79.37837979999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_4d4337d0df494908810151e80c0548fb = L.marker(
            [43.7460785, -79.3886261],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_15b5d60302784c558703bf1ff4d7d114 = L.marker(
            [43.7565994, -79.5120697],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_f34d5348e07e4f8386255d00628cf6c8 = L.marker(
            [43.6659164, -79.4074707],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_89188efc3b4649a1b2d95b36bb819778 = L.marker(
            [43.768383, -79.3490372],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_8fbfa26c8bcd42aba83e30b48b1e974f = L.marker(
            [43.7333374, -79.26329040000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_13403da8e51c493cb484baef301d562a = L.marker(
            [43.7621841, -79.3211517],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_209a3e37c4af427fb16e0a306163822e = L.marker(
            [43.8115196, -79.3145447],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_bc97871c51524c55a6fa89eb6a6f97d8 = L.marker(
            [43.7768669, -79.31622309999999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_c0adf13649f84c6b86ddac6d05103903 = L.marker(
            [43.7383575, -79.3097839],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_478ca6f71e48434087bc42cb9535b4ca = L.marker(
            [43.7524796, -79.5607376],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_531662b958394c38988f59f7f2948db4 = L.marker(
            [43.7429314, -79.21129609999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_aaff6cf4b63e43c5b56efd3b2b88893d = L.marker(
            [43.6823387, -79.5266037],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_f0a494cdd13a4e1fb1bd503c3cd475bf = L.marker(
            [43.7751579, -79.4143829],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_c70985bac21d41bc9239c1813b7834fe = L.marker(
            [43.7130508, -79.41177370000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b64ad2b123f94176bb8ac96e31d335b9 = L.marker(
            [43.7775917, -79.22657779999999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_524a2a5233144b86aaed77a63733ca65 = L.marker(
            [43.7170258, -79.5372925],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_2597eb62f2544dc58e8e7574de19848d = L.marker(
            [43.7412338, -79.2372665],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_43f5caba79574ee7bdbe4ad27585d0b4 = L.marker(
            [43.7068634, -79.32061],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b3168745b8fa4f9d9473494c48db1e2b = L.marker(
            [43.6030045, -79.50748440000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_01d3173ecb9b46d3bbdcd1a7e55fda0c = L.marker(
            [43.7165794, -79.3303757],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_bcbf52d42adf4d6191b0780f7936df93 = L.marker(
            [43.7915154, -79.2981033],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_c8d177e96b8a4cbab8b381b908ba8cfd = L.marker(
            [43.6412277, -79.42667390000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_554bc88890c04f669c8d3ec260ce0f7a = L.marker(
            [43.6932449, -79.50466920000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_39d3fb3d3e524e1e952d3ea1c1994e86 = L.marker(
            [43.6844292, -79.3469238],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_8158a412adb84c8d93c1f79f78cb3628 = L.marker(
            [43.7899132, -79.217453],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_a3ed5a8d3e8b460b9f2cc4ceac7f5115 = L.marker(
            [43.6859818, -79.3562927],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_fb2139aa6236497894a116077bae6c93 = L.marker(
            [43.6989937, -79.43503570000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b63dda4f03da49f7898f1de2ad0003a3 = L.marker(
            [43.7072258, -79.2952805],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_d7c2aedb837e44c9a2c2606ae93a7c8a = L.marker(
            [43.7335815, -79.4837265],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_d44b5c1d51ce46cea08c478da346214b = L.marker(
            [43.6446571, -79.5307999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_ba691605ddaa4e2a9fcdf6f61c91b544 = L.marker(
            [43.7689018, -79.28555300000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_f610e642f4944d35b22b3d98332e8ddd = L.marker(
            [43.8019333, -79.3593979],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_0dc968b597744d53960d150e1115c333 = L.marker(
            [43.69834520000001, -79.5113144],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_5e68bf8d6e4e4b9980abf00e89f06977 = L.marker(
            [43.7797699, -79.41557309999997],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_753a7f504986430fbc284fa5f6118d7c = L.marker(
            [43.6962624, -79.44768520000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_13f98864c5eb4ba2972e7f072d88b097 = L.marker(
            [43.732395200000006, -79.41941070000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_bcf433aa2c544338b5ca1782291d680e = L.marker(
            [43.6507835, -79.41349790000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_9c93ae7783e84c029c6dae440655cd61 = L.marker(
            [43.7003136, -79.38697049999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_f7a218cb14284a28aaa933d02db93374 = L.marker(
            [43.8012695, -79.29660030000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_c116cbabca804ee984cdc185de83fbb1 = L.marker(
            [43.7054634, -79.40009309999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_5da07d955cb344a0b6e2dd02e3336afc = L.marker(
            [43.6684685, -79.4390335],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_50cc2981a25a424b94d0132c0800cdfb = L.marker(
            [43.6924973, -79.31573490000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_27c5534211464d87901cbd994ac587a5 = L.marker(
            [43.8110886, -79.26600649999997],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_d63a496798824d16bc5c76dcf6151e65 = L.marker(
            [43.6661034, -79.587677],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_147cf41d9bfe4f3f9778af6da2eb4c84 = L.marker(
            [43.7035904, -79.45242309999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_fd0147af0e9a452ba6284d3bcc003758 = L.marker(
            [43.6831703, -79.41830440000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_a3f50f2a49a04243959f3904130264f4 = L.marker(
            [43.6403351, -79.4379196],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_83e22522be7649cd9e9ff7b290ca8471 = L.marker(
            [43.77856060000001, -79.3287048],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_6af1e3a7c93e4846b9329311cd172f38 = L.marker(
            [43.69122700000001, -79.47039790000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_39ba314b5d9647529b3a4945f84f236d = L.marker(
            [43.67886729999999, -79.3268204],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_4a08fbf58cfd46cf8dcaf7bd974cad69 = L.marker(
            [43.74681470000001, -79.58380890000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_f4925b7987fd4787a64c6585fa4d15fe = L.marker(
            [43.7865829, -79.18830870000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_15166efd9a0f422793da0f7b2725dc46 = L.marker(
            [43.6508102, -79.5223618],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_55b4110174844501a082a32d9a125a5c = L.marker(
            [43.7205162, -79.4807281],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_44f0747fea9f42d7be0f9821882fde97 = L.marker(
            [43.78576279999999, -79.28945920000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_ac60128bdbe244c8ab248879e42e2bb7 = L.marker(
            [43.7876511, -79.35165409999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_497fe04f4c9e41148b93abea2a5b361b = L.marker(
            [43.7738609, -79.4430084],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b81759f439914f948fa46a15fb8f8bd2 = L.marker(
            [43.63902279999999, -79.41591640000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_6a683f07b0b34d59b2ce4dcd74b1aa7e = L.marker(
            [43.67033770000001, -79.45579529999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b40073bb9a8e4df6a2d5813ba8915e66 = L.marker(
            [43.7222824, -79.2621231],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_3524a8974f5541bc94b75fed19e24f62 = L.marker(
            [43.7407112, -79.4388733],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_6e0504c85fca44aabc5d05e69ad177a3 = L.marker(
            [43.6801109, -79.2925949],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_fb2a26530ac94d95bc940d9332a2e09e = L.marker(
            [43.6918983, -79.4562531],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_fe2b4f2a48884a9e8c8dc701c7235e29 = L.marker(
            [43.6862335, -79.3933334],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_dc8d063bc26540fc9bf44581d2179c00 = L.marker(
            [43.7482834, -79.1958313],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_08dd346cf2894247ab3f10c9bd3fad81 = L.marker(
            [43.8042259, -79.1687851],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_295bb3f2b2cb47bc80fe75803d5659a0 = L.marker(
            [43.6664314, -79.376503],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_cb495bf53fcf4864b2fd3ac12d02c685 = L.marker(
            [43.746933, -79.4344177],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b376bfd1778844158d664ca299644f37 = L.marker(
            [43.7290611, -79.4459915],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_d5293ccbfc2d4a69bc1b6802cc5936cd = L.marker(
            [43.65922929999999, -79.5141373],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_6a0ed45e87254365a6f49a9b91166f25 = L.marker(
            [43.7354164, -79.3471375],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_53ad3a3ec1b94ee787ad278de87f9acf = L.marker(
            [43.674408, -79.4337845],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_9b3050f87df14cc0adb87e98b3f83d6a = L.marker(
            [43.6142044, -79.51727290000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_c15f50338e6a4cb8b8e8c6dc1d2a0404 = L.marker(
            [43.695816, -79.5627365],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_34e602a3ada04a1fb0b425811f183138 = L.marker(
            [43.5936623, -79.5370026],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_3cdfc377bb304a70ae5bd241174e623b = L.marker(
            [43.7816391, -79.4552994],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_6d917247aeed4a62b53247259710fee4 = L.marker(
            [43.7336807, -79.2270126],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_3b20449bbfc34ff6bce4297307615224 = L.marker(
            [43.7276535, -79.5495377],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_389c802579554c399635db0ee3289cd1 = L.marker(
            [43.684288, -79.3192368],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_df7147c91c55499ea551e4299e1aafdb = L.marker(
            [43.6056442, -79.538269],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_c72d6d0ee63b40c982ce8d9dfb61314e = L.marker(
            [43.65161129999999, -79.4740906],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_72c569c94ad348648c1fd88cef4d9542 = L.marker(
            [43.67752460000001, -79.3586502],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_9e664bf1e7b443c8a14c727d00210783 = L.marker(
            [43.77435300000001, -79.4998016],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_d56880b30da549f3be580e0135df6500 = L.marker(
            [43.63097379999999, -79.481514],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_ecc4302f2f00491db4a77e179ad246e4 = L.marker(
            [43.7074776, -79.34362790000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_2ef9158ec8bb4cbeb4605cb3796451b4 = L.marker(
            [43.6716728, -79.4937515],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_87f2b0b20ba344ae8e88d76f221d4a7a = L.marker(
            [43.6785393, -79.54489140000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_9b13a7077ce44c35acda7a27a7c2aac3 = L.marker(
            [43.6973763, -79.3964691],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_ab9854d92c944ddba3b7754b87a2feea = L.marker(
            [43.7051086, -79.3749313],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_19adebcd542f480197cbfd8efb5407c3 = L.marker(
            [43.627346, -79.5623703],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_4082ab6bcf494d799cdd47cf5d89b06b = L.marker(
            [43.6708565, -79.4751968],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_43a335f32aef4cf1b8885a5c3e9cefd7 = L.marker(
            [43.6868706, -79.4685745],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_2e39ff4b6fce4c9da1393a4700d187b9 = L.marker(
            [43.7436333, -79.57647709999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_38b3526f1c9b4125bf479bbc312e0166 = L.marker(
            [43.766407, -79.42574309999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_8286afe2209545bd957d24937e42b73e = L.marker(
            [43.7276649, -79.40623470000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_c09a4fd1bb4547419ab22a37608515a1 = L.marker(
            [43.6777802, -79.45579529999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_0486997371704fcab58aded1e7cbb9ce = L.marker(
            [43.694545700000006, -79.3352661],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_5aae7b361ae146538bad6cd08d7cdac4 = L.marker(
            [43.6729698, -79.342392],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_ed13df694811443bb140f10a1e59a566 = L.marker(
            [43.6854134, -79.4192352],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_9e8df13dd24b4847becc68f0c72d2b9f = L.marker(
            [43.7075539, -79.50511170000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_5466dd1f69d744cf8a9b781c39e062c7 = L.marker(
            [43.6677322, -79.3103333],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_5b66b2a5ffa94d7fb5e763b89781ca34 = L.marker(
            [43.6473045, -79.4327087],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_e83b3dcfd51f40088e433cd99b5ff2c5 = L.marker(
            [43.65834810000001, -79.4347763],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_0b672cd3508d461689be21e2a1f0494c = L.marker(
            [43.68095020000001, -79.39377590000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_bc17030fd8344b708b63d102fc89934b = L.marker(
            [43.6655273, -79.4992065],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_b87aa73f8f26425fbd25ecc072f16892 = L.marker(
            [43.79071810000001, -79.15214540000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_47fcf48131f84d779958a5f46418e36d = L.marker(
            [43.686058, -79.4247742],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_3a5bd7d7b66647a88a6c4beb90f3bf96 = L.marker(
            [43.6605263, -79.4146652],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_7bd75eb70a774534820df13c26e7cacb = L.marker(
            [43.6733017, -79.55154420000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_853a30321c644608ba52c4601d52848b = L.marker(
            [43.6384354, -79.56092070000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_7bb6d784b24a47cdab57416e3ba4b3a5 = L.marker(
            [43.7911301, -79.3933563],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
        var marker_7ccf891befef4c50b0f7fce5fe0a1894 = L.marker(
            [43.651577, -79.603096],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_ff3e9cde803f49a0be446ab0a3e288fb);
        
    
            var heat_map_79896b6cb8e14c59b418bf00749b5fa3 = L.heatLayer(
                [[43.6684494, -79.3430939, 514.0], [43.759285, -79.50792690000002, 2006.0], [43.69755170000001, -79.5016632, 829.0], [43.7217026, -79.5715103, 694.0], [43.6638908, -79.50348659999999, 193.0], [43.65730670000001, -79.3734589, 3609.0], [43.6663628, -79.31660459999998, 648.0], [43.65811160000001, -79.4020233, 1293.0], [43.768856, -79.4669266, 951.0], [43.80780410000001, -79.21559909999998, 2024.0], [43.641552, -79.4745407, 647.0], [43.69513320000001, -79.3034821, 853.0], [43.65588, -79.3632202, 980.0], [43.6639061, -79.38415529999997, 6301.0], [43.6500702, -79.3968811, 3263.0], [43.699131, -79.2543335, 1411.0], [43.7494888, -79.5327911, 968.0], [43.6724319, -79.33419040000003, 1949.0], [43.77361679999999, -79.2611313, 2277.0], [43.6878204, -79.43493649999998, 788.0], [43.65807720000001, -79.3847122, 3564.0], [43.6449547, -79.397644, 5674.0], [43.7564392, -79.3607712, 916.0], [43.663723, -79.3705215, 1119.0], [43.7666092, -79.1856613, 2857.0], [43.6943665, -79.2736511, 1427.0], [43.7439919, -79.5988693, 4338.0], [43.7630234, -79.3175278, 2350.0], [43.768837, -79.37837979999998, 798.0], [43.7460785, -79.3886261, 420.0], [43.7565994, -79.5120697, 1981.0], [43.6659164, -79.4074707, 2908.0], [43.768383, -79.3490372, 472.0], [43.7333374, -79.26329040000002, 804.0], [43.7621841, -79.3211517, 1377.0], [43.8115196, -79.3145447, 687.0], [43.7768669, -79.31622309999999, 1016.0], [43.7383575, -79.3097839, 594.0], [43.7524796, -79.5607376, 1590.0], [43.7429314, -79.21129609999998, 1241.0], [43.6823387, -79.5266037, 521.0], [43.7751579, -79.4143829, 1867.0], [43.7130508, -79.41177370000003, 468.0], [43.7775917, -79.22657779999999, 3158.0], [43.7170258, -79.5372925, 692.0], [43.7412338, -79.2372665, 1804.0], [43.7068634, -79.32061, 970.0], [43.6030045, -79.50748440000002, 841.0], [43.7165794, -79.3303757, 1001.0], [43.7915154, -79.2981033, 1570.0], [43.6412277, -79.42667390000003, 1272.0], [43.6932449, -79.50466920000002, 870.0], [43.6844292, -79.3469238, 464.0], [43.7899132, -79.217453, 871.0], [43.6859818, -79.3562927, 299.0], [43.6989937, -79.43503570000001, 568.0], [43.7072258, -79.2952805, 2163.0], [43.7335815, -79.4837265, 2974.0], [43.6446571, -79.5307999, 2589.0], [43.7689018, -79.28555300000002, 1756.0], [43.8019333, -79.3593979, 687.0], [43.69834520000001, -79.5113144, 1410.0], [43.7797699, -79.41557309999997, 1275.0], [43.6962624, -79.44768520000002, 1117.0], [43.732395200000006, -79.41941070000001, 1135.0], [43.6507835, -79.41349790000002, 1217.0], [43.7003136, -79.38697049999998, 413.0], [43.8012695, -79.29660030000002, 1414.0], [43.7054634, -79.40009309999998, 325.0], [43.6684685, -79.4390335, 2450.0], [43.6924973, -79.31573490000002, 434.0], [43.8110886, -79.26600649999997, 1008.0], [43.6661034, -79.587677, 676.0], [43.7035904, -79.45242309999998, 1571.0], [43.6831703, -79.41830440000003, 457.0], [43.6403351, -79.4379196, 1416.0], [43.77856060000001, -79.3287048, 421.0], [43.69122700000001, -79.47039790000002, 615.0], [43.67886729999999, -79.3268204, 885.0], [43.74681470000001, -79.58380890000002, 2257.0], [43.7865829, -79.18830870000002, 816.0], [43.6508102, -79.5223618, 454.0], [43.7205162, -79.4807281, 459.0], [43.78576279999999, -79.28945920000002, 1331.0], [43.7876511, -79.35165409999998, 1285.0], [43.7738609, -79.4430084, 1078.0], [43.63902279999999, -79.41591640000001, 1324.0], [43.67033770000001, -79.45579529999998, 704.0], [43.7222824, -79.2621231, 1577.0], [43.7407112, -79.4388733, 735.0], [43.6801109, -79.2925949, 1691.0], [43.6918983, -79.4562531, 328.0], [43.6862335, -79.3933334, 1628.0], [43.7482834, -79.1958313, 402.0], [43.8042259, -79.1687851, 1940.0], [43.6664314, -79.376503, 1530.0], [43.746933, -79.4344177, 643.0], [43.7290611, -79.4459915, 873.0], [43.65922929999999, -79.5141373, 473.0], [43.7354164, -79.3471375, 938.0], [43.674408, -79.4337845, 623.0], [43.6142044, -79.51727290000002, 2040.0], [43.695816, -79.5627365, 1116.0], [43.5936623, -79.5370026, 557.0], [43.7816391, -79.4552994, 837.0], [43.7336807, -79.2270126, 1014.0], [43.7276535, -79.5495377, 581.0], [43.684288, -79.3192368, 1191.0], [43.6056442, -79.538269, 460.0], [43.65161129999999, -79.4740906, 1057.0], [43.67752460000001, -79.3586502, 709.0], [43.77435300000001, -79.4998016, 3141.0], [43.63097379999999, -79.481514, 942.0], [43.7074776, -79.34362790000002, 746.0], [43.6716728, -79.4937515, 1227.0], [43.6785393, -79.54489140000003, 1081.0], [43.6973763, -79.3964691, 1235.0], [43.7051086, -79.3749313, 507.0], [43.627346, -79.5623703, 293.0], [43.6708565, -79.4751968, 1043.0], [43.6868706, -79.4685745, 503.0], [43.7436333, -79.57647709999998, 612.0], [43.766407, -79.42574309999998, 548.0], [43.7276649, -79.40623470000001, 445.0], [43.6777802, -79.45579529999998, 818.0], [43.694545700000006, -79.3352661, 600.0], [43.6729698, -79.342392, 425.0], [43.6854134, -79.4192352, 360.0], [43.7075539, -79.50511170000001, 712.0], [43.6677322, -79.3103333, 853.0], [43.6473045, -79.4327087, 703.0], [43.65834810000001, -79.4347763, 602.0], [43.68095020000001, -79.39377590000002, 234.0], [43.6655273, -79.4992065, 535.0], [43.79071810000001, -79.15214540000002, 311.0], [43.686058, -79.4247742, 371.0], [43.6605263, -79.4146652, 788.0], [43.6733017, -79.55154420000002, 363.0], [43.6384354, -79.56092070000003, 450.0], [43.7911301, -79.3933563, 428.0], [43.651577, -79.603096, 23.0]],
                {
                    minOpacity: 0.2,
                    maxZoom: 11,
                    max: 6301,
                    radius: 30,
                    blur: 20,
                    gradient: null
                    })
                .addTo(map_848f32327bb547c2ba242552f380edf9);
        
</script>&#34; style=&#34;position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/div&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np


import matplotlib.pyplot as plt
import seaborn as sns

import re #regular expression matching operations

import folium #maping crime on the map
from folium.plugins import HeatMap, MarkerCluster #making maping visually appealing

%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h2&gt;

&lt;p&gt;I downloaded the original csv file &lt;code&gt;MCI_2014_to_2018.csv&lt;/code&gt;from &lt;a href=&#34;http://data.torontopolice.on.ca/datasets/mci-2014-to-2018&#34; target=&#34;_blank&#34;&gt;http://data.torontopolice.on.ca/datasets/mci-2014-to-2018&lt;/a&gt;. I preprocessed the data and selected only the variables of my interest and saved the file as &lt;code&gt;toronto_crime.csv&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;crime = pd.read_csv(&#39;toronto_crime.csv&#39;)
crime = crime.drop(columns = &amp;quot;Unnamed: 0&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The original data set already breaks down the &lt;code&gt;occurrencedate&lt;/code&gt; (yyy-MM-dd HH”T”:mm:ss.SSS’Z’) into marginal time categories, i.e. year, month, hour, etc. However, I wanted to show how easily Python and pandas can generate these variables from the string format.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;crime[&amp;quot;datetime&amp;quot;] = pd.to_datetime(crime[&amp;quot;occurrencedate&amp;quot;])

crime[&#39;year&#39;] = crime[&#39;datetime&#39;].dt.year
crime[&#39;month&#39;] = crime[&#39;datetime&#39;].dt.month
crime[&#39;dayofweek&#39;] = crime[&#39;datetime&#39;].dt.dayofweek
crime[&#39;hour&#39;] = crime[&#39;datetime&#39;].dt.hour
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;crime[&amp;quot;MCI&amp;quot;] = crime[&amp;quot;MCI&amp;quot;].astype(&#39;category&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am not presenting EDA of the date in this post. If you wish to see some of the methods I have used, I recommend you read my previous post on &lt;a href=&#34;https://www.jiristodulka.com/post/machine/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;“Machine Learning and Diabetes”&lt;/strong&gt;&lt;/a&gt;, where I disclose some of the frequently used commands.&lt;/p&gt;

&lt;h1 id=&#34;crime-composition-in-toronto&#34;&gt;Crime Composition in Toronto&lt;/h1&gt;

&lt;p&gt;Technically, visualizing the crime composition in the City if Toronto was the most difficult part of my work. Initially, my aim was to make the following visualization of every &lt;code&gt;MCI&lt;/code&gt;. However, as you can see in the following code, this would take a lot of space. Therefore, I have decided to focus only on:
* Crime in general
* Different categories of assault in offence.&lt;/p&gt;

&lt;p&gt;Additionally, I wanted to share an appealing visualization rather than pie charts (I originally started with). I admit, I had a lot of fun while playing with the following code. &lt;strong&gt;The whole objective with preprocessing data was to store values and labels separately in the same order&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Therefore, the following section has three parts:
1.  Generating values and labels for Crimes
2.  Generating values and labels for Assaluts
3.  &lt;strong&gt;Visualizing Crime and Assaults&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since I am obsessed with writing neat and clean codes, or at least I always try my best, I wonder if you can come up with a more efficient solution? If so, can you kindly share it with me?&lt;/p&gt;

&lt;h2 id=&#34;1-values-and-labels-crime&#34;&gt;1. Values and Labels: Crime&lt;/h2&gt;

&lt;p&gt;Obtaining values and labels from crime was the easiest step and I do not think I need to explain the logic behind it. The process is as simple as &amp;ldquo;ask for values&amp;rdquo; and &amp;ldquo;ask for labels&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;values_crime = crime[&amp;quot;MCI&amp;quot;].value_counts() 
labels_crime = crime[&amp;quot;MCI&amp;quot;].value_counts().keys()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-values-and-labels-assault&#34;&gt;2. Values and Labels: Assault&lt;/h2&gt;

&lt;p&gt;However, coming up with the values and labels for assaults was a different story and it took me a while before I came up with and realized the &lt;strong&gt;Step 3.&lt;/strong&gt; .&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; I filtered for rows with any form of &amp;ldquo;ASSAULT&amp;rdquo; in &lt;code&gt;offence&lt;/code&gt; variable. I called the filtered df &lt;code&gt;crime_assault&lt;/code&gt;. You can notice that I specified the selection to be case insensitive by &lt;code&gt;flags=re.IGNORECASE&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; I counted the values in &lt;code&gt;crime_assault&lt;/code&gt;, i.e. &amp;ldquo;How many times each criminal act classified as assault appears in the data&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; This step was actually not necessary. However, when you look at the chart below, you should notice the category &lt;code&gt;other&lt;/code&gt;. In fact, it consolidates the other three types of assaults: *Peace Officer Wpn/Cbh, Force/Thrt/Impede, and Aggravated Assault Avails Pros in the range from 251 to 12. As you can see, the values are marginal (compared to Assaults of 62194). Since the categories were overlaying in the initial plot, I have decided to consolidate them. I iterated over the rows in &lt;code&gt;values_assault&lt;/code&gt; with the aim to &lt;strong&gt;rename&lt;/strong&gt; the key of the value in &lt;code&gt;offence&lt;/code&gt; smaller than 1500. I had to save it as &lt;code&gt;pd.DataFrame&lt;/code&gt; object because of the following step.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Since the purpose of the whole procedure was to plot the data with labels and values, it was essential to store the values in the exact index order as the labels: &lt;code&gt;sort_values(&amp;quot;offence&amp;quot;,ascending=False)&lt;/code&gt;. In this step, I was grouping the data according to &lt;code&gt;index&lt;/code&gt;. It is because &lt;code&gt;values_assault&lt;/code&gt; was originally a Series object. However, after I stored it as &lt;code&gt;pd.DataFrame&lt;/code&gt; object, I had to use &lt;code&gt;index&lt;/code&gt; because &lt;code&gt;key&lt;/code&gt; is strictly associated with Series objects.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; I saved the index values, i.e. the categories of assault, as strings.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;To wrap it up, the goal of these 5 steps was to store the values and labels separately in the same order to facilitate visualization.&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;crime_assault = crime[crime[&amp;quot;offence&amp;quot;].str.contains(&#39;ASSAULT&#39;, flags=re.IGNORECASE, regex=True)] #Step 1.

values_assault = crime_assault[&amp;quot;offence&amp;quot;].value_counts() #Step 2.

for key,value in values_assault.iteritems(): #Step 3.
    if value &amp;lt; 1500:
       values_assault= pd.DataFrame(values_assault.rename({key: &amp;quot;other&amp;quot;}))


values_assault=values_assault.groupby(values_assault.index).sum().sort_values(&amp;quot;offence&amp;quot;,ascending=False) #Step 4.

labels_assault = values_assault.index #Step 5.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;visualization&#34;&gt;Visualization&lt;/h2&gt;

&lt;p&gt;As I already said, I wanted to generate an appealing visualization. I took inspiration from &lt;strong&gt;Kevin Amipara&lt;/strong&gt; and his article called &lt;a href=&#34;https://medium.com/@kvnamipara/a-better-visualisation-of-pie-charts-by-matplotlib-935b7667d77f&#34; target=&#34;_blank&#34;&gt;“A better visualization of Pie charts by MatPlotLib”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The only changes I have made was to i.) plotting the two plots in subplots, and ii.) adding the legends.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(num=None, figsize=(15, 12))


##############################   Crime    ################################
plt.subplot(1,2,1)
plt.pie(values_crime, autopct=&#39;%1.1f%%&#39;, pctdistance=0.85, startangle=90, explode = [0.05]*labels_crime.shape[0])

    #draw circle
centre_circle = plt.Circle((0,0),0.70,fc=&#39;white&#39;)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

    #title+legend
plt.title(&amp;quot;Share of Criminal Offences in Toronto&amp;quot;,size=20)
plt.legend(labels_crime,loc=2)

    # Equal aspect ratio ensures that pie is drawn as a circle
plt.axis(&#39;equal&#39;)  
plt.tight_layout()

##############################   Assault   ################################
plt.subplot(1,2,2)
plt.pie(values_assault, autopct=&#39;%1.1f%%&#39;,pctdistance=0.85, startangle=90, explode = [0.05]*labels_assault.shape[0])
    
    #draw circle
centre_circle = plt.Circle((0,0),0.70,fc=&#39;white&#39;)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

    #title+legend
plt.title(&amp;quot;Share of Assaults in Toronto&amp;quot;,size=20)
plt.legend(labels_assault,loc=1)

    # Equal aspect ratio ensures that pie is drawn as a circle
plt.axis(&#39;equal&#39;)  
plt.tight_layout()

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./2019-06-27-toronto-crime-and-folium_20_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see from the left chart (Criminal Offences), Torontians experience assaults most of the time. Assaults represent over 50% of the criminal activity in the city. Furthermore, “Break and Enter” represents 21% of the activity.&lt;/p&gt;

&lt;p&gt;Looking at the right plot (Assaults), you can infer more interesting facts. Firstly, 17% of the assaults are conducted with a weapon. Specifically, if you happened to be assaulted in Toronto, there is a 17% chance you were threaten by weapon. Unfortunately, I cannot infer the share of fire guns regarding these assaults. And according to “Assault Bodily Harm”, a person assaulted is 5% more likely to be bodily harmed.&lt;/p&gt;

&lt;h2 id=&#34;mapping-crime&#34;&gt;Mapping Crime&lt;/h2&gt;

&lt;p&gt;Since my family is visiting me soon from Europe, I was already concerned for their safety. I thought it would be effective to find places with high criminal density, so I can avoid them with my family. There are two ways in which you can do it:
* 1. Utilize longitude and latitude coordinates as axis
* 2. Use a model (e.g.f&lt;code&gt;folium&lt;/code&gt;) and plot crime on a map&lt;/p&gt;

&lt;h3 id=&#34;longitude-and-latitude-on-x-and-y-axis&#34;&gt;Longitude and Latitude on x and y Axis&lt;/h3&gt;

&lt;p&gt;This is the easiest method of how to inspect criminality in the map. Simply put, &lt;code&gt;Long&lt;/code&gt; and &lt;code&gt;Lat&lt;/code&gt; are nothing but the coordinates. Given the high density of crime in the last four years, plotting crimes on a scatter plot should form a coherent map of the City of Toronto. Notice that I played with the arguments in &lt;code&gt;plt.scatter()&lt;/code&gt;. The density was so high that I had to reduce dot sizes and decrease transparency as much as possible. The process required some trial and error method. Consequentially, one can infer several things from such simple mapping.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(num=None, figsize=(10, 8))
plt.scatter(&amp;quot;Long&amp;quot;, &amp;quot;Lat&amp;quot;, data = crime, c = &#39;y&#39;,alpha = 0.1, edgecolor = &#39;black&#39;, s=2)
plt.grid()
plt.xlabel(&#39;long&#39;)
plt.ylabel(&#39;lat&#39;)
plt.title(&#39;Toronto Crime&#39;)
plt.tight_layout()
plt.axis(&#39;tight&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./2019-06-27-toronto-crime-and-folium_25_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Firstly, there are a few high-density areas on the map. The most evident one is in the south of the City – downtown. The neighborhoods around also exhibit high criminal activity and there are also other spots suggesting criminal neighborhoods can be clustered. Contrary, the white spots are green - park - areas.
You can also infer that crime appears mostly alongside the major roads. Technically, you can observe every major street, avenue, and road in the City. Additionally, you can clearly see, the Young Street heading North from Downtown. Did you also know that Young St. is said to be the longest street in the world? Remember, these are just dots in the scatter plot.&lt;/p&gt;

&lt;h2 id=&#34;neighborhoods-you-should-avoid-in-toronto&#34;&gt;Neighborhoods You Should avoid in Toronto&lt;/h2&gt;

&lt;p&gt;The following section supports my previous statement that criminal neighborhoods can be clustered. In the following code, I grouped the data by &lt;code&gt;Neighbourhood&lt;/code&gt; (with 141 discrete values), and counted &lt;code&gt;MCI&lt;/code&gt; in each district, i.e. &lt;code&gt;top_N&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then, I dropped the duplicates in &lt;code&gt;Neighbourhood&lt;/code&gt; in &lt;code&gt;crime&lt;/code&gt;, and got &lt;code&gt;Long&lt;/code&gt; and &lt;code&gt;Lat&lt;/code&gt; coordinates. I joined the data with &lt;code&gt;top_N&lt;/code&gt;. As a result, I obtained the &lt;code&gt;map_data&lt;/code&gt; DataFrame object, where each row (with &lt;code&gt;Neighbourhood&lt;/code&gt; index) records &lt;code&gt;Lat&lt;/code&gt;, &lt;code&gt;Lon&lt;/code&gt;, and &lt;code&gt;MCI&lt;/code&gt; accordingly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Top N Criminal Neighbourhoods in Toronto 
top_N = crime.groupby(&#39;Neighbourhood&#39;)[[&#39;MCI&#39;]].count().sort_values(by=[&#39;MCI&#39;])


# Coordinates Criminal Neighbourhoods
map_data = crime[[&#39;Neighbourhood&#39;, &#39;Lat&#39;, &#39;Long&#39;]].drop_duplicates(&#39;Neighbourhood&#39;).set_index(&#39;Neighbourhood&#39;) \
    .join(top_N, how=&#39;inner&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the most criminal neighborhoods in Toronto bellow:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;map_data.sort_values(by=[&#39;MCI&#39;], ascending=False).head(10)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Lat&lt;/th&gt;
      &lt;th&gt;Long&lt;/th&gt;
      &lt;th&gt;MCI&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Neighbourhood&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Church-Yonge Corridor (75)&lt;/th&gt;
      &lt;td&gt;43.663906&lt;/td&gt;
      &lt;td&gt;-79.384155&lt;/td&gt;
      &lt;td&gt;6301&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Waterfront Communities-The Island (77)&lt;/th&gt;
      &lt;td&gt;43.644955&lt;/td&gt;
      &lt;td&gt;-79.397644&lt;/td&gt;
      &lt;td&gt;5674&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;West Humber-Clairville (1)&lt;/th&gt;
      &lt;td&gt;43.743992&lt;/td&gt;
      &lt;td&gt;-79.598869&lt;/td&gt;
      &lt;td&gt;4338&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Moss Park (73)&lt;/th&gt;
      &lt;td&gt;43.657307&lt;/td&gt;
      &lt;td&gt;-79.373459&lt;/td&gt;
      &lt;td&gt;3609&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Bay Street Corridor (76)&lt;/th&gt;
      &lt;td&gt;43.658077&lt;/td&gt;
      &lt;td&gt;-79.384712&lt;/td&gt;
      &lt;td&gt;3564&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Kensington-Chinatown (78)&lt;/th&gt;
      &lt;td&gt;43.650070&lt;/td&gt;
      &lt;td&gt;-79.396881&lt;/td&gt;
      &lt;td&gt;3263&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Woburn (137)&lt;/th&gt;
      &lt;td&gt;43.777592&lt;/td&gt;
      &lt;td&gt;-79.226578&lt;/td&gt;
      &lt;td&gt;3158&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;York University Heights (27)&lt;/th&gt;
      &lt;td&gt;43.774353&lt;/td&gt;
      &lt;td&gt;-79.499802&lt;/td&gt;
      &lt;td&gt;3141&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Downsview-Roding-CFB (26)&lt;/th&gt;
      &lt;td&gt;43.733581&lt;/td&gt;
      &lt;td&gt;-79.483727&lt;/td&gt;
      &lt;td&gt;2974&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Annex (95)&lt;/th&gt;
      &lt;td&gt;43.665916&lt;/td&gt;
      &lt;td&gt;-79.407471&lt;/td&gt;
      &lt;td&gt;2908&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Finally, I could use &lt;code&gt;map_data&lt;/code&gt; DataFrame alongside with &lt;code&gt;folium&lt;/code&gt; and visualize criminality in neighborhoods as clusters with the heatmap.&lt;/p&gt;

&lt;p&gt;The process follows a certain logic:
1. create a Map (&lt;code&gt;m&lt;/code&gt;) with using longitude and latitude of the place of your interest.
2. if you want to add a marker or other feature from &lt;code&gt;folium.plugins&lt;/code&gt;, generate the object and use &lt;code&gt;add_to(M)&lt;/code&gt; where M usually represents &lt;code&gt;folium.Map()&lt;/code&gt; object or other module, e.g. &lt;code&gt;HeatMap()&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Step 1: Creating &amp;amp; adding clustering functionality, i.e. &lt;code&gt;MarkerCluster()&lt;/code&gt;, to the map (&lt;code&gt;m&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Step 2: Creating &amp;amp; adding &lt;code&gt;Marker&lt;/code&gt; for every row, i.e. neighborhood, based on &lt;code&gt;Long&lt;/code&gt; and &lt;code&gt;Lat&lt;/code&gt; to the cluster (&lt;code&gt;cluster&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Step 3: Creating &amp;amp; adding &lt;code&gt;HeatMap&lt;/code&gt;to the map (&lt;code&gt;m&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, I could use &lt;code&gt;map_data&lt;/code&gt; DataFrame along with &lt;code&gt;folium&lt;/code&gt; and visualize criminality in the neighborhoods as clusters within the heatmap.&lt;/p&gt;

&lt;p&gt;The process follows a certain logic:
1. Create a Map (&lt;code&gt;m&lt;/code&gt;) with using longitude and latitude of the place of your interest.
2. If you want to add a marker or another feature from &lt;code&gt;folium.plugins&lt;/code&gt;, generate the object and use &lt;code&gt;add_to(M)&lt;/code&gt; where M usually represents &lt;code&gt;folium.Map()&lt;/code&gt; object or other module, e.g. &lt;code&gt;HeatMap()&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Step 1: Creating &amp;amp; adding clustering functionality, i.e. &lt;code&gt;MarkerCluster()&lt;/code&gt;, to the map (&lt;code&gt;m&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Step 2: Creating &amp;amp; adding Marker for every row, i.e. neighborhood, based on Long and Lat to the cluster (&lt;code&gt;cluster&lt;/code&gt;).&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Step 3: Creating &amp;amp; adding &lt;code&gt;HeatMap&lt;/code&gt; to the map (&lt;code&gt;m&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Mapping Criminal Neighbourhoods
m = folium.Map(
location=[43.702270, -79.366074],
zoom_start=11
)

#Step 1: Clusters
cluster = MarkerCluster().add_to(m)

#Step 2: Clusters breaking into Markers
for x in map_data.iterrows():
folium.Marker([x[1].Lat, x[1].Long]).add_to(cluster)
    
#Step 3: Heat
max_crime = map_data[&#39;MCI&#39;].max() # max value as reference for the darkets shade

heat = HeatMap(map_data.values,
            min_opacity=0.2,
            max_val=max_crime,
            radius=30, blur=20, 
            max_zoom=11)

heat.add_to(m)

m # call m to see the heat map with clusters
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;width:100%;&#34;&gt;&lt;div style=&#34;position:relative;width:100%;height:0;padding-bottom:60%;&#34;&gt;&lt;iframe src=&#34;data:text/html;charset=utf-8;base64,<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <script>L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;</script>
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
    <meta name="viewport" content="width=device-width,
        initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <style>#map_c89271cfdd6b41caae2b868221f7a07b {
        position: relative;
        width: 100.0%;
        height: 100.0%;
        left: 0.0%;
        top: 0.0%;
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/leaflet.markercluster.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.Default.css"/>
    <script src="https://leaflet.github.io/Leaflet.heat/dist/leaflet-heat.js"></script>
</head>
<body>    
    
    <div class="folium-map" id="map_c89271cfdd6b41caae2b868221f7a07b" ></div>
</body>
<script>    
    
    
        var bounds = null;
    

    var map_c89271cfdd6b41caae2b868221f7a07b = L.map(
        'map_c89271cfdd6b41caae2b868221f7a07b', {
        center: [43.70227, -79.366074],
        zoom: 11,
        maxBounds: bounds,
        layers: [],
        worldCopyJump: false,
        crs: L.CRS.EPSG3857,
        zoomControl: true,
        });


    
    var tile_layer_828a7cd2310345c9ad56a1d562363bb7 = L.tileLayer(
        'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',
        {
        "attribution": null,
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_c89271cfdd6b41caae2b868221f7a07b);
    
            var marker_cluster_af17f584e0d846fcbc0515d0680e2fa9 = L.markerClusterGroup({});
            map_c89271cfdd6b41caae2b868221f7a07b.addLayer(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
            
    
        var marker_829359ccef134399b07dae94860910d7 = L.marker(
            [43.6684494, -79.3430939],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_7b0b7c692fc845bf8d06110d2748cc42 = L.marker(
            [43.759285, -79.50792690000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e6108254cbc44bb696ada35068f750ad = L.marker(
            [43.69755170000001, -79.5016632],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_66364942e9c846cb9025addea7bb9edc = L.marker(
            [43.7217026, -79.5715103],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_796ce93f36214d39b83c29ac78e0edd9 = L.marker(
            [43.6638908, -79.50348659999999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e8b21565cc9348e5a9a229884e431779 = L.marker(
            [43.65730670000001, -79.3734589],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_697cc9612e5146fdb3261cd4517eb74d = L.marker(
            [43.6663628, -79.31660459999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c3bcd1d556254de2ad2f5d63274e2fef = L.marker(
            [43.65811160000001, -79.4020233],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d15c9dae96564f789ee3fa71e87a4b44 = L.marker(
            [43.768856, -79.4669266],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_069d8ce897ac43f9b7ebfa85768ae61b = L.marker(
            [43.80780410000001, -79.21559909999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d557b9a75586479eacaa433f6ad39c5f = L.marker(
            [43.641552, -79.4745407],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_8b6b44d41faa494e9a7abdb77489db25 = L.marker(
            [43.69513320000001, -79.3034821],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_8c32d0502c1e4edeab25f68411f9395e = L.marker(
            [43.65588, -79.3632202],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_aaa6aeb3b6e34f8b810ddb02589fda18 = L.marker(
            [43.6639061, -79.38415529999997],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_1e9e5cd85b6c4512980d25f93bfd2228 = L.marker(
            [43.6500702, -79.3968811],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_f3440f44e425470ba194baaa12e7c947 = L.marker(
            [43.699131, -79.2543335],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_904b88b249ba43cba7407f47b3b628a9 = L.marker(
            [43.7494888, -79.5327911],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_731151bf3a064c788874853505eab08e = L.marker(
            [43.6724319, -79.33419040000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e9b72d5f69dc4423ab8cebb1b6054256 = L.marker(
            [43.77361679999999, -79.2611313],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_31dcc311dc7f4b3fac8a58f3dd7f045e = L.marker(
            [43.6878204, -79.43493649999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_8ede9d976abd43bda33942ef1aaf7c52 = L.marker(
            [43.65807720000001, -79.3847122],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_f4e047bb6a6744aba30b30de1a658054 = L.marker(
            [43.6449547, -79.397644],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_65ba3c4954d8475cbb72cca636db0a16 = L.marker(
            [43.7564392, -79.3607712],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_5aae9c6328774ccf81523a74241d10f5 = L.marker(
            [43.663723, -79.3705215],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_3b62b85a173d4b619184ac70e1f2e5af = L.marker(
            [43.7666092, -79.1856613],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_ca830910016d475a9177fbade3a2653c = L.marker(
            [43.6943665, -79.2736511],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_ed1e453dc8334f33977da6b81db36564 = L.marker(
            [43.7439919, -79.5988693],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_3e130e0d9da8408aa23391eaea7c82fe = L.marker(
            [43.7630234, -79.3175278],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_04b329379f32486990239ef29dad08ab = L.marker(
            [43.768837, -79.37837979999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c6302a0f4814433ba56de9e169351093 = L.marker(
            [43.7460785, -79.3886261],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_4c9086b6c2fd48008f98e9db68cc7393 = L.marker(
            [43.7565994, -79.5120697],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_28ed3eb1199c4652a0c365ca09efbbb6 = L.marker(
            [43.6659164, -79.4074707],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_25fd50565e8e4fc08871e863449dfbee = L.marker(
            [43.768383, -79.3490372],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_dc0dd140e7084c2f8e7a8a3e663e7b04 = L.marker(
            [43.7333374, -79.26329040000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_fc2544b427074545bc59d4c47019f3e9 = L.marker(
            [43.7621841, -79.3211517],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e276951ccac8429c804bacedb375e2c6 = L.marker(
            [43.8115196, -79.3145447],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_b4b41a5fccc34eb9b02300cdd4e5aa2e = L.marker(
            [43.7768669, -79.31622309999999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d4f79598ca6549c3b6b5ee1be0d93bdd = L.marker(
            [43.7383575, -79.3097839],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c9c56ff43fa6404da85c50f69742aad4 = L.marker(
            [43.7524796, -79.5607376],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_61c71b58f7924071b4471a73b3c7ce85 = L.marker(
            [43.7429314, -79.21129609999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d70a70d0771742329464b9f5c87636b9 = L.marker(
            [43.6823387, -79.5266037],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_53a9a7b128ba42b692c5f6c5f3b4c5ed = L.marker(
            [43.7751579, -79.4143829],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_0e6e9cec59ba4eee9340522c562355e4 = L.marker(
            [43.7130508, -79.41177370000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_aec82b2772c2400d968e512c8316185a = L.marker(
            [43.7775917, -79.22657779999999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_6ce88d21171f4228aa34a35a6f67d1fb = L.marker(
            [43.7170258, -79.5372925],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_0733c6382e56479ebca5926141c0fc72 = L.marker(
            [43.7412338, -79.2372665],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e2f4ce5c6d6f4b588053591080119d9b = L.marker(
            [43.7068634, -79.32061],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_71a37fda482148fd9599e901e4d45d45 = L.marker(
            [43.6030045, -79.50748440000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c2c7d2f65ff24ad1b96ec53b564174ea = L.marker(
            [43.7165794, -79.3303757],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_993ec59a078f47b68a0a467dd634c689 = L.marker(
            [43.7915154, -79.2981033],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_1a326de4c5cc40e0b1063751b7778dc8 = L.marker(
            [43.6412277, -79.42667390000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_43f1495a8abc4647bae73b67634ba133 = L.marker(
            [43.6932449, -79.50466920000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_264886b19991409eaf975088b850e6f9 = L.marker(
            [43.6844292, -79.3469238],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_f7c07d0dad984eba9dd4a6b84ee68050 = L.marker(
            [43.7899132, -79.217453],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c2544d5d99ed418d8ab5056aed58dbb2 = L.marker(
            [43.6859818, -79.3562927],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_9cd057c7d59c438f8440d03914d21d82 = L.marker(
            [43.6989937, -79.43503570000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d04cc97ac51b4441bd6f3318ed21f5cd = L.marker(
            [43.7072258, -79.2952805],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_3bb19aab23ff40afb9dfb6918d890e32 = L.marker(
            [43.7335815, -79.4837265],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_fa0dc57c794f467c9cd01fa599b7d7e4 = L.marker(
            [43.6446571, -79.5307999],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_2cf02e357516422ebacc23873a4fcf28 = L.marker(
            [43.7689018, -79.28555300000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_2bd3a05136054d3e9a9214c429d4d0b8 = L.marker(
            [43.8019333, -79.3593979],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_8d31101cc9494c49a0882cd639e50930 = L.marker(
            [43.69834520000001, -79.5113144],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_4ae9a68303084f66a287643b9805952a = L.marker(
            [43.7797699, -79.41557309999997],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e914cbc261614cf5af571111b1340c19 = L.marker(
            [43.6962624, -79.44768520000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_112747f796cb451fb4c4c6a39c227cf0 = L.marker(
            [43.732395200000006, -79.41941070000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_5071c7e2a92e4f6c842213161f7283ba = L.marker(
            [43.6507835, -79.41349790000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_46f2e6249c4a4a00a3d65329a17ea728 = L.marker(
            [43.7003136, -79.38697049999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_028e7ce3a8214877ac6afa0a8741530a = L.marker(
            [43.8012695, -79.29660030000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_1db04ce8ddf448ff9dfbb20bd1a1b518 = L.marker(
            [43.7054634, -79.40009309999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_f5dfaca0a4194ef19af716635ee5b76c = L.marker(
            [43.6684685, -79.4390335],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_10b5363c7cd740ebb4592c2e490e0421 = L.marker(
            [43.6924973, -79.31573490000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_5979ce036d634c69abc19cc2dccd0778 = L.marker(
            [43.8110886, -79.26600649999997],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c455be45180b4089b333488c0328d104 = L.marker(
            [43.6661034, -79.587677],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_47838cb65f314384b4172a505cbc1a7b = L.marker(
            [43.7035904, -79.45242309999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e33c1df3ff8c41abbb16a799db09f544 = L.marker(
            [43.6831703, -79.41830440000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d110b79e38cc41fd917196795c3629de = L.marker(
            [43.6403351, -79.4379196],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_39b3cd701e604a16a8612e22d7706b6e = L.marker(
            [43.77856060000001, -79.3287048],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_4279a4a1a56f4b728f448fadf9adee60 = L.marker(
            [43.69122700000001, -79.47039790000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_429668bf629f45c980c2e417d3801c59 = L.marker(
            [43.67886729999999, -79.3268204],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_707fa6a47bd04adfba51e32df2532a6b = L.marker(
            [43.74681470000001, -79.58380890000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_f0c6a486f5b24860a829f46105c3a259 = L.marker(
            [43.7865829, -79.18830870000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_ca41c75e1b0a42628d45b0bfb67cba17 = L.marker(
            [43.6508102, -79.5223618],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_0883af36a1ae4175b9201b9b82c936a5 = L.marker(
            [43.7205162, -79.4807281],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_1f7d9a4358fa4a82940cd153e92e903b = L.marker(
            [43.78576279999999, -79.28945920000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_3ac6d5d1561b43f59e95202f3ff0b568 = L.marker(
            [43.7876511, -79.35165409999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_21b77c98874a46ab89ed9f4a52231a02 = L.marker(
            [43.7738609, -79.4430084],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_8c0b534a2d1e44898c551842ae5592e3 = L.marker(
            [43.63902279999999, -79.41591640000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_af30252efa6d4af58803fa82ca5358be = L.marker(
            [43.67033770000001, -79.45579529999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_5d8b0c69291842c5a99193f510706529 = L.marker(
            [43.7222824, -79.2621231],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_6b0b7444e555430993aa00f4920cd58c = L.marker(
            [43.7407112, -79.4388733],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d0f5d0b24610432a9e296fe4c7a22762 = L.marker(
            [43.6801109, -79.2925949],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e0320c5b767a477abd0ad0db5aaeb5e2 = L.marker(
            [43.6918983, -79.4562531],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_2c91a61e965e49abb8ad65c7dfd15e9c = L.marker(
            [43.6862335, -79.3933334],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_7df36917feb34acca8c984c9638e8351 = L.marker(
            [43.7482834, -79.1958313],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_fe01c404f2774d648232cfd7853ab78b = L.marker(
            [43.8042259, -79.1687851],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_364751018cb745d38ddfcc6d18036e5b = L.marker(
            [43.6664314, -79.376503],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_baa61eb27dbd466eb08ebc87c73e6be5 = L.marker(
            [43.746933, -79.4344177],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_8f155a01c41e4b489adb1da2867a39da = L.marker(
            [43.7290611, -79.4459915],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_3a8941f0a12849c39cdab64c769b493b = L.marker(
            [43.65922929999999, -79.5141373],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_50bef7a76411455da2b6e2074df6f845 = L.marker(
            [43.7354164, -79.3471375],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_cd48f44892b7473b89dd2da63e439d4d = L.marker(
            [43.674408, -79.4337845],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_a71b931bd02041eca9a32a446f2c8350 = L.marker(
            [43.6142044, -79.51727290000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_ab06cff06e5042e9b7d8d6baba5e2896 = L.marker(
            [43.695816, -79.5627365],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_8becd2d7621a474497ccbf2f43990c45 = L.marker(
            [43.5936623, -79.5370026],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_6ea7d737508c46078d8181fa47937a0b = L.marker(
            [43.7816391, -79.4552994],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_afa3b428c3b54ca09b407b2d0e6e15cd = L.marker(
            [43.7336807, -79.2270126],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d895ef7e1720480493a4641f696a5327 = L.marker(
            [43.7276535, -79.5495377],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_fb25514165db4312868970a18880d744 = L.marker(
            [43.684288, -79.3192368],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_ca447c7de93c412c9688f16713d4513d = L.marker(
            [43.6056442, -79.538269],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d148cbfb19ff4cd79e0b47ee73ac601a = L.marker(
            [43.65161129999999, -79.4740906],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_3bc20985bb06404e800b1604440da5c3 = L.marker(
            [43.67752460000001, -79.3586502],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_4fe85bed8170460599868428276ca5f7 = L.marker(
            [43.77435300000001, -79.4998016],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_ad2e2e81821e45ee9b30aa7c816db5a9 = L.marker(
            [43.63097379999999, -79.481514],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_1df54db137b3476db1f139c880e1e22f = L.marker(
            [43.7074776, -79.34362790000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_f77015f39cf14bb284ab9584cab4a478 = L.marker(
            [43.6716728, -79.4937515],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_bed3a17acbd24c579ec520390b87d1f0 = L.marker(
            [43.6785393, -79.54489140000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_e45a8228333441dabb32905f75cbcc2e = L.marker(
            [43.6973763, -79.3964691],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_72dbfab000344e478a2cb3e80a417382 = L.marker(
            [43.7051086, -79.3749313],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_2be3584a488a4299a8f5a4fcbff3ea49 = L.marker(
            [43.627346, -79.5623703],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_33b8e98c37104a029f389abac1055069 = L.marker(
            [43.6708565, -79.4751968],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_efd821c7fa734de68e74d0a0f634df33 = L.marker(
            [43.6868706, -79.4685745],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_a23c6b3c049e4de3947f26a0206ee3c4 = L.marker(
            [43.7436333, -79.57647709999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_dc54ebbe0e844314875ab4b889a6f3d0 = L.marker(
            [43.766407, -79.42574309999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_04e47f1a62204c528f5a89627e076585 = L.marker(
            [43.7276649, -79.40623470000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_29a87dfb9c704ed88fcc02c8123bdb1f = L.marker(
            [43.6777802, -79.45579529999998],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c4566b5ef9124ab488d8df9ed60255ba = L.marker(
            [43.694545700000006, -79.3352661],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_afaa0a98dd6b47edb9970ab191cecd02 = L.marker(
            [43.6729698, -79.342392],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_d82f0c18f3cc40b2aa9e7eaa930766a6 = L.marker(
            [43.6854134, -79.4192352],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_29249adc6459462db2b128320c35ed37 = L.marker(
            [43.7075539, -79.50511170000001],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_dc2d48e1078b4620ad79abd82ce2217d = L.marker(
            [43.6677322, -79.3103333],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_2887c69078674f948bd8e8255c90c63d = L.marker(
            [43.6473045, -79.4327087],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_347ab226bf89442c954089be8c6c037b = L.marker(
            [43.65834810000001, -79.4347763],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_bc5c43625af94a47848c62aacc52a33d = L.marker(
            [43.68095020000001, -79.39377590000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_5296fddac1b34398bcd40c4efb532ee7 = L.marker(
            [43.6655273, -79.4992065],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_56bb281de2a5495a9398328d517f4c76 = L.marker(
            [43.79071810000001, -79.15214540000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_721ca467ae4c4a1fb2fe9af6a4bbe3be = L.marker(
            [43.686058, -79.4247742],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_176fc2a37c5b4f1391221b1cfb13d8bc = L.marker(
            [43.6605263, -79.4146652],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_43876ec0613b42bab88ae06bee9952da = L.marker(
            [43.6733017, -79.55154420000002],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_c3770a1809584b49bbb7961931434152 = L.marker(
            [43.6384354, -79.56092070000003],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_9871068fc7d54368ab857788d96059d9 = L.marker(
            [43.7911301, -79.3933563],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
        var marker_856205f3f11c48359f61b7e915bfee08 = L.marker(
            [43.651577, -79.603096],
            {
                icon: new L.Icon.Default(),
                }
            ).addTo(marker_cluster_af17f584e0d846fcbc0515d0680e2fa9);
        
    
            var heat_map_06a209957fc241488cde595120842938 = L.heatLayer(
                [[43.6684494, -79.3430939, 514.0], [43.759285, -79.50792690000002, 2006.0], [43.69755170000001, -79.5016632, 829.0], [43.7217026, -79.5715103, 694.0], [43.6638908, -79.50348659999999, 193.0], [43.65730670000001, -79.3734589, 3609.0], [43.6663628, -79.31660459999998, 648.0], [43.65811160000001, -79.4020233, 1293.0], [43.768856, -79.4669266, 951.0], [43.80780410000001, -79.21559909999998, 2024.0], [43.641552, -79.4745407, 647.0], [43.69513320000001, -79.3034821, 853.0], [43.65588, -79.3632202, 980.0], [43.6639061, -79.38415529999997, 6301.0], [43.6500702, -79.3968811, 3263.0], [43.699131, -79.2543335, 1411.0], [43.7494888, -79.5327911, 968.0], [43.6724319, -79.33419040000003, 1949.0], [43.77361679999999, -79.2611313, 2277.0], [43.6878204, -79.43493649999998, 788.0], [43.65807720000001, -79.3847122, 3564.0], [43.6449547, -79.397644, 5674.0], [43.7564392, -79.3607712, 916.0], [43.663723, -79.3705215, 1119.0], [43.7666092, -79.1856613, 2857.0], [43.6943665, -79.2736511, 1427.0], [43.7439919, -79.5988693, 4338.0], [43.7630234, -79.3175278, 2350.0], [43.768837, -79.37837979999998, 798.0], [43.7460785, -79.3886261, 420.0], [43.7565994, -79.5120697, 1981.0], [43.6659164, -79.4074707, 2908.0], [43.768383, -79.3490372, 472.0], [43.7333374, -79.26329040000002, 804.0], [43.7621841, -79.3211517, 1377.0], [43.8115196, -79.3145447, 687.0], [43.7768669, -79.31622309999999, 1016.0], [43.7383575, -79.3097839, 594.0], [43.7524796, -79.5607376, 1590.0], [43.7429314, -79.21129609999998, 1241.0], [43.6823387, -79.5266037, 521.0], [43.7751579, -79.4143829, 1867.0], [43.7130508, -79.41177370000003, 468.0], [43.7775917, -79.22657779999999, 3158.0], [43.7170258, -79.5372925, 692.0], [43.7412338, -79.2372665, 1804.0], [43.7068634, -79.32061, 970.0], [43.6030045, -79.50748440000002, 841.0], [43.7165794, -79.3303757, 1001.0], [43.7915154, -79.2981033, 1570.0], [43.6412277, -79.42667390000003, 1272.0], [43.6932449, -79.50466920000002, 870.0], [43.6844292, -79.3469238, 464.0], [43.7899132, -79.217453, 871.0], [43.6859818, -79.3562927, 299.0], [43.6989937, -79.43503570000001, 568.0], [43.7072258, -79.2952805, 2163.0], [43.7335815, -79.4837265, 2974.0], [43.6446571, -79.5307999, 2589.0], [43.7689018, -79.28555300000002, 1756.0], [43.8019333, -79.3593979, 687.0], [43.69834520000001, -79.5113144, 1410.0], [43.7797699, -79.41557309999997, 1275.0], [43.6962624, -79.44768520000002, 1117.0], [43.732395200000006, -79.41941070000001, 1135.0], [43.6507835, -79.41349790000002, 1217.0], [43.7003136, -79.38697049999998, 413.0], [43.8012695, -79.29660030000002, 1414.0], [43.7054634, -79.40009309999998, 325.0], [43.6684685, -79.4390335, 2450.0], [43.6924973, -79.31573490000002, 434.0], [43.8110886, -79.26600649999997, 1008.0], [43.6661034, -79.587677, 676.0], [43.7035904, -79.45242309999998, 1571.0], [43.6831703, -79.41830440000003, 457.0], [43.6403351, -79.4379196, 1416.0], [43.77856060000001, -79.3287048, 421.0], [43.69122700000001, -79.47039790000002, 615.0], [43.67886729999999, -79.3268204, 885.0], [43.74681470000001, -79.58380890000002, 2257.0], [43.7865829, -79.18830870000002, 816.0], [43.6508102, -79.5223618, 454.0], [43.7205162, -79.4807281, 459.0], [43.78576279999999, -79.28945920000002, 1331.0], [43.7876511, -79.35165409999998, 1285.0], [43.7738609, -79.4430084, 1078.0], [43.63902279999999, -79.41591640000001, 1324.0], [43.67033770000001, -79.45579529999998, 704.0], [43.7222824, -79.2621231, 1577.0], [43.7407112, -79.4388733, 735.0], [43.6801109, -79.2925949, 1691.0], [43.6918983, -79.4562531, 328.0], [43.6862335, -79.3933334, 1628.0], [43.7482834, -79.1958313, 402.0], [43.8042259, -79.1687851, 1940.0], [43.6664314, -79.376503, 1530.0], [43.746933, -79.4344177, 643.0], [43.7290611, -79.4459915, 873.0], [43.65922929999999, -79.5141373, 473.0], [43.7354164, -79.3471375, 938.0], [43.674408, -79.4337845, 623.0], [43.6142044, -79.51727290000002, 2040.0], [43.695816, -79.5627365, 1116.0], [43.5936623, -79.5370026, 557.0], [43.7816391, -79.4552994, 837.0], [43.7336807, -79.2270126, 1014.0], [43.7276535, -79.5495377, 581.0], [43.684288, -79.3192368, 1191.0], [43.6056442, -79.538269, 460.0], [43.65161129999999, -79.4740906, 1057.0], [43.67752460000001, -79.3586502, 709.0], [43.77435300000001, -79.4998016, 3141.0], [43.63097379999999, -79.481514, 942.0], [43.7074776, -79.34362790000002, 746.0], [43.6716728, -79.4937515, 1227.0], [43.6785393, -79.54489140000003, 1081.0], [43.6973763, -79.3964691, 1235.0], [43.7051086, -79.3749313, 507.0], [43.627346, -79.5623703, 293.0], [43.6708565, -79.4751968, 1043.0], [43.6868706, -79.4685745, 503.0], [43.7436333, -79.57647709999998, 612.0], [43.766407, -79.42574309999998, 548.0], [43.7276649, -79.40623470000001, 445.0], [43.6777802, -79.45579529999998, 818.0], [43.694545700000006, -79.3352661, 600.0], [43.6729698, -79.342392, 425.0], [43.6854134, -79.4192352, 360.0], [43.7075539, -79.50511170000001, 712.0], [43.6677322, -79.3103333, 853.0], [43.6473045, -79.4327087, 703.0], [43.65834810000001, -79.4347763, 602.0], [43.68095020000001, -79.39377590000002, 234.0], [43.6655273, -79.4992065, 535.0], [43.79071810000001, -79.15214540000002, 311.0], [43.686058, -79.4247742, 371.0], [43.6605263, -79.4146652, 788.0], [43.6733017, -79.55154420000002, 363.0], [43.6384354, -79.56092070000003, 450.0], [43.7911301, -79.3933563, 428.0], [43.651577, -79.603096, 23.0]],
                {
                    minOpacity: 0.2,
                    maxZoom: 11,
                    max: 6301,
                    radius: 30,
                    blur: 20,
                    gradient: null
                    })
                .addTo(map_c89271cfdd6b41caae2b868221f7a07b);
        
</script>&#34; style=&#34;position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Consequently, I created both visually appealing and interactive map of the most dense criminal places in the city. It is possible to click on any cluster and zoom in and out to see places that you might want to avoid.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supervised Machine Learning With Scikit-learn and Diabetes</title>
      <link>/post/supervised-ml/</link>
      <pubDate>Tue, 28 May 2019 19:26:32 -0400</pubDate>
      
      <guid>/post/supervised-ml/</guid>
      <description>

&lt;p&gt;In this post, I will explore and configure a few classification algorithms (supervised machine learniIn this post, I will explore and configure a few classification algorithms (supervised machine learning). I originally wanted to apply ML and fit some models on diabetes data. The diabetes dataset is one of the most well known data available to ML enthusiasts for their learning purposes.
After I had started writing my script for the post, I found an &lt;a href=&#34;https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8&#34; target=&#34;_blank&#34;&gt;article&lt;/a&gt; written by Lahiru Liyanapathirana more than an year ago. It turned out that Lahiru wrote his article with the same intention; i.e. to fit the best suitable model. He did a great job and exhausted a wide range of suitable algorithms from Scikit-learn. However, this did not discourage me! Unlike Lahiru, I will focus only on four classifiers consecutively. Specifically, i.) k-nearest neighbor (k-NN), ii.) logistic regression, iii.) decision tree and iv.) random forest.
However, and in addition to Lahiru’s article, I will show and introduce a few “new” techniques. First, I will share how and why I handled the missing data differently (this turned out to have a negative impact on performance). Secondly, I will be fitting the model using &lt;code&gt;Pipeline&lt;/code&gt; and &lt;code&gt;SimpleImputer&lt;/code&gt; as the arguments of &lt;code&gt;GridSearchCV&lt;/code&gt;. To achieve the best performance and generality, I will be also tuning the hyperparameters for every model individually. Last but not least, I maintain that accuracy is not and should not be the only criteria when evaluating the model’s performance!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/jiristo/MLdiabetes_inpectplot.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;loading-the-necessary-packages&#34;&gt;Loading the necessary packages&lt;/h1&gt;

&lt;p&gt;I loaded the following packages according to their consecutive roles in the process.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# data handeling
import pandas as pd
import numpy as np

# Data preprocessing for ML
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

# model traning and testing faciliators
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer 
from sklearn.preprocessing import StandardScaler

# Overfiitting/underfitting guide
from sklearn.model_selection import GridSearchCV

# ML models to be explored
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Performace measurements
from sklearn.metrics import classification_report

# Visualization
import scikitplot as skplt
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from warnings import simplefilter
# ignore all future warnings
simplefilter(action=&#39;ignore&#39;, category= FutureWarning)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;exploratory-data-analysis-eda&#34;&gt;Exploratory Data Analysis (EDA)&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&amp;quot;diabetes_data.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;pregnancies&lt;/th&gt;
      &lt;th&gt;glucose&lt;/th&gt;
      &lt;th&gt;diastolic&lt;/th&gt;
      &lt;th&gt;triceps&lt;/th&gt;
      &lt;th&gt;insulin&lt;/th&gt;
      &lt;th&gt;bmi&lt;/th&gt;
      &lt;th&gt;dpf&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;diabetes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;148&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;33.6&lt;/td&gt;
      &lt;td&gt;0.627&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;26.6&lt;/td&gt;
      &lt;td&gt;0.351&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;23.3&lt;/td&gt;
      &lt;td&gt;0.672&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
      &lt;td&gt;28.1&lt;/td&gt;
      &lt;td&gt;0.167&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;137&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;43.1&lt;/td&gt;
      &lt;td&gt;2.288&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Looking at the head of the data frame, there are several 0 values. Aside from the label (binominal diabetes), the observation suggests some missing data! I bet if your insulin level was 0 and you would be still reading these lines, on a funny note you probably would be the first undead person interested in ML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.describe()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;pregnancies&lt;/th&gt;
      &lt;th&gt;glucose&lt;/th&gt;
      &lt;th&gt;diastolic&lt;/th&gt;
      &lt;th&gt;triceps&lt;/th&gt;
      &lt;th&gt;insulin&lt;/th&gt;
      &lt;th&gt;bmi&lt;/th&gt;
      &lt;th&gt;dpf&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;diabetes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
      &lt;td&gt;768.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;3.845052&lt;/td&gt;
      &lt;td&gt;120.894531&lt;/td&gt;
      &lt;td&gt;69.105469&lt;/td&gt;
      &lt;td&gt;20.536458&lt;/td&gt;
      &lt;td&gt;79.799479&lt;/td&gt;
      &lt;td&gt;31.992578&lt;/td&gt;
      &lt;td&gt;0.471876&lt;/td&gt;
      &lt;td&gt;33.240885&lt;/td&gt;
      &lt;td&gt;0.348958&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;3.369578&lt;/td&gt;
      &lt;td&gt;31.972618&lt;/td&gt;
      &lt;td&gt;19.355807&lt;/td&gt;
      &lt;td&gt;15.952218&lt;/td&gt;
      &lt;td&gt;115.244002&lt;/td&gt;
      &lt;td&gt;7.884160&lt;/td&gt;
      &lt;td&gt;0.331329&lt;/td&gt;
      &lt;td&gt;11.760232&lt;/td&gt;
      &lt;td&gt;0.476951&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.078000&lt;/td&gt;
      &lt;td&gt;21.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;99.000000&lt;/td&gt;
      &lt;td&gt;62.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;27.300000&lt;/td&gt;
      &lt;td&gt;0.243750&lt;/td&gt;
      &lt;td&gt;24.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;117.000000&lt;/td&gt;
      &lt;td&gt;72.000000&lt;/td&gt;
      &lt;td&gt;23.000000&lt;/td&gt;
      &lt;td&gt;30.500000&lt;/td&gt;
      &lt;td&gt;32.000000&lt;/td&gt;
      &lt;td&gt;0.372500&lt;/td&gt;
      &lt;td&gt;29.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;140.250000&lt;/td&gt;
      &lt;td&gt;80.000000&lt;/td&gt;
      &lt;td&gt;32.000000&lt;/td&gt;
      &lt;td&gt;127.250000&lt;/td&gt;
      &lt;td&gt;36.600000&lt;/td&gt;
      &lt;td&gt;0.626250&lt;/td&gt;
      &lt;td&gt;41.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;17.000000&lt;/td&gt;
      &lt;td&gt;199.000000&lt;/td&gt;
      &lt;td&gt;122.000000&lt;/td&gt;
      &lt;td&gt;99.000000&lt;/td&gt;
      &lt;td&gt;846.000000&lt;/td&gt;
      &lt;td&gt;67.100000&lt;/td&gt;
      &lt;td&gt;2.420000&lt;/td&gt;
      &lt;td&gt;81.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Based on the summary statistics, &lt;code&gt;min&lt;/code&gt; indicates other features with missing values: i.) &lt;code&gt;glucose&lt;/code&gt; ii.) &lt;code&gt;diastolic&lt;/code&gt; (blood pressure), iii.) &lt;code&gt;triceps&lt;/code&gt;, iv.) &lt;code&gt;bmi&lt;/code&gt;, i.e. $bmi = \frac{body weight}{height}$. Frankly, anyone with 0 blood pressure or bmi (body-mass-index) must be dead. Therefore, it is necessary to clean the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
df.isnull().sum()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
pregnancies    768 non-null int64
glucose        768 non-null int64
diastolic      768 non-null int64
triceps        768 non-null int64
insulin        768 non-null int64
bmi            768 non-null float64
dpf            768 non-null float64
age            768 non-null int64
diabetes       768 non-null int64
dtypes: float64(2), int64(7)
memory usage: 54.1 KB





pregnancies    0
glucose        0
diastolic      0
triceps        0
insulin        0
bmi            0
dpf            0
age            0
diabetes       0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are 768 all numeric records with no &lt;code&gt;NaN&lt;/code&gt;. Therefore, &lt;code&gt;0&lt;/code&gt; values in the data represent missing records. How many “zeros” are in each column of the data frame (&lt;code&gt;df&lt;/code&gt;)? I answered the question with a for loop and &lt;code&gt;count_nonzero()&lt;/code&gt; method of NumPy array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for colname in df.columns[:8]:
    print(&#39;0s in &amp;quot;{variable}&amp;quot;: {count}&#39;.format(
        variable=colname,
        count=np.count_nonzero(df[colname] == 0)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0s in &amp;quot;pregnancies&amp;quot;: 111
0s in &amp;quot;glucose&amp;quot;: 5
0s in &amp;quot;diastolic&amp;quot;: 35
0s in &amp;quot;triceps&amp;quot;: 227
0s in &amp;quot;insulin&amp;quot;: 374
0s in &amp;quot;bmi&amp;quot;: 11
0s in &amp;quot;dpf&amp;quot;: 0
0s in &amp;quot;age&amp;quot;: 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In general, there are three approaches in which a data scientist could handle so many missing values and they are as below:
1.  Delete any observation with missing values, which would result in substantial loss of data.
2.  Substitute missing values with either mean, median, or mode; which can be a great trade between regression to the mean but keeping the data. However, it is not the best solution when it comes to the substitution of more than half of values in a variable.
3.  Give up; hell no.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before dealing with the missing values, let’s examine the data visually&lt;/strong&gt;. Since Lahiru already presented the histograms of the variables, I have decided to expand the insight by examining the box plots. Unlike the histogram, a box plot is a great tool to identify outliers in the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.drop(&amp;quot;diabetes&amp;quot;,axis=1).boxplot(figsize = [10, 7])
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./supervised-ML_16_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Obviously, the variables exhibit various distribution patterns. For example, &lt;code&gt;insulin&lt;/code&gt; reaches both the greatest maxima and exhibits many overlapping outliers. Its distribution is right-skewed, and I expect the variable is a very strong predictor for the diagnosis of diabetes. You do not have to be an expert in ML but I do not think the data is the best sample for ML. Firstly, there are a lot of missing values. Secondly, ML is not very effective when it comes to handling outliers.&lt;/p&gt;

&lt;p&gt;Since, I will be using &lt;code&gt;RandomForestClassifier&lt;/code&gt;, there is a way to quickly and visually investigate the importance of these features. This is the tool: &lt;code&gt;skplt.estimators.plot_feature_importances&lt;/code&gt;. It plots the classifier’s feature importance. You can visually inspect how much the variable, relative to other features, correlates to the occurrence of diabetes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df2 = df[(df.insulin !=0) &amp;amp; (df.bmi !=0) &amp;amp; (df.pregnancies !=0) &amp;amp; (df.glucose !=0)]
feature_names = df2.columns[:-1]

randfor = RandomForestClassifier()
randfor.fit(df2.drop(columns = &amp;quot;diabetes&amp;quot;, axis=1),df2[&amp;quot;diabetes&amp;quot;])

skplt.estimators.plot_feature_importances(randfor, feature_names=feature_names, figsize=(9, 5))
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./supervised-ML_19_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The result supports my assumption that &lt;code&gt;glucose&lt;/code&gt; is a very strong predictor for the presence of diabetes.&lt;/p&gt;

&lt;h3 id=&#34;data-wrangling&#34;&gt;Data Wrangling&lt;/h3&gt;

&lt;p&gt;I see nothing wrong in deleting a few observations with missing values, however, in my case the approach would result in a loss of too many observations. Lahirru has probably realized the same and therefore deleted only the rows with missing data in &lt;code&gt;glucose&lt;/code&gt;, &lt;code&gt;diastolic&lt;/code&gt;, and &lt;code&gt;bmi&lt;/code&gt;. Therefore, he deleted only a small fraction of all observations.
To deal with the issue in my own way, I have decided to combine 1.) and 2.) approaches discussed above. I was curious whether I would achieve higher accuracy and precision and what recall would be. For the sake of learning, I even decided to share my experience regardless of the result.
I designed my approach as follows:
* Firstly, I decided to delete every row where the number of &lt;code&gt;0s&lt;/code&gt; was greater than 1. On the other hand, I made the assumption which probably does not hold, i.e., pregnancies do not exhibit any missing values and observed &lt;code&gt;0s&lt;/code&gt; are valid records indicating that the person is not pregnant.
* Secondly, I replaced the remaining missing values with a `mode.
    * I will handle the operation using SimpleImputer in the Pipeline while fitting the model to the training data set and predicting.&lt;/p&gt;

&lt;p&gt;The following chunk of code summarizes the first step of my approach:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for index,row in df.iterrows():
    zero_count = row[1:6]
    if np.count_nonzero(zero_count==0) &amp;gt; 1:
        df.drop(index, inplace=True)

df.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(534, 9)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result, I deleted only 234 rows. Deleting every observation with a missing value would result in loss of at least 374 rows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for colname in df.columns[:8]:
    print(&#39;Ex-post0s in &amp;quot;{variable}&amp;quot;: {count}&#39;.format(
        variable=colname,
        count=np.count_nonzero(df[colname] == 0)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Ex-post0s in &amp;quot;pregnancies&amp;quot;: 78
Ex-post0s in &amp;quot;glucose&amp;quot;: 1
Ex-post0s in &amp;quot;diastolic&amp;quot;: 0
Ex-post0s in &amp;quot;triceps&amp;quot;: 0
Ex-post0s in &amp;quot;insulin&amp;quot;: 140
Ex-post0s in &amp;quot;bmi&amp;quot;: 1
Ex-post0s in &amp;quot;dpf&amp;quot;: 0
Ex-post0s in &amp;quot;age&amp;quot;: 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, one should examine correlations between the variables and label. There are many ways to do so, but I particularly prefer the heat map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize = [8, 5.5])
sns.heatmap(df2.corr(), square=False, cmap=&#39;RdYlGn&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./supervised-ML_26_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I evaluated the correlations from three perspectives; i.e.,1.) General pattern, 2.) Correlation between &lt;code&gt;diabetes&lt;/code&gt; and other variables, 3.) Correlation amongst all variables.
1. The data set contains only variables positively correlated with presence of &lt;code&gt;diabetes&lt;/code&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Most of the times, the variables exhibit low correlations between themselves and &lt;code&gt;diabetes&lt;/code&gt;. However, &lt;code&gt;glucose&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; seem to be much more correlated with the target than other variables.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;All the features are mostly uncorrelated. However, you can notice correlations above 0.6 between:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt; and &lt;code&gt;pregnancies&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glucose&lt;/code&gt; and &lt;code&gt;insulin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;triceps&lt;/code&gt; and &lt;code&gt;bmi&lt;/code&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I consulted a relative of mine who studies  medicine to take a look at the observations. She confirmed the correlations are in line with her expectancy.
Finally, I substituted every &lt;code&gt;0&lt;/code&gt; in the data frame (except in the label) with &lt;code&gt;NaN&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;Finally, I substitued  every &lt;code&gt;0&lt;/code&gt; in the data frame (except in the label) with &lt;code&gt;NaN&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.glucose.replace(0, np.nan, inplace = True)
df.insulin.replace(0, np.nan, inplace = True)
df.bmi.replace(0, np.nan, inplace = True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pre-setting-and-modeling-strategy&#34;&gt;Pre-setting and Modeling Strategy&lt;/h3&gt;

&lt;p&gt;Last but not least, I split the data into labels and 2D arrays for training and testing as is the standard (and not rocket science) approach in ML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = df.drop(columns = &amp;quot;diabetes&amp;quot;, axis=1)
y = df[&amp;quot;diabetes&amp;quot;]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;imputer&#34;&gt;Imputer&lt;/h4&gt;

&lt;p&gt;Referring back to the second step in my proposed approach, I utilized &lt;code&gt;SimpleImputer&lt;/code&gt; and I passed it into &lt;code&gt;Pipeline&lt;/code&gt;.  The function substitutes &lt;code&gt;NaN&lt;/code&gt; with a chosen value relative to the variable, e.g. &lt;code&gt;mean&lt;/code&gt;, &lt;code&gt;mode&lt;/code&gt;, etc.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;imp = SimpleImputer(strategy = &amp;quot;most_frequent&amp;quot;, missing_values = np.nan)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;following steps&lt;/strong&gt; summarize my approach to fit a particular model. Moreover, I designed the logic to be perfectly replicable:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;create the list &lt;code&gt;steps_model&lt;/code&gt; containing functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;imp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;create &lt;code&gt;pipeline_model&lt;/code&gt; by calling &lt;code&gt;Pipeline&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pass &lt;code&gt;steps_model&lt;/code&gt; as the argument
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a list &lt;code&gt;parameters_model&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the object contains the parameters of a particular model with ranges of values
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create &lt;code&gt;cv_model&lt;/code&gt; by calling &lt;code&gt;GridSearchCV&lt;/code&gt; (cross validation)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Specify the arguments as follows: &lt;code&gt;pipeline _model , param_grid = parameters_model, scoring = &amp;quot;roc_auc&amp;quot;, cv = 5&lt;/code&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Call &lt;code&gt;cv_model.fit(X_train, y_train)&lt;/code&gt; to train a model&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Predict the label &lt;code&gt;pred_model&lt;/code&gt; by calling &lt;code&gt;cv_model.predict(X_test)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Evaluate performance by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cv_model.best_params_(y_test, pred_model)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;accuracy_score(y_test, pred_model)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;classification_report(y_test, pred_model)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;The strategy is identical to all the models that I will attempt to fit, i.e.: i.) k-nearest neighbor (k-NN), ii.) logistic regression, iii.) decision tree and iv.) random forest.&lt;/strong&gt;
Therefore, , I will discuss the key issues only in &lt;code&gt;I.)k - Nearest Neighbour&lt;/code&gt;. I am intentionally avoiding commenting on other models because the logic remains the same. However,  the list** &lt;code&gt;parameters_model&lt;/code&gt; &lt;strong&gt;should be unique to each classifier because the parameters differ from model to model.&lt;/strong&gt;
&lt;strong&gt;I will comment on the performance and draw a conclusion in the last part of the article.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;i-k-nearest-neighbour&#34;&gt;I.) K - Nearest Neighbour&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 1.
steps_knn = [(&amp;quot;imputation&amp;quot;, imp),
         (&amp;quot;scaler&amp;quot;, StandardScaler()),
         (&amp;quot;knn&amp;quot;, KNeighborsClassifier())]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 2.
pipeline_knn = Pipeline(steps_knn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 3. For KNN, I only specified the range of paramters from 1 to 51.
parameters_knn = {&amp;quot;knn__n_neighbors&amp;quot;:np.arange(1,51)}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 4.
cv_knn = GridSearchCV(pipeline_knn, param_grid = parameters_knn, scoring = &amp;quot;roc_auc&amp;quot;, cv = 5 )
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;training-the-model-step-5&#34;&gt;Training the Model (Step 5.)&lt;/h6&gt;

&lt;p&gt;By calling &lt;code&gt;cv_model&lt;/code&gt; on the label and 2D data frame, the function utilizes all the steps from &lt;code&gt;steps_model&lt;/code&gt;at first. Specifically, &lt;strong&gt;only and only the data is&lt;/strong&gt;: i.) cleaned (NaNs substituted with the mode), ii.) standardized, iii.) &lt;strong&gt;it can be classified by the classifier&lt;/strong&gt;.   Additionally, the process goes hand in hand with &lt;code&gt;GridSearchCV&lt;/code&gt; what splits &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt; into 5 identical data sets and performs cross-validation on each of them. However, each data set is split into different test and train sections. While cross-validating, &lt;code&gt;GridSearchCV&lt;/code&gt; searches for the best parameters specified in &lt;code&gt;parameters_model&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 5.
cv_knn.fit(X_train, y_train) 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;,
       estimator=Pipeline(memory=None,
     steps=[(&#39;imputation&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan,
       strategy=&#39;most_frequent&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;knn&#39;, KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
           metric_params=None, n_jobs=None, n_neighbors=5, p=2,
           weights=&#39;uniform&#39;))]),
       fit_params=None, iid=&#39;warn&#39;, n_jobs=None,
       param_grid={&#39;knn__n_neighbors&#39;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,
       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=&#39;roc_auc&#39;, verbose=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 6.
pred_knn = cv_knn.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;measuring-performance-step-7&#34;&gt;Measuring Performance (Step 7.)&lt;/h5&gt;

&lt;p&gt;&lt;code&gt;classification_report&lt;/code&gt; is one of the methods to see a bigger picture of the model’s performance. Since I classified the binary variable relevant to human health, I want to focus on the &lt;code&gt;precision&lt;/code&gt; and  &lt;code&gt;recall&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;precision&lt;/code&gt; is also called &lt;strong&gt;positive predictive value (PPV)&lt;/strong&gt;. Mathematically: $PPV = \frac{TP}{TP+FP}$ It is similar to accuracy, but focuses only on data the model has predicted  to be positive, i.e. &lt;code&gt;diabetes = 1&lt;/code&gt;. Referring to a confusion matrix, &lt;code&gt;precision&lt;/code&gt; of 1 means that there were no false positives.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;recall&lt;/code&gt;, also called sensitivity OR True Positive Rate (TPR), answers the question on how complete the results are, i.e. did the model miss any positive classes and to what extent? Mathematically expressed:$TPR = \frac{TP}{TP+FN}$ In our case, low &lt;code&gt;recall&lt;/code&gt; would mean that the model has incorrectly classified a lot of individuals with diabetes as healthy ones.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One should ask, what is the superior metric from the two? &lt;strong&gt;A friend of mine, a data scientist, told me he was asked the same at his interview&lt;/strong&gt;. In fact, it really depends!
For example, imagine cancer diagnostics, would you rather classify few more patients as false positive and after more precise examination conclude they had no cancer or would you rather let escape the ones with cancer as healthy individuals? &lt;strong&gt;In this particular case, the model should minimize $FN$ in the confusion matrix. Consequently, recall,i.e. TPR, which should be closer to 1.&lt;/strong&gt;
Lastly, there is always a trade-off between the two negatively correlated metrics.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 7.
print(&amp;quot;KNN Parameters are: {}&amp;quot;.format(cv_knn.best_params_))
print(&amp;quot;KNN Accuracy is: {}&amp;quot;.format(accuracy_score(y_test, pred_knn)))
print(classification_report(y_test, pred_knn))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;KNN Parameters are: {&#39;knn__n_neighbors&#39;: 47}
KNN Accuracy is: 0.7142857142857143
              precision    recall  f1-score   support

           0       0.70      0.96      0.81       100
           1       0.83      0.31      0.45        61

   micro avg       0.71      0.71      0.71       161
   macro avg       0.76      0.64      0.63       161
weighted avg       0.75      0.71      0.67       161
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ii-logistic-reg&#34;&gt;II.) Logistic Reg&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Steps 1. &amp;amp; 2.
pipeline_logreg = Pipeline([(&amp;quot;imputation&amp;quot;, imp),
         (&amp;quot;scaler&amp;quot;, StandardScaler()),
         (&amp;quot;logreg&amp;quot;, LogisticRegression())])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 3.
parameters_logreg = {&amp;quot;logreg__C&amp;quot;:np.arange(0.1, 1.1, 0.1),
             &amp;quot;logreg__penalty&amp;quot;:(&amp;quot;l1&amp;quot;, &amp;quot;l2&amp;quot;)}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 4.
cv_logreg = GridSearchCV(pipeline_logreg, param_grid = parameters_logreg, cv = 5 )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 5.
cv_logreg.fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;,
       estimator=Pipeline(memory=None,
     steps=[(&#39;imputation&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan,
       strategy=&#39;most_frequent&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;logreg&#39;, LogisticRegression(C=1.0, class_weight=None, dual=Fa...enalty=&#39;l2&#39;, random_state=None, solver=&#39;warn&#39;,
          tol=0.0001, verbose=0, warm_start=False))]),
       fit_params=None, iid=&#39;warn&#39;, n_jobs=None,
       param_grid={&#39;logreg__C&#39;: array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), &#39;logreg__penalty&#39;: (&#39;l1&#39;, &#39;l2&#39;)},
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=None, verbose=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 6.
pred_logreg = cv_logreg.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;iii-decision-tree&#34;&gt;III.) Decision Tree&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Steps 1. &amp;amp; 2.
pipeline_dectree = Pipeline([(&amp;quot;imputation&amp;quot;, imp),
         (&amp;quot;scaler&amp;quot;, StandardScaler()),
         (&amp;quot;dectree&amp;quot;, DecisionTreeClassifier())])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 3.
parameters_dectree = {&#39;dectree__max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;, None],
          #&#39;dectree__min_samples_split&#39;: np.arange(2, 15), 
          #&#39;dectree__min_samples_leaf&#39;:np.arange(2, 15),
          &#39;dectree__random_state&#39;:[42]}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 4.
cv_dectree = GridSearchCV(pipeline_dectree, param_grid = parameters_dectree, cv = 5 )
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 5.
cv_dectree.fit(X_train,y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;,
       estimator=Pipeline(memory=None,
     steps=[(&#39;imputation&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan,
       strategy=&#39;most_frequent&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;dectree&#39;, DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,
            ...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter=&#39;best&#39;))]),
       fit_params=None, iid=&#39;warn&#39;, n_jobs=None,
       param_grid={&#39;dectree__max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;, None], &#39;dectree__random_state&#39;: [42]},
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=None, verbose=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Step 6.
pred_dectree = cv_dectree.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;iv-random-forest&#34;&gt;IV.) Random Forest&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipeline_randfst = Pipeline([(&amp;quot;imputation&amp;quot;, imp),
         (&amp;quot;scaler&amp;quot;, StandardScaler()),
         (&amp;quot;randfst&amp;quot;, RandomForestClassifier())])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;paraparameters_randfst = {&amp;quot;randfst__n_estimators&amp;quot;: np.arange(5,200,5),
                          &amp;quot;randfst__criterion&amp;quot;:[&#39;gini&#39;,&#39;entropy&#39;],
                          #&amp;quot;randfst__n_jobs&amp;quot;:[-1],
                          &amp;quot;randfst__random_state&amp;quot;:[42]}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_randfst = GridSearchCV(pipeline_randfst, param_grid = paraparameters_randfst, cv = 5)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_randfst.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;,
       estimator=Pipeline(memory=None,
     steps=[(&#39;imputation&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan,
       strategy=&#39;most_frequent&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;randfst&#39;, RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            ...obs=None,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False))]),
       fit_params=None, iid=&#39;warn&#39;, n_jobs=None,
       param_grid={&#39;randfst__n_estimators&#39;: array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,
        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,
       135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195]), &#39;randfst__criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;randfst__random_state&#39;: [42]},
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=None, verbose=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred_randfst = cv_randfst.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-7-best-model-selection&#34;&gt;Step 7. &amp;amp;  Best Model Selection&lt;/h3&gt;

&lt;p&gt;I already mentioned &lt;code&gt;precision&lt;/code&gt; and &lt;code&gt;recall&lt;/code&gt; but did explain &lt;code&gt;accuracy&lt;/code&gt;. In fact, &lt;code&gt;accuracy&lt;/code&gt; may  not be the best parameter for choosing the right model. Consider test data with 100 individuals from which 99 subjects are healthy ones and only 1 individual has diabetes. Also, assume the model successfully classified 99 healthy people but completely failed to classify the one individual with diabetes.&lt;/p&gt;

&lt;p&gt;Gived that $accuracy = \frac{TP + TN} {TP + TN + FP + FN}$, than the accuracy would be 99%. However, the algorithm missed 100% individuals in the positive class.&lt;/p&gt;

&lt;p&gt;The following code displays the best parameters and accuracy of every classifier I fit. Moreover, &lt;code&gt;classification_report&lt;/code&gt; displays the table with &lt;code&gt;recall&lt;/code&gt; and &lt;code&gt;precision&lt;/code&gt;, so you can effectively evaluate each model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;KNN Parameters are: {}&amp;quot;.format(cv_knn.best_params_))
print(&amp;quot;KNN Accuracy is: {}&amp;quot;.format(accuracy_score(y_test, pred_knn)))
print(classification_report(y_test, pred_knn))

print(&amp;quot;Log. reg Parameters are: {}&amp;quot;.format(cv_logreg.best_params_))
print(&amp;quot;Log. reg accuracy is: {}&amp;quot;.format(accuracy_score(y_test, pred_logreg)))
print(classification_report(y_test, pred_logreg))

print(&amp;quot;Dec. tree Parameters are: {}&amp;quot;.format(cv_dectree.best_params_))
print(&amp;quot;Dec. tree accuracy is: {}&amp;quot;.format(accuracy_score(y_test, pred_dectree)))
print(classification_report(y_test, pred_dectree))

print(&amp;quot;Rand. forest Parameters are: {}&amp;quot;.format(cv_randfst.best_params_))
print(&amp;quot;Rand. forest accuracy is: {}&amp;quot;.format(accuracy_score(y_test, pred_randfst)))
print(classification_report(y_test, pred_randfst))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;KNN Parameters are: {&#39;knn__n_neighbors&#39;: 47}
KNN Accuracy is: 0.7142857142857143
              precision    recall  f1-score   support

           0       0.70      0.96      0.81       100
           1       0.83      0.31      0.45        61

   micro avg       0.71      0.71      0.71       161
   macro avg       0.76      0.64      0.63       161
weighted avg       0.75      0.71      0.67       161

Log. reg Parameters are: {&#39;logreg__C&#39;: 0.1, &#39;logreg__penalty&#39;: &#39;l2&#39;}
Log. reg accuracy is: 0.7639751552795031
              precision    recall  f1-score   support

           0       0.76      0.91      0.83       100
           1       0.78      0.52      0.63        61

   micro avg       0.76      0.76      0.76       161
   macro avg       0.77      0.72      0.73       161
weighted avg       0.77      0.76      0.75       161

Dec. tree Parameters are: {&#39;dectree__max_features&#39;: None, &#39;dectree__random_state&#39;: 42}
Dec. tree accuracy is: 0.6708074534161491
              precision    recall  f1-score   support

           0       0.74      0.73      0.73       100
           1       0.56      0.57      0.57        61

   micro avg       0.67      0.67      0.67       161
   macro avg       0.65      0.65      0.65       161
weighted avg       0.67      0.67      0.67       161

Rand. forest Parameters are: {&#39;randfst__criterion&#39;: &#39;entropy&#39;, &#39;randfst__n_estimators&#39;: 70, &#39;randfst__random_state&#39;: 42}
Rand. forest accuracy is: 0.7453416149068323
              precision    recall  f1-score   support

           0       0.74      0.91      0.82       100
           1       0.76      0.48      0.59        61

   micro avg       0.75      0.75      0.75       161
   macro avg       0.75      0.69      0.70       161
weighted avg       0.75      0.75      0.73       161
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the logistic model performs with the highest accuracy of 0.77 meaning the model predicted 77% of cases correctly. However, you do not want to send any patient home with diabetes as if she was healthy. From this perspective, you should choose the model with &lt;code&gt;recall&lt;/code&gt; close to 1. Since, the logistic model has &lt;code&gt;recall = 0.57&lt;/code&gt;, you may consider to decide for the &lt;strong&gt;decision tree classifier&lt;/strong&gt;. Considering overall accuracy, the model performs by 10 p.p. worse. However, it has the largest &lt;code&gt;recall&lt;/code&gt; from all the considered classifiers. Administering this model, you would have sent home the lowest possible number of patients with diabetes at the cost of reexamining greater number of healthy individuals.&lt;/p&gt;

&lt;p&gt;I am not arguing &lt;strong&gt;decision tree&lt;/strong&gt; is the best possible model for such type of data. Actually, I do not assume someone uses this particular branch of machine learning when it comes to health. I am aware of cases where &lt;strong&gt;deep learning and neural network models&lt;/strong&gt; are used instead. For example, a visual algorithm can detect cancer when it is trained on pictures of human cells.&lt;/p&gt;

&lt;p&gt;The point I am trying to make is that accuracy is not the best and only one parameter for model selection. There are many factors one must consider when a model needs to be selected.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;I exI explained and demonstrated how important is it to explore your data before modeling. Specifically, there are more possibilities of how missing values can be recorded. To see a bigger picture of your data, it is important to ask yourself some basic questions, e.g. “can someone have 0 blood pressure?”. Once you identify missing values in your data, you should decide on how to deal with them. Would you delete every record with a missing value or would you substitute it with a mean, or would you try to be efficient and keep as much data as possible? I also put forth my point of view on how to find the feature&amp;rsquo;s importance on the final outcome. Finally, I evaluated the models based on &lt;code&gt;recall&lt;/code&gt; rather than on accuracy because &lt;code&gt;recall&lt;/code&gt; close to 1 minimizes the number of cases when a patient with either diabetes or cancer would be classified and treated incorrectly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping Reddit: Text (Sentiment) Analysis</title>
      <link>/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/wordcloud2/wordcloud.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/wordcloud2/wordcloud2-all.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/wordcloud2/hover.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/wordcloud2-binding/wordcloud2.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Canada recently joined few other countries that have completely decriminalized cannabis consumption, hence making it entirely legal. While many argued the Country has infringed a number of international agreements, others sang chorales. Whatever your attitude towards legalization of cannabis may be, there are one set of people that are definitely happy and they are the “Canadian Cannabis consumers”. However, only provincial, i.e. governmental, legal entities are allowed to sell and distribute marihuana products until April 2019. Marihuana consumers may not break the law while smoking or passing a joint any more. What were the sentiments tied to people in the early days of Cannabis legalization and when the consumers were allowed to make online orders? Did the new policies meet the users’ expectations? I am sure, some future and official studies will answer the question very soon. However, I attempted to find the answer on my own. In this post, I will explain how I scraped one of the biggest website forums - Reddit. Additionally, I will demonstrate how I performed a simple sentiment analysis on a tidy dataset I had created.&lt;/p&gt;
&lt;p&gt;GitHub Repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/jiristo/webscraping_inspectplot.git&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;reading-html-code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reading HTML Code&lt;/h1&gt;
&lt;p&gt;Reddit is a static website. At the time, when I was scraping the forum, I did not know about its public API. However, the main purpose of my effort was to i.) learn web scraping and create my own data set, and ii.) understand web structure. To conduct the analysis, I was mostly interested in pure text, i.e. reviews, and their authors. Any contributor can also assign points (in the form of likes or dislikes) to the most appealing comments posted by other users. Each post also displays a time frame when the comment was posted. &lt;strong&gt;rvest&lt;/strong&gt; library, developed by Hadley Wickham, is powerful enough to secure any accessible data within an HTML. Moreover, the package supports the syntax of tidyverse.&lt;/p&gt;
&lt;p&gt;First of all, I connected to the webpage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- (&amp;quot;https://old.reddit.com/r/canadients/comments/9ovapz/legal_cannabis_in_canada_megathread/?limit=500&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I struggled a little while selecting all the nodes with comments and relevant information (my variables). However, you need to read HTML code, &lt;code&gt;call read_html()&lt;/code&gt;, only once. Basically, every comment on Reddit is displayed in a “bubble”. And most importantly, the “bubbles” contain all the required information! Being new to HTML and web scraping, I experienced difficulties to specify the correct arguments inside html_nodes(). Examining HTML code and conducting some research, I realized I had to use CSS selectors to style the elements in HTML. To my understanding &lt;code&gt;.&lt;/code&gt; as the argument in quotation marks means: “select classes so &lt;code&gt;.entry&lt;/code&gt; selector selects all the objects with the entry class. The output assigned to reviews is a list (of nodes) of length 484. Again, every node contains relevant information about each comment. Therefore, I called &lt;code&gt;read_html()&lt;/code&gt; and &lt;code&gt;html_nodes()&lt;/code&gt; only once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(rvest)

reviews &amp;lt;- url %&amp;gt;%
  read_html() %&amp;gt;%
  html_nodes(&amp;#39;.entry&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;generating-the-variables&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Generating the Variables&lt;/h6&gt;
&lt;p&gt;I started with the authors. &lt;code&gt;html_node(&amp;quot;.author&amp;quot;)&lt;/code&gt; selects all the objects within the author class from &lt;code&gt;reviews&lt;/code&gt;. The output is a vector of a length of the &lt;code&gt;reviews&lt;/code&gt;. Each element in the list is a node with the class &lt;code&gt;author&lt;/code&gt;. For example: &lt;code&gt;&amp;lt;a href=&amp;quot;https://old.reddit.com/user/Hendrix194&amp;quot; class=&amp;quot;author may-blank id-t2_n4hdv&amp;quot;&amp;gt;Hendrix194&amp;lt;/a&amp;gt;&lt;/code&gt;. This would be useless unless you call other functions from &lt;strong&gt;rvest&lt;/strong&gt;, i.e. &lt;code&gt;html_text()&lt;/code&gt;. It extracts the selector content that is, in this case, the author’s name: &lt;strong&gt;Hendrix194&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;author&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Author&lt;/h6&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;author &amp;lt;- reviews %&amp;gt;%
  html_node(&amp;quot;.author&amp;quot;) %&amp;gt;%
  html_text()%&amp;gt;%
  str_trim() 

author &amp;lt;- as.factor(author)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After obtaining and examining the value &lt;strong&gt;author&lt;/strong&gt;, I found several &lt;code&gt;[deleted]&lt;/code&gt; values. Initially, I thought these authors and their comments were deleted. However, for whatever reason, the comments were still visible. Therefore, I decided to not dispose them. Finally, I stored the vector as a factor because each author is an individual entity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comment&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Comment&lt;/h6&gt;
&lt;p&gt;Naturally, each comment is the most important variable for my analysis. The approach was identical to scraping &lt;code&gt;author&lt;/code&gt;. Once again, the function selects all the objects with the class. In addition to solely extracting the selector content by &lt;code&gt;html_text()&lt;/code&gt;, I specified an additional argument &lt;code&gt;trim = TRUE&lt;/code&gt;. &lt;code&gt;trim&lt;/code&gt; eliminates any space character (which is invisible to human eye) before and after a string. Additionally, I dispose of newline separators &lt;code&gt;\n&lt;/code&gt; by calling the &lt;code&gt;gsub()&lt;/code&gt; function and specifying the pattern as the function’s argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comment &amp;lt;- reviews %&amp;gt;%
  html_node(&amp;quot;.md&amp;quot;) %&amp;gt;%
  html_text(trim = TRUE ) %&amp;gt;%
  gsub(&amp;quot;\n&amp;quot;,&amp;quot;&amp;quot;,.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After I extracted the content of &lt;code&gt;.score&lt;/code&gt;, I ended up with a string vector where each value was composed of two words. Additionally, the first word was supposed to be numeric, e.g. &lt;code&gt;&amp;quot;3 points&amp;quot;&lt;/code&gt;. To get it, I simply supplied 1; i.e. specified the position of my word of interest, as the argument inside &lt;code&gt;stringr::word()&lt;/code&gt; and piped the result into &lt;code&gt;as.integer()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(stringr)

likes &amp;lt;- reviews %&amp;gt;%
  html_node(&amp;quot;.score&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;%
  word(1) %&amp;gt;%
  as.integer() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initially, I thought I would only be able to scrape the time variable as it is displayed on Reddit, e.g. &lt;code&gt;&amp;quot;3 months ago&amp;quot;&lt;/code&gt;. Fortunately, such format is the output of &lt;code&gt;java&lt;/code&gt;, which together with the &lt;code&gt;HTML&lt;/code&gt; and &lt;code&gt;CSS&lt;/code&gt;, makes any website’s content readable to a human. Note, that I also did not extract selector’s content as I did with the previous variables.&lt;/p&gt;
&lt;p&gt;Finally, I formatted &lt;code&gt;time&lt;/code&gt; with the base &lt;code&gt;striptime()&lt;/code&gt; function and transformed it by &lt;code&gt;ymd_hms()&lt;/code&gt; into POSIXct. Such formatting may have major benefits when analyzing sentiments in units of minutes, seconds but also at lower frequencies!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# lubridate
date &amp;lt;- reviews %&amp;gt;%
  html_node(&amp;quot;time&amp;quot;)%&amp;gt;%
  html_attr(&amp;quot;title&amp;quot;)%&amp;gt;%
  strptime(format = &amp;quot;%a %b %d %H:%M:%S %Y&amp;quot;,tz = &amp;quot;UTC&amp;quot;)%&amp;gt;%
  ymd_hms()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Data Frames&lt;/h6&gt;
&lt;p&gt;I ended up with 4 vectors. Firstly, I merged all of them into one data frame &lt;code&gt;dataset&lt;/code&gt;. I also called &lt;code&gt;filter&lt;/code&gt; and &lt;code&gt;mutate&lt;/code&gt; to simultaneously clean the data a little bit and create &lt;code&gt;id&lt;/code&gt; - a unique value to every comment. &lt;code&gt;dataset&lt;/code&gt; is my entry level data frame for two additional tidy datasets. Additionally, it is easy to search for any &lt;code&gt;id&lt;/code&gt; and examine the associated comment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data frame from vectors
dataset &amp;lt;- data.frame(author, likes, comment, date, stringsAsFactors = FALSE) %&amp;gt;%
  filter(!is.na(author))%&amp;gt;%
  # Filtering comments of few words
  filter(str_count(comment)&amp;gt;=5) %&amp;gt;%
  # creating ID for each comment so I can refer to it later
  mutate(id = row_number())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Secondly, it was necessary to clean &lt;code&gt;dataset$comment&lt;/code&gt;. Specifically, any stop words, single and meaningless characters, and numbers that are redundant for the majority of text analysis. My goal was to create a new &lt;strong&gt;tidy data frame&lt;/strong&gt; where each observation would be a single word from the comment it appears in. I achieved that through &lt;code&gt;tidytext::unnest_tokens()&lt;/code&gt;, &lt;code&gt;tidytext::stop_words&lt;/code&gt;, &lt;code&gt;dplyr::anti_join()&lt;/code&gt;. &lt;code&gt;unnest_token()&lt;/code&gt; takes three arguments: i.) dataset, ii.) new variable (&lt;code&gt;word&lt;/code&gt;), and iii.) name of a string column (&lt;code&gt;comment&lt;/code&gt;). The output is the tidy data frame where number of rows per entity (&lt;code&gt;comment&lt;/code&gt; or &lt;code&gt;id&lt;/code&gt;) is equal to number of words in &lt;code&gt;comment&lt;/code&gt;. It is also worthy of mention that the function sets all the letters to lower case by default. I also called &lt;code&gt;anti_join()&lt;/code&gt; three times to filter: i.) stop words (&lt;code&gt;stop_words&lt;/code&gt; is a vector of the stop words from &lt;code&gt;tidytext&lt;/code&gt;), ii.) URL links (&lt;code&gt;url_words&lt;/code&gt;), iii.) and numeric strings. Note that the argument in &lt;code&gt;anti_join()&lt;/code&gt; must not be a vector! Finally, I filtered any words of lower length or equal to 3 characters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#tidytext
url_words &amp;lt;- tibble(
  word = c(&amp;quot;https&amp;quot;,&amp;quot;http&amp;quot;)) #filternig weblinks

tidy_data &amp;lt;- dataset %&amp;gt;% unnest_tokens(word, comment) %&amp;gt;% anti_join(stop_words, by=&amp;quot;word&amp;quot;) %&amp;gt;%  anti_join(url_words, by=&amp;quot;word&amp;quot;) %&amp;gt;% 
# filtering &amp;quot;numeric&amp;quot; words  
anti_join(tibble(word = as.character (c(1:10000000))), by=&amp;quot;word&amp;quot;) %&amp;gt;%
# stop word already filter but there were still redundent words, e.g. nchar(word) &amp;lt; 3
  filter(nchar(word)&amp;gt;=3) 

rm(url_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, I created &lt;code&gt;data_sentiment&lt;/code&gt; data frame from &lt;code&gt;tidy_data.&lt;/code&gt; I achieved that by filtering and joining the data from &lt;code&gt;nrc&lt;/code&gt; lexicon. I had chosen &lt;code&gt;nrc&lt;/code&gt; because it contains a variety of emotions associated with the word. I also created &lt;code&gt;author_total&lt;/code&gt;, i.e. how many words in total has each author posted to the forum. &lt;code&gt;inner_join(get_sentiments(&amp;quot;nrc&amp;quot;), by=&amp;quot;word&amp;quot;)&lt;/code&gt; joins &lt;code&gt;nrc&lt;/code&gt; lexicon with sentiments to &lt;code&gt;tidy_data&lt;/code&gt;. Note that &lt;code&gt;inner_join()&lt;/code&gt; filters out any words out of the intersection of any two data sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_sentiment &amp;lt;- tidy_data %&amp;gt;% 
    # Group by author
    group_by(author) %&amp;gt;% 
    # Define a new column author_total; i.e. how many words an author has posted
    mutate(author_total=n()) %&amp;gt;%
    ungroup() %&amp;gt;%
    # Implement sentiment analysis with the NRC lexicon
    inner_join(get_sentiments(&amp;quot;nrc&amp;quot;), by=&amp;quot;word&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These steps above summarize my preprocessing and data cleaning strategies. Having three different data sets, there was nothing else impeding the analysis. From this step, it was easy to explore and analyze the data more precisely.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis-eda&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Exploratory Data Analysis (EDA)&lt;/h6&gt;
&lt;p&gt;Collecting any time varying variable, one should always know the time span.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;range(dataset$date) #time span&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2018-10-17 04:17:53 UTC&amp;quot; &amp;quot;2019-03-10 04:45:13 UTC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;range(dataset$date)[2]-range(dataset$date)[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 144.019 days&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interesting! The very first comment on the forum was posted on the day when legalization happened, i.e. &lt;code&gt;2018-10-17&lt;/code&gt;. On the other hand, the last comment was submitted (in time of writing this post) in March 2019. This tells us the discussion has been alive for 145 days, i.e. almost 5 months. Initially, I thought that the time frame would be sufficient for interesting observations and conclusions. However, the frequency of new comments matters as well. Is there a representative number of submitted comments for each month? One way to answer the question would be a visual examination.&lt;/p&gt;
&lt;p&gt;The following plot displays the count of new comments in time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset %&amp;gt;% select(date,comment) %&amp;gt;% 
  mutate(date = round_date(date, &amp;quot;1 day&amp;quot;))  %&amp;gt;% group_by(date) %&amp;gt;% mutate(n_comments = n()) %&amp;gt;% 
  # filter(date &amp;lt; ymd(&amp;quot;2018-10-25&amp;quot;)) %&amp;gt;%
  # Had to round  up the date object into &amp;quot;week&amp;quot; units, otherwise grouping a mutating would not work   (too narrow interval)

  ggplot(aes(date,n_comments)) +
  geom_line(linetype = 1)+
  ggtitle(&amp;quot;Number of Comments in Time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Numeber%20of%20Comments%20in%20Time%20and%20PLOT-1.png&#34; width=&#34;672&#34; /&gt; Unfortunately, the discussion was truly “alive” only a few weeks after October 17th. After the end of October 2018, new comments per day or even month were marginal.&lt;/p&gt;
&lt;p&gt;Even though I could not answer my initial questions in the full extent, I could focus on the cannabis consumers’ initial attitude. So how many comments were posted there?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(dataset) # n comments&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 460&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Besides the number of comments, and the frequency of new ones, the total of contributors to the discussion should be represented as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlevels(dataset$author) #n levels&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 166&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relative to the total number of comments, I maintain that enough authors have contributed into the discussion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;text-analysis&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Text Analysis&lt;/h6&gt;
&lt;p&gt;Did you already examine this post’s cover picture? It is the word cloud from &lt;code&gt;wordcloud2&lt;/code&gt; package and it displays the most frequent words in the discussion. By default, words with the highest frequency are centered in the middle of the plot. I like word clouds because they are great tools for very first inspection of text data. You can easily infer the subject being discussed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(wordcloud2)

tidy_data %&amp;gt;% select(word) %&amp;gt;% count(word,sort=T) %&amp;gt;%
wordcloud2(backgroundColor = &amp;quot;black&amp;quot;, color = &amp;quot;green&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;wordcloud2 html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;word&#34;:[&#34;cannabis&#34;,&#34;legal&#34;,&#34;ocs&#34;,&#34;people&#34;,&#34;time&#34;,&#34;day&#34;,&#34;weed&#34;,&#34;shipping&#34;,&#34;visa&#34;,&#34;card&#34;,&#34;market&#34;,&#34;buy&#34;,&#34;alberta&#34;,&#34;lol&#34;,&#34;ontario&#34;,&#34;strains&#34;,&#34;government&#34;,&#34;online&#34;,&#34;debit&#34;,&#34;black&#34;,&#34;credit&#34;,&#34;oil&#34;,&#34;pretty&#34;,&#34;update&#34;,&#34;canada&#34;,&#34;store&#34;,&#34;website&#34;,&#34;yeah&#34;,&#34;it’s&#34;,&#34;morning&#34;,&#34;prices&#34;,&#34;bought&#34;,&#34;hours&#34;,&#34;line&#34;,&#34;seeds&#34;,&#34;data&#34;,&#34;days&#34;,&#34;don’t&#34;,&#34;illegal&#34;,&#34;street&#34;,&#34;border&#34;,&#34;flower&#34;,&#34;home&#34;,&#34;medical&#34;,&#34;post&#34;,&#34;smoke&#34;,&#34;stuff&#34;,&#34;trade&#34;,&#34;allowed&#34;,&#34;guys&#34;,&#34;i’m&#34;,&#34;lot&#34;,&#34;shipped&#34;,&#34;wait&#34;,&#34;bank&#34;,&#34;canadian&#34;,&#34;laws&#34;,&#34;night&#34;,&#34;price&#34;,&#34;smoking&#34;,&#34;sold&#34;,&#34;stock&#34;,&#34;that’s&#34;,&#34;thinking&#34;,&#34;week&#34;,&#34;cbd&#34;,&#34;guess&#34;,&#34;happy&#34;,&#34;legally&#34;,&#34;mine&#34;,&#34;policy&#34;,&#34;pre&#34;,&#34;retail&#34;,&#34;stores&#34;,&#34;strain&#34;,&#34;address&#34;,&#34;bud&#34;,&#34;call&#34;,&#34;called&#34;,&#34;can’t&#34;,&#34;company&#34;,&#34;delivery&#34;,&#34;drive&#34;,&#34;edibles&#34;,&#34;feel&#34;,&#34;found&#34;,&#34;information&#34;,&#34;issue&#34;,&#34;locations&#34;,&#34;mastercard&#34;,&#34;ocs.ca&#34;,&#34;packaging&#34;,&#34;prepaid&#34;,&#34;province&#34;,&#34;public&#34;,&#34;purchase&#34;,&#34;read&#34;,&#34;sell&#34;,&#34;selling&#34;,&#34;ship&#34;,&#34;smoked&#34;,&#34;specifically&#34;,&#34;start&#34;,&#34;super&#34;,&#34;you’re&#34;,&#34;amount&#34;,&#34;bit&#34;,&#34;buds&#34;,&#34;cost&#34;,&#34;edit&#34;,&#34;fee&#34;,&#34;google&#34;,&#34;grey&#34;,&#34;hope&#34;,&#34;kush&#34;,&#34;law&#34;,&#34;legalization&#34;,&#34;marijuana&#34;,&#34;medreleaf&#34;,&#34;money&#34;,&#34;postal&#34;,&#34;question&#34;,&#34;rolling&#34;,&#34;sativa&#34;,&#34;service&#34;,&#34;statement&#34;,&#34;vanilla&#34;,&#34;waiting&#34;,&#34;worth&#34;,&#34;american&#34;,&#34;aurora&#34;,&#34;based&#34;,&#34;coming&#34;,&#34;considered&#34;,&#34;dont&#34;,&#34;fine&#34;,&#34;flowr&#34;,&#34;half&#34;,&#34;haze&#34;,&#34;industry&#34;,&#34;info&#34;,&#34;live&#34;,&#34;middle&#34;,&#34;montreal&#34;,&#34;news&#34;,&#34;nice&#34;,&#34;option&#34;,&#34;paid&#34;,&#34;park&#34;,&#34;pink&#34;,&#34;plants&#34;,&#34;pot&#34;,&#34;products&#34;,&#34;reason&#34;,&#34;search&#34;,&#34;security&#34;,&#34;site&#34;,&#34;thc&#34;,&#34;trading&#34;,&#34;vape&#34;,&#34;weedmd&#34;,&#34;weekend&#34;,&#34;access&#34;,&#34;actual&#34;,&#34;ago&#34;,&#34;avenue&#34;,&#34;business&#34;,&#34;buying&#34;,&#34;cards&#34;,&#34;coffee&#34;,&#34;concentrate&#34;,&#34;concentrates&#34;,&#34;cool&#34;,&#34;country&#34;,&#34;customer&#34;,&#34;doubt&#34;,&#34;edison&#34;,&#34;eigth&#34;,&#34;experience&#34;,&#34;free&#34;,&#34;fuck&#34;,&#34;ghost&#34;,&#34;gift&#34;,&#34;gram&#34;,&#34;green&#34;,&#34;grow&#34;,&#34;growing&#34;,&#34;guy&#34;,&#34;hash&#34;,&#34;helps&#34;,&#34;hotel&#34;,&#34;ill&#34;,&#34;illicit&#34;,&#34;makes&#34;,&#34;oils&#34;,&#34;page&#34;,&#34;past&#34;,&#34;phone&#34;,&#34;picture&#34;,&#34;pictures&#34;,&#34;power&#34;,&#34;pricing&#34;,&#34;processing&#34;,&#34;provinces&#34;,&#34;purchased&#34;,&#34;quality&#34;,&#34;questions&#34;,&#34;roll&#34;,&#34;saturday&#34;,&#34;seed&#34;,&#34;selection&#34;,&#34;shatter&#34;,&#34;shop&#34;,&#34;similar&#34;,&#34;smell&#34;,&#34;sort&#34;,&#34;sounds&#34;,&#34;specific&#34;,&#34;surprised&#34;,&#34;taxes&#34;,&#34;thread&#34;,&#34;times&#34;,&#34;told&#34;,&#34;tomorrow&#34;,&#34;train&#34;,&#34;travel&#34;,&#34;type&#34;,&#34;user&#34;,&#34;wondering&#34;,&#34;yesterday&#34;,&#34;3.5g&#34;,&#34;act&#34;,&#34;aged&#34;,&#34;ahead&#34;,&#34;amazing&#34;,&#34;amex&#34;,&#34;anxiety&#34;,&#34;assume&#34;,&#34;bad&#34;,&#34;ban&#34;,&#34;banking&#34;,&#34;broken&#34;,&#34;burn&#34;,&#34;calgary&#34;,&#34;canopy&#34;,&#34;changed&#34;,&#34;check&#34;,&#34;chill&#34;,&#34;code&#34;,&#34;complain&#34;,&#34;completely&#34;,&#34;container&#34;,&#34;declined&#34;,&#34;doesn’t&#34;,&#34;door&#34;,&#34;drinking&#34;,&#34;email&#34;,&#34;entry&#34;,&#34;gonna&#34;,&#34;grabbed&#34;,&#34;growth&#34;,&#34;hack&#34;,&#34;haha&#34;,&#34;hear&#34;,&#34;hey&#34;,&#34;house&#34;,&#34;indica&#34;,&#34;issues&#34;,&#34;l&#39;acadie&#34;,&#34;left&#34;,&#34;licenced&#34;,&#34;limited&#34;,&#34;local&#34;,&#34;location&#34;,&#34;love&#34;,&#34;luck&#34;,&#34;main&#34;,&#34;major&#34;,&#34;means&#34;,&#34;minutes&#34;,&#34;oct&#34;,&#34;officers&#34;,&#34;organigram&#34;,&#34;pay&#34;,&#34;paying&#34;,&#34;person&#34;,&#34;pick&#34;,&#34;plain&#34;,&#34;police&#34;,&#34;privacy&#34;,&#34;product&#34;,&#34;quarter&#34;,&#34;quick&#34;,&#34;rafael&#34;,&#34;ready&#34;,&#34;recently&#34;,&#34;reddit&#34;,&#34;redecan&#34;,&#34;safe&#34;,&#34;san&#34;,&#34;sense&#34;,&#34;shit&#34;,&#34;sorted&#34;,&#34;sour&#34;,&#34;south&#34;,&#34;started&#34;,&#34;statements&#34;,&#34;sticking&#34;,&#34;tide&#34;,&#34;tobacco&#34;,&#34;top&#34;,&#34;trouble&#34;,&#34;ultra&#34;,&#34;vertical&#34;,&#34;walk&#34;,&#34;websites&#34;,&#34;2gs&#34;,&#34;3.5&#34;,&#34;admits&#34;,&#34;advice&#34;,&#34;age&#34;,&#34;albertacannabis.org&#34;,&#34;americans&#34;,&#34;answer&#34;,&#34;aphria&#34;,&#34;apparently&#34;,&#34;apply&#34;,&#34;arrive&#34;,&#34;banned&#34;,&#34;billing&#34;,&#34;bong&#34;,&#34;brands&#34;,&#34;bulk&#34;,&#34;campaign&#34;,&#34;cannafarms&#34;,&#34;catherine&#34;,&#34;cbp&#34;,&#34;centre&#34;,&#34;charge&#34;,&#34;christmas&#34;,&#34;cities&#34;,&#34;comment&#34;,&#34;comments&#34;,&#34;concern&#34;,&#34;concerned&#34;,&#34;continue&#34;,&#34;cops&#34;,&#34;correct&#34;,&#34;crazy&#34;,&#34;criminal&#34;,&#34;current&#34;,&#34;damn&#34;,&#34;deal&#34;,&#34;dealer&#34;,&#34;deliver&#34;,&#34;demographic&#34;,&#34;destroy&#34;,&#34;dry&#34;,&#34;dude&#34;,&#34;due&#34;,&#34;earlier&#34;,&#34;easier&#34;,&#34;eat&#34;,&#34;expect&#34;,&#34;fairly&#34;,&#34;fear&#34;,&#34;federal&#34;,&#34;figured&#34;,&#34;fixed&#34;,&#34;food&#34;,&#34;forgot&#34;,&#34;fresh&#34;,&#34;friday&#34;,&#34;frustrating&#34;,&#34;fucked&#34;,&#34;grams&#34;,&#34;grown&#34;,&#34;guards&#34;,&#34;guessing&#34;,&#34;happen&#34;,&#34;happening&#34;,&#34;hard&#34;,&#34;heads&#34;,&#34;heard&#34;,&#34;heavy&#34;,&#34;hit&#34;,&#34;hold&#34;,&#34;holds&#34;,&#34;hoping&#34;,&#34;impressed&#34;,&#34;included&#34;,&#34;isn’t&#34;,&#34;job&#34;,&#34;keeping&#34;,&#34;kids&#34;,&#34;laid&#34;,&#34;lcbo&#34;,&#34;leafs&#34;,&#34;level&#34;,&#34;license&#34;,&#34;licenses&#34;,&#34;light&#34;,&#34;linked&#34;,&#34;literally&#34;,&#34;lp&#39;s&#34;,&#34;marketing&#34;,&#34;matter&#34;,&#34;meant&#34;,&#34;medicine&#34;,&#34;mind&#34;,&#34;missed&#34;,&#34;monday&#34;,&#34;month&#34;,&#34;mortar&#34;,&#34;notification&#34;,&#34;nova&#34;,&#34;nugs&#34;,&#34;office&#34;,&#34;outrageous&#34;,&#34;overpriced&#34;,&#34;packaged&#34;,&#34;password&#34;,&#34;patriot&#34;,&#34;pen&#34;,&#34;permit&#34;,&#34;personally&#34;,&#34;picked&#34;,&#34;pics&#34;,&#34;pipe&#34;,&#34;plane&#34;,&#34;plant&#34;,&#34;posts&#34;,&#34;probable&#34;,&#34;producer&#34;,&#34;psychedelic&#34;,&#34;quebec&#34;,&#34;real&#34;,&#34;reasons&#34;,&#34;received&#34;,&#34;recommend&#34;,&#34;recommendations&#34;,&#34;records&#34;,&#34;reddit&#39;s&#34;,&#34;registered&#34;,&#34;related&#34;,&#34;rolled&#34;,&#34;royal&#34;,&#34;run&#34;,&#34;sale&#34;,&#34;scale&#34;,&#34;servers&#34;,&#34;shadows&#34;,&#34;shot&#34;,&#34;sit&#34;,&#34;slip&#34;,&#34;snoop&#34;,&#34;social&#34;,&#34;solei&#34;,&#34;spam&#34;,&#34;steal&#34;,&#34;stick&#34;,&#34;supply&#34;,&#34;supposed&#34;,&#34;talking&#34;,&#34;telling&#34;,&#34;terrible&#34;,&#34;there’s&#34;,&#34;they’re&#34;,&#34;tilray&#34;,&#34;toronto&#34;,&#34;tracking&#34;,&#34;trip&#34;,&#34;u.s&#34;,&#34;updated&#34;,&#34;version&#34;,&#34;white&#34;,&#34;won’t&#34;,&#34;word&#34;,&#34;workers&#34;,&#34;wouldn’t&#34;,&#34;yep&#34;,&#34;18th&#34;,&#34;20am&#34;,&#34;25th&#34;,&#34;31.99&#34;,&#34;4th&#34;,&#34;9.24&#34;,&#34;9.95&#34;,&#34;abcann&#34;,&#34;absurd&#34;,&#34;account&#34;,&#34;add&#34;,&#34;added&#34;,&#34;adding&#34;,&#34;afraid&#34;,&#34;aglc.ca&#34;,&#34;airports&#34;,&#34;alta&#34;,&#34;amazed&#34;,&#34;amounts&#34;,&#34;amsterdam&#34;,&#34;anymore&#34;,&#34;apps&#34;,&#34;article&#34;,&#34;ass&#34;,&#34;avoid&#34;,&#34;aware&#34;,&#34;awesome&#34;,&#34;balance&#34;,&#34;basically&#34;,&#34;basis&#34;,&#34;beans&#34;,&#34;bed&#34;,&#34;beleave&#34;,&#34;birthday&#34;,&#34;block&#34;,&#34;blvd&#34;,&#34;boat&#34;,&#34;booze&#34;,&#34;bottle&#34;,&#34;boutique&#34;,&#34;breaks&#34;,&#34;brick&#34;,&#34;bright&#34;,&#34;bring&#34;,&#34;bro&#34;,&#34;broadway&#34;,&#34;btw&#34;,&#34;buddy&#34;,&#34;bunch&#34;,&#34;burns&#34;,&#34;busy&#34;,&#34;bylaw&#34;,&#34;cancel&#34;,&#34;canna&#34;,&#34;canntrust&#34;,&#34;cap&#34;,&#34;capsules&#34;,&#34;car&#34;,&#34;carry&#34;,&#34;cash&#34;,&#34;casual&#34;,&#34;casuals&#34;,&#34;caught&#34;,&#34;chance&#34;,&#34;charges&#34;,&#34;cheaper&#34;,&#34;checked&#34;,&#34;checkout&#34;,&#34;cheers&#34;,&#34;choices&#34;,&#34;chopping&#34;,&#34;claim&#34;,&#34;clamping&#34;,&#34;clones&#34;,&#34;closer&#34;,&#34;closing&#34;,&#34;coast&#34;,&#34;commerce&#34;,&#34;companies&#34;,&#34;competition&#34;,&#34;competitive&#34;,&#34;confident&#34;,&#34;confirm&#34;,&#34;congratulations&#34;,&#34;consume&#34;,&#34;consumption&#34;,&#34;couch&#34;,&#34;countryside&#34;,&#34;couple&#34;,&#34;court.happy&#34;,&#34;cove&#34;,&#34;crack&#34;,&#34;crashed&#34;,&#34;created&#34;,&#34;cronos&#34;,&#34;crossing&#34;,&#34;dads&#34;,&#34;danksgiving&#34;,&#34;date&#34;,&#34;deals&#34;,&#34;decent&#34;,&#34;demand&#34;,&#34;didn’t&#34;,&#34;digits&#34;,&#34;disappear&#34;,&#34;dna&#34;,&#34;dollar&#34;,&#34;dollars&#34;,&#34;dream&#34;,&#34;drink&#34;,&#34;dumb&#34;,&#34;dunno&#34;,&#34;east&#34;,&#34;easy&#34;,&#34;eaton&#34;,&#34;edible&#34;,&#34;edmonton&#34;,&#34;effect&#34;,&#34;eighth&#34;,&#34;eighths&#34;,&#34;emblem&#34;,&#34;empty&#34;,&#34;encrypted&#34;,&#34;ents&#34;,&#34;error&#34;,&#34;evening&#34;,&#34;exchange&#34;,&#34;excited&#34;,&#34;exciting&#34;,&#34;existing&#34;,&#34;explicitly&#34;,&#34;export&#34;,&#34;eye&#34;,&#34;facilitate&#34;,&#34;failed&#34;,&#34;fair&#34;,&#34;family&#34;,&#34;fast&#34;,&#34;fees&#34;,&#34;feet&#34;,&#34;field&#34;,&#34;figure&#34;,&#34;fire&#34;,&#34;fireside&#34;,&#34;flexdelivery&#34;,&#34;footer&#34;,&#34;foreign&#34;,&#34;fort&#34;,&#34;front&#34;,&#34;funny&#34;,&#34;fyi&#34;,&#34;game&#34;,&#34;genetics&#34;,&#34;giving&#34;,&#34;goal&#34;,&#34;god&#34;,&#34;gotta&#34;,&#34;govt&#34;,&#34;grabbing&#34;,&#34;grove&#34;,&#34;gst&#34;,&#34;hahaha&#34;,&#34;hands&#34;,&#34;harvest&#34;,&#34;haven&#34;,&#34;haven’t&#34;,&#34;hexo&#34;,&#34;highly&#34;,&#34;hits&#34;,&#34;honest&#34;,&#34;household&#34;,&#34;huge&#34;,&#34;hydropothecary&#34;,&#34;i’ve&#34;,&#34;ice&#34;,&#34;ideally&#34;,&#34;idiots&#34;,&#34;images&#34;,&#34;imagine&#34;,&#34;imgur.com&#34;,&#34;import&#34;,&#34;individual&#34;,&#34;insulted&#34;,&#34;interim&#34;,&#34;internet&#34;,&#34;involved&#34;,&#34;irisa&#34;,&#34;jars&#34;,&#34;joint&#34;,&#34;joints&#34;,&#34;kinda&#34;,&#34;knowing&#34;,&#34;label&#34;,&#34;landed&#34;,&#34;landing&#34;,&#34;lands&#34;,&#34;larger&#34;,&#34;lazy&#34;,&#34;lbs&#34;,&#34;lead&#34;,&#34;learn&#34;,&#34;leave&#34;,&#34;legalized&#34;,&#34;legislation&#34;,&#34;licensed&#34;,&#34;life&#34;,&#34;lights&#34;,&#34;liiv&#34;,&#34;lined&#34;,&#34;lines&#34;,&#34;lineup&#34;,&#34;list&#34;,&#34;load&#34;,&#34;loaded&#34;,&#34;logo&#34;,&#34;lower&#34;,&#34;lowest&#34;,&#34;lps&#34;,&#34;lungs&#34;,&#34;macleod&#34;,&#34;markham&#34;,&#34;massive&#34;,&#34;meantime&#34;,&#34;mention&#34;,&#34;midnight&#34;,&#34;mix&#34;,&#34;model&#34;,&#34;mom&#34;,&#34;mynslc.com&#34;,&#34;nb.com&#34;,&#34;nervous&#34;,&#34;newstrike&#34;,&#34;nope&#34;,&#34;normal&#34;,&#34;notice&#34;,&#34;notices&#34;,&#34;nslc&#34;,&#34;oaks&#34;,&#34;october&#34;,&#34;offense&#34;,&#34;offer&#34;,&#34;offering&#34;,&#34;opportunity&#34;,&#34;ounce&#34;,&#34;oven&#34;,&#34;packed&#34;,&#34;pardoning&#34;,&#34;patrol&#34;,&#34;personal&#34;,&#34;physical&#34;,&#34;planned&#34;,&#34;planning&#34;,&#34;plastic&#34;,&#34;poor&#34;,&#34;possession&#34;,&#34;pound&#34;,&#34;pour&#34;,&#34;press&#34;,&#34;priced&#34;,&#34;prime&#34;,&#34;private&#34;,&#34;produce&#34;,&#34;proper&#34;,&#34;protected&#34;,&#34;purchases&#34;,&#34;purchasing&#34;,&#34;purulator&#34;,&#34;putting&#34;,&#34;quantities&#34;,&#34;quarters&#34;,&#34;québec&#34;,&#34;queue&#34;,&#34;railway&#34;,&#34;ran&#34;,&#34;randylaheyjr&#34;,&#34;reasonable&#34;,&#34;receiving&#34;,&#34;rediquette&#34;,&#34;refuse&#34;,&#34;registration&#34;,&#34;regulations&#34;,&#34;report&#34;,&#34;reporting&#34;,&#34;reserve&#34;,&#34;response&#34;,&#34;rest&#34;,&#34;restrictions&#34;,&#34;retailer&#34;,&#34;ripped&#34;,&#34;road&#34;,&#34;rolls&#34;,&#34;rosin&#34;,&#34;rue&#34;,&#34;rule&#34;,&#34;sask&#34;,&#34;school&#34;,&#34;searching&#34;,&#34;sells&#34;,&#34;separate&#34;,&#34;server&#34;,&#34;shake&#34;,&#34;shark&#34;,&#34;shelf&#34;,&#34;shift&#34;,&#34;ships&#34;,&#34;shitty&#34;,&#34;shock&#34;,&#34;shoppers&#34;,&#34;shopping&#34;,&#34;shops&#34;,&#34;shrink&#34;,&#34;shut&#34;,&#34;sign&#34;,&#34;sites&#34;,&#34;sitting&#34;,&#34;slightly&#34;,&#34;smoker&#34;,&#34;smokes&#34;,&#34;soil&#34;,&#34;source&#34;,&#34;sovereignty&#34;,&#34;spend&#34;,&#34;sprays&#34;,&#34;spruce&#34;,&#34;sqdc&#34;,&#34;starseed&#34;,&#34;sticky&#34;,&#34;stomach&#34;,&#34;stoned&#34;,&#34;stop&#34;,&#34;stored&#34;,&#34;story&#34;,&#34;strategy&#34;,&#34;strawberry&#34;,&#34;strong&#34;,&#34;subs&#34;,&#34;substance&#34;,&#34;subtle&#34;,&#34;successfully&#34;,&#34;suddenly&#34;,&#34;suffering&#34;,&#34;suppose&#34;,&#34;symbl&#34;,&#34;system&#34;,&#34;taking&#34;,&#34;target&#34;,&#34;tax&#34;,&#34;term&#34;,&#34;terms&#34;,&#34;terrascend&#34;,&#34;that&#39;d&#34;,&#34;ton&#34;,&#34;tons&#34;,&#34;tourist&#34;,&#34;town&#34;,&#34;trades&#34;,&#34;traffic&#34;,&#34;trail&#34;,&#34;transporting&#34;,&#34;true&#34;,&#34;tuesday&#34;,&#34;united&#34;,&#34;users&#34;,&#34;usual&#34;,&#34;valley&#34;,&#34;vaping&#34;,&#34;vapor&#34;,&#34;verification&#34;,&#34;vie&#34;,&#34;violates&#34;,&#34;visited&#34;,&#34;visiting&#34;,&#34;wake&#34;,&#34;warehouse&#34;,&#34;warning&#34;,&#34;warrant&#34;,&#34;watch&#34;,&#34;we’re&#34;,&#34;wednesday&#34;,&#34;weird&#34;,&#34;west&#34;,&#34;who’ve&#34;,&#34;wide&#34;,&#34;wipe&#34;,&#34;woodstock&#34;,&#34;world&#34;,&#34;worry&#34;,&#34;www.cannabis&#34;,&#34;yorkton&#34;,&#34;youre&#34;,&#34;0.01&#34;,&#34;0.5g&#34;,&#34;01am&#34;,&#34;0a2moosomin&#34;,&#34;0e0estevan&#34;,&#34;0v2martensville&#34;,&#34;1.4136368&#34;,&#34;1.4867420&#34;,&#34;100,000&#34;,&#34;11pm&#34;,&#34;15am&#34;,&#34;17some&#34;,&#34;17th&#34;,&#34;195c&#34;,&#34;1ec5c87b&#34;,&#34;1st&#34;,&#34;200mg&#34;,&#34;20celebrate&#34;,&#34;20pm&#34;,&#34;21st&#34;,&#34;262.36&#34;,&#34;27am&#34;,&#34;27th&#34;,&#34;2pm&#34;,&#34;3.44g&#34;,&#34;3.50&#34;,&#34;3.5gs&#34;,&#34;30,000&#34;,&#34;30am&#34;,&#34;30g&#34;,&#34;39.99&#34;,&#34;3n0wiid&#34;,&#34;4.20&#34;,&#34;4.5&#34;,&#34;4.85&#34;,&#34;421a&#34;,&#34;48.95&#34;,&#34;49.95&#34;,&#34;5.50&#34;,&#34;5.5g&#34;,&#34;50,000&#34;,&#34;50k&#34;,&#34;56.50&#34;,&#34;56amontariohttps&#34;,&#34;5mg&#34;,&#34;6.95&#34;,&#34;62.99&#34;,&#34;6am&#34;,&#34;6pm&#34;,&#34;7.5g&#34;,&#34;7.95&#34;,&#34;77.63&#34;,&#34;78.70&#34;,&#34;8th&#34;,&#34;8ths&#34;,&#34;9p9ens&#34;,&#34;aaaa&#34;,&#34;aah&#34;,&#34;about.they&#34;,&#34;abroad&#34;,&#34;absolutely&#34;,&#34;absorbed&#34;,&#34;abuser&#34;,&#34;accept&#34;,&#34;acceptable&#34;,&#34;accepted&#34;,&#34;access.and&#34;,&#34;accessories&#34;,&#34;ace&#34;,&#34;acmpr&#34;,&#34;acquire&#34;,&#34;acquired&#34;,&#34;act.pot&#34;,&#34;activation&#34;,&#34;activities&#34;,&#34;acts&#34;,&#34;adam&#34;,&#34;addict&#34;,&#34;addressed&#34;,&#34;adds&#34;,&#34;admissible&#34;,&#34;admission&#34;,&#34;adult&#34;,&#34;advance&#34;,&#34;advertising&#34;,&#34;afaik&#34;,&#34;agents&#34;,&#34;aglc&#34;,&#34;agree&#34;,&#34;agreed&#34;,&#34;ahhh&#34;,&#34;airline&#34;,&#34;airplane&#34;,&#34;airspace&#34;,&#34;airtight&#34;,&#34;albertacannabis&#34;,&#34;albertacannabis.orghere&#34;,&#34;albertahttps&#34;,&#34;albertfire&#34;,&#34;alien&#34;,&#34;alive&#34;,&#34;amazing.both&#34;,&#34;amazing.i&#34;,&#34;amazon&#34;,&#34;america&#34;,&#34;america&#39;s&#34;,&#34;analysis&#34;,&#34;angles&#34;,&#34;angyfox13&#34;,&#34;announced&#34;,&#34;annoying&#34;,&#34;answered&#34;,&#34;apologies&#34;,&#34;appeal&#34;,&#34;applies&#34;,&#34;appreciated&#34;,&#34;approach&#34;,&#34;approved&#34;,&#34;area.edit&#34;,&#34;aren’t&#34;,&#34;arrange&#34;,&#34;arriving&#34;,&#34;arrogant&#34;,&#34;ash&#34;,&#34;assessment&#34;,&#34;asshats&#34;,&#34;assumed&#34;,&#34;assuming&#34;,&#34;assure&#34;,&#34;athabasca&#34;,&#34;atleast&#34;,&#34;atm&#34;,&#34;attempt&#34;,&#34;attitude&#34;,&#34;attracting&#34;,&#34;august&#34;,&#34;aussie&#34;,&#34;australian&#34;,&#34;authorized&#34;,&#34;ave&#34;,&#34;average&#34;,&#34;awake&#34;,&#34;aws&#34;,&#34;b8015&#34;,&#34;babies&#34;,&#34;backlogged&#34;,&#34;backwards&#34;,&#34;bag&#34;,&#34;bake&#34;,&#34;baked&#34;,&#34;banks&#34;,&#34;barter&#34;,&#34;base&#34;,&#34;bastards&#34;,&#34;batteries&#34;,&#34;battleford&#34;,&#34;bcuz&#34;,&#34;beatles&#34;,&#34;beer&#34;,&#34;beginner.weedmd&#34;,&#34;bellerose&#34;,&#34;bench&#34;,&#34;benefit&#34;,&#34;benson&#34;,&#34;bet&#34;,&#34;bias&#34;,&#34;bickel&#34;,&#34;biohazard&#34;,&#34;biosector&#34;,&#34;bit.i&#34;,&#34;bits&#34;,&#34;blaze&#34;,&#34;blend&#34;,&#34;board&#34;,&#34;booo&#34;,&#34;booooo&#34;,&#34;boost&#34;,&#34;bootleggers&#34;,&#34;bore&#34;,&#34;bored&#34;,&#34;born&#34;,&#34;bother&#34;,&#34;boulevard&#34;,&#34;box&#34;,&#34;boxes&#34;,&#34;brain&#34;,&#34;brand&#34;,&#34;brandace&#34;,&#34;branding&#34;,&#34;brandno&#34;,&#34;breaking&#34;,&#34;breeders&#34;,&#34;bringing&#34;,&#34;brings&#34;,&#34;brother&#34;,&#34;browse&#34;,&#34;browser&#34;,&#34;brunswick&#34;,&#34;brunswickhttps&#34;,&#34;brutal&#34;,&#34;bubbler&#34;,&#34;bucks&#34;,&#34;buisness&#34;,&#34;bullshit&#34;,&#34;bummed&#34;,&#34;bureaucrat&#34;,&#34;buried&#34;,&#34;businesses&#34;,&#34;butter&#34;,&#34;buyer&#34;,&#34;bylaws&#34;,&#34;c45&#34;,&#34;cache&#34;,&#34;cake&#34;,&#34;calgarynova&#34;,&#34;calgarysmall&#34;,&#34;caller&#34;,&#34;calling&#34;,&#34;calls&#34;,&#34;canabis&#34;,&#34;canadagrows&#34;,&#34;canadas&#34;,&#34;canadianmoms&#34;,&#34;canadians&#34;,&#34;canadients&#34;,&#34;cancellation&#34;,&#34;cancelled&#34;,&#34;cancelling&#34;,&#34;cannabisocs&#34;,&#34;cannabutter&#34;,&#34;cannaflower&#34;,&#34;caps&#34;,&#34;card.edit&#34;,&#34;card.so&#34;,&#34;cardholder&#34;,&#34;care&#34;,&#34;cart&#34;,&#34;catching&#34;,&#34;catsa&#34;,&#34;cause.you&#34;,&#34;caused&#34;,&#34;centennial&#34;,&#34;cents&#34;,&#34;cerebral&#34;,&#34;challenging&#34;,&#34;chamber&#34;,&#34;charged&#34;,&#34;charging&#34;,&#34;charlottetown&#34;,&#34;chatted&#34;,&#34;cheap&#34;,&#34;checking&#34;,&#34;checks&#34;,&#34;child&#34;,&#34;childproof&#34;,&#34;children&#34;,&#34;china&#34;,&#34;choice&#34;,&#34;chose&#34;,&#34;cigarette&#34;,&#34;circulated&#34;,&#34;citation&#34;,&#34;cite&#34;,&#34;citizen&#34;,&#34;citizen.i&#34;,&#34;citizens&#34;,&#34;city&#34;,&#34;clandestine&#34;,&#34;clarify&#34;,&#34;clearing&#34;,&#34;click&#34;,&#34;clinic&#34;,&#34;clone&#34;,&#34;close&#34;,&#34;closet&#34;,&#34;clue&#34;,&#34;coconut&#34;,&#34;coffeeshops&#34;,&#34;coils&#34;,&#34;cold&#34;,&#34;colors&#34;,&#34;columbia&#34;,&#34;columbiaonline&#34;,&#34;comanies&#34;,&#34;comfortable&#34;,&#34;comments.hope&#34;,&#34;comments.thanks&#34;,&#34;committed&#34;,&#34;committing&#34;,&#34;common&#34;,&#34;communication&#34;,&#34;community&#34;,&#34;compare&#34;,&#34;competitively&#34;,&#34;competitors.the&#34;,&#34;complaining&#34;,&#34;complicated&#34;,&#34;complies&#34;,&#34;complies.no&#34;,&#34;concierge&#34;,&#34;concrete&#34;,&#34;condo&#34;,&#34;confirmation&#34;,&#34;confusing&#34;,&#34;congrats&#34;,&#34;connect&#34;,&#34;connection&#34;,&#34;conspiracy&#34;,&#34;constitute&#34;,&#34;construction&#34;,&#34;consultandgrow.cathey&#34;,&#34;consumers&#34;,&#34;contact&#34;,&#34;containers&#34;,&#34;continent&#34;,&#34;continuing&#34;,&#34;controlled&#34;,&#34;convenient&#34;,&#34;convert&#34;,&#34;convicted&#34;,&#34;cop&#34;,&#34;corner&#34;,&#34;corp&#34;,&#34;corporations&#34;,&#34;couldn’t&#34;,&#34;countdowns&#34;,&#34;countries.any&#34;,&#34;courtroom&#34;,&#34;covering&#34;,&#34;covers&#34;,&#34;crappy&#34;,&#34;crash&#34;,&#34;create&#34;,&#34;creative&#34;,&#34;crept&#34;,&#34;criminally&#34;,&#34;crippling&#34;,&#34;critical&#34;,&#34;cross&#34;,&#34;crosses&#34;,&#34;crossing.jeez&#34;,&#34;crossings&#34;,&#34;crotch&#34;,&#34;crowded&#34;,&#34;crown&#34;,&#34;curious&#34;,&#34;customers&#34;,&#34;customs&#34;,&#34;d&#39;octobre&#34;,&#34;d290&#34;,&#34;dab&#34;,&#34;dad&#34;,&#34;daily&#34;,&#34;dates&#34;,&#34;day.everyone&#34;,&#34;day.i&#34;,&#34;daysdidn&#39;t&#34;,&#34;dead&#34;,&#34;decarb&#34;,&#34;decarbed&#34;,&#34;deceiving&#34;,&#34;decide&#34;,&#34;decided&#34;,&#34;decides&#34;,&#34;declining&#34;,&#34;deemed&#34;,&#34;deep&#34;,&#34;deeper&#34;,&#34;def&#34;,&#34;default.here&#34;,&#34;defined&#34;,&#34;defitnely&#34;,&#34;dehydrate&#34;,&#34;delahaze&#34;,&#34;delay&#34;,&#34;delays&#34;,&#34;delivered&#34;,&#34;deliveries&#34;,&#34;delivers&#34;,&#34;density&#34;,&#34;dent&#34;,&#34;deny&#34;,&#34;depending&#34;,&#34;depts&#34;,&#34;describe&#34;,&#34;deserve&#34;,&#34;desired.all&#34;,&#34;desk&#34;,&#34;detailed&#34;,&#34;details&#34;,&#34;determine&#34;,&#34;determined&#34;,&#34;device&#34;,&#34;devonwaldo&#39;s&#34;,&#34;dick&#34;,&#34;didnt&#34;,&#34;difficult&#34;,&#34;dig&#34;,&#34;digest&#34;,&#34;diligent&#34;,&#34;dipping&#34;,&#34;direct&#34;,&#34;directly&#34;,&#34;disappeared&#34;,&#34;disappears&#34;,&#34;disappointed&#34;,&#34;disappointing&#34;,&#34;disapproving&#34;,&#34;discord&#34;,&#34;discount&#34;,&#34;discovered&#34;,&#34;discrete&#34;,&#34;discuss&#34;,&#34;discussion&#34;,&#34;dislike&#34;,&#34;dispensaries&#34;,&#34;dispensary&#34;,&#34;dispensery&#34;,&#34;disproportionately&#34;,&#34;dissapointed&#34;,&#34;distalite.the&#34;,&#34;distillate&#34;,&#34;doable&#34;,&#34;doe&#34;,&#34;dogged&#34;,&#34;dominant&#34;,&#34;doob&#34;,&#34;dosing&#34;,&#34;doug&#34;,&#34;downtown&#34;,&#34;downvote&#34;,&#34;downvoted&#34;,&#34;downvotes&#34;,&#34;downvoting&#34;,&#34;dozen&#34;,&#34;drag&#34;,&#34;draw&#34;,&#34;dreamed&#34;,&#34;dreams&#34;,&#34;dried&#34;,&#34;driven&#34;,&#34;driver&#34;,&#34;drop&#34;,&#34;drops&#34;,&#34;drove&#34;,&#34;drug&#34;,&#34;drugs&#34;,&#34;dum&#34;,&#34;dynavap&#34;,&#34;eastnewfoundland&#34;,&#34;editor&#34;,&#34;edmonton420&#34;,&#34;edmontonalternative&#34;,&#34;edmontoncannabis&#34;,&#34;edmontonfire&#34;,&#34;edmontonnova&#34;,&#34;edmontonwestside&#34;,&#34;educated&#34;,&#34;education&#34;,&#34;elements&#34;,&#34;emails&#34;,&#34;employee&#34;,&#34;energetic&#34;,&#34;enforceable&#34;,&#34;enforcing&#34;,&#34;enjoyable&#34;,&#34;enter&#34;,&#34;enters&#34;,&#34;epic&#34;,&#34;essential&#34;,&#34;est&#34;,&#34;eventually&#34;,&#34;evidence&#34;,&#34;excuse&#34;,&#34;exist&#34;,&#34;expectations&#34;,&#34;expensive&#34;,&#34;expensive.money&#34;,&#34;experience:order&#34;,&#34;experience:packaging&#34;,&#34;experience.website:great&#34;,&#34;experienced&#34;,&#34;experiences&#34;,&#34;experienceunable&#34;,&#34;expert&#34;,&#34;expired&#34;,&#34;explain&#34;,&#34;explained&#34;,&#34;extra&#34;,&#34;extremely&#34;,&#34;facilitating&#34;,&#34;factors&#34;,&#34;fall&#34;,&#34;familiar&#34;,&#34;faq&#34;,&#34;farmers&#34;,&#34;fat&#34;,&#34;favorites&#34;,&#34;favourite&#34;,&#34;favourites&#34;,&#34;federally&#34;,&#34;feds&#34;,&#34;feedback&#34;,&#34;feeling&#34;,&#34;feels&#34;,&#34;fields&#34;,&#34;fight&#34;,&#34;figr&#34;,&#34;fill&#34;,&#34;filling&#34;,&#34;filtered&#34;,&#34;finagling&#34;,&#34;finally&#34;,&#34;financial&#34;,&#34;finished&#34;,&#34;firecrackers&#34;,&#34;flagging&#34;,&#34;flat&#34;,&#34;flawed&#34;,&#34;flex&#34;,&#34;flight&#34;,&#34;floods&#34;,&#34;flowers&#34;,&#34;flu&#34;,&#34;fluffy&#34;,&#34;fly&#34;,&#34;flying&#34;,&#34;follow&#34;,&#34;fool&#34;,&#34;football&#34;,&#34;forced&#34;,&#34;ford&#34;,&#34;fortunate&#34;,&#34;forward&#34;,&#34;fredericton&#34;,&#34;freedom.i&#34;,&#34;fri&#34;,&#34;frostiness&#34;,&#34;fucked.shipping&#34;,&#34;fuckers&#34;,&#34;fucking&#34;,&#34;fuss&#34;,&#34;future&#34;,&#34;gagner&#34;,&#34;games&#34;,&#34;generalize&#34;,&#34;giant&#34;,&#34;girlfriend&#34;,&#34;glad&#34;,&#34;glass&#34;,&#34;glut&#34;,&#34;god’s&#34;,&#34;good.otherwise&#34;,&#34;got.also&#34;,&#34;gov’t&#34;,&#34;government&#39;s&#34;,&#34;government’s&#34;,&#34;grab&#34;,&#34;grade&#34;,&#34;grail&#34;,&#34;gramflat&#34;,&#34;grand&#34;,&#34;grandchildren&#34;,&#34;granted&#34;,&#34;grants&#34;,&#34;grass&#34;,&#34;grateful&#34;,&#34;greenhouses&#34;,&#34;greens&#34;,&#34;grind&#34;,&#34;grocery&#34;,&#34;ground&#34;,&#34;grovenova&#34;,&#34;grower&#34;,&#34;grower.wasn&#39;t&#34;,&#34;guaranteed&#34;,&#34;guide&#34;,&#34;gwill&#34;,&#34;habit&#34;,&#34;hah&#34;,&#34;hairs&#34;,&#34;halifax&#34;,&#34;hammer&#34;,&#34;hand&#34;,&#34;handed&#34;,&#34;handle&#34;,&#34;handling&#34;,&#34;handy&#34;,&#34;happened&#34;,&#34;hardcore&#34;,&#34;harper&#34;,&#34;harriman&#34;,&#34;hassle&#34;,&#34;hassles&#34;,&#34;hatmanitobahttps&#34;,&#34;hatnova&#34;,&#34;hatnumo&#34;,&#34;head&#34;,&#34;headache&#34;,&#34;headband&#34;,&#34;headed&#34;,&#34;hearing&#34;,&#34;hearsay&#34;,&#34;heavier&#34;,&#34;hectic&#34;,&#34;hell&#34;,&#34;helloocs.ca&#34;,&#34;helping&#34;,&#34;here.either&#34;,&#34;here’s&#34;,&#34;heroin&#34;,&#34;history&#34;,&#34;holders&#34;,&#34;holding&#34;,&#34;holes&#34;,&#34;homework&#34;,&#34;honestly&#34;,&#34;honey&#34;,&#34;hops&#34;,&#34;hotels&#34;,&#34;hotter&#34;,&#34;hour&#34;,&#34;hubert&#34;,&#34;hung&#34;,&#34;hybrid&#34;,&#34;hybridliiv&#34;,&#34;hydro&#34;,&#34;hydroponics&#34;,&#34;i’d&#34;,&#34;i’ll&#34;,&#34;id&#39;d&#34;,&#34;idea&#34;,&#34;ideal&#34;,&#34;idiot&#34;,&#34;idiotsweaksauce&#34;,&#34;illegally&#34;,&#34;imgur&#34;,&#34;immediately&#34;,&#34;improve&#34;,&#34;improved&#34;,&#34;inadmissible&#34;,&#34;inadmissible.link&#34;,&#34;incentive&#34;,&#34;include&#34;,&#34;incognito&#34;,&#34;income&#34;,&#34;increase&#34;,&#34;increases&#34;,&#34;indicas&#34;,&#34;info:canada&#34;,&#34;information.edit&#34;,&#34;informationhttps&#34;,&#34;informed&#34;,&#34;ingest&#34;,&#34;ingested&#34;,&#34;insane&#34;,&#34;inside&#34;,&#34;insight&#34;,&#34;inspection&#34;,&#34;inspired&#34;,&#34;intense&#34;,&#34;intention&#34;,&#34;intentions&#34;,&#34;interacting&#34;,&#34;interchangeably&#34;,&#34;interior&#34;,&#34;international&#34;,&#34;interview&#34;,&#34;investigated&#34;,&#34;investigation&#34;,&#34;invisible&#34;,&#34;iron&#34;,&#34;island&#34;,&#34;issue.i&#34;,&#34;issued&#34;,&#34;issuing&#34;,&#34;it.as&#34;,&#34;jack&#34;,&#34;jar&#34;,&#34;jean&#34;,&#34;jesus&#34;,&#34;jet&#34;,&#34;jill&#34;,&#34;jive&#34;,&#34;jnkrbwdk&#34;,&#34;jobs&#34;,&#34;joke&#34;,&#34;joking&#34;,&#34;journey&#34;,&#34;judge&#34;,&#34;jurisdiction&#34;,&#34;justly&#34;,&#34;kamloops&#34;,&#34;kensington&#34;,&#34;kick&#34;,&#34;kidding&#34;,&#34;kiefy&#34;,&#34;kills&#34;,&#34;kinky&#34;,&#34;label.liiv&#34;,&#34;labelling&#34;,&#34;labour&#34;,&#34;labradorhttp&#34;,&#34;lack&#34;,&#34;lady&#34;,&#34;lagged.i&#34;,&#34;land&#34;,&#34;landrace&#34;,&#34;largest&#34;,&#34;late&#34;,&#34;laughable&#34;,&#34;laughter&#34;,&#34;launch&#34;,&#34;laying&#34;,&#34;layout&#34;,&#34;leading&#34;,&#34;leaves&#34;,&#34;leaving&#34;,&#34;led&#34;,&#34;leftthe&#34;,&#34;legalities&#34;,&#34;legalize&#34;,&#34;legit&#34;,&#34;lethbridge&#34;,&#34;licence&#34;,&#34;licensee&#34;,&#34;licensing&#34;,&#34;lifetime&#34;,&#34;lift.co&#34;,&#34;likes&#34;,&#34;liking&#34;,&#34;lil&#34;,&#34;link&#34;,&#34;links&#34;,&#34;lipid&#34;,&#34;liquor&#34;,&#34;lists&#34;,&#34;lmao&#34;,&#34;lmfao&#34;,&#34;loading&#34;,&#34;locally&#34;,&#34;located&#34;,&#34;locking&#34;,&#34;logic&#34;,&#34;login&#34;,&#34;lol.like&#34;,&#34;lol’d&#34;,&#34;lolol&#34;,&#34;lolwtf&#34;,&#34;longtime&#34;,&#34;looked&#34;,&#34;lots&#34;,&#34;lounges&#34;,&#34;lucky&#34;,&#34;lug&#34;,&#34;luggage&#34;,&#34;lumped&#34;,&#34;lying&#34;,&#34;macro&#34;,&#34;magazine&#34;,&#34;magento&#34;,&#34;magneto&#34;,&#34;mail&#34;,&#34;majour&#34;,&#34;mangled&#34;,&#34;mango&#34;,&#34;manitoba&#34;,&#34;manitobans.saskatchewanjimmy&#39;s&#34;,&#34;map&#34;,&#34;mappeel&#34;,&#34;maprosemont&#34;,&#34;mapville&#34;,&#34;marie&#34;,&#34;maritimes&#34;,&#34;marked&#34;,&#34;marketconvenient&#34;,&#34;marketplace&#34;,&#34;master&#34;,&#34;mastercards&#34;,&#34;match&#34;,&#34;match.edit&#34;,&#34;mate&#34;,&#34;material&#34;,&#34;materials&#34;,&#34;math&#34;,&#34;me.all&#34;,&#34;me.oh&#34;,&#34;media&#34;,&#34;medically&#34;,&#34;memory&#34;,&#34;mentioned&#34;,&#34;mentioning&#34;,&#34;meridian&#34;,&#34;mess&#34;,&#34;message&#34;,&#34;messing&#34;,&#34;metal&#34;,&#34;method&#34;,&#34;milled&#34;,&#34;million&#34;,&#34;min&#34;,&#34;mines&#34;,&#34;minimal&#34;,&#34;minimum&#34;,&#34;mins&#34;,&#34;minute&#34;,&#34;mirabel&#34;,&#34;miss&#34;,&#34;missing&#34;,&#34;misunderstood&#34;,&#34;mixed&#34;,&#34;mmpr&#34;,&#34;moc&#34;,&#34;mode&#34;,&#34;modest&#34;,&#34;moist&#34;,&#34;mom&#39;s.they&#34;,&#34;moment.throw&#34;,&#34;moms&#34;,&#34;monopoly&#34;,&#34;months&#34;,&#34;months.thanks&#34;,&#34;moochers&#34;,&#34;more.edit&#34;,&#34;mornings&#34;,&#34;moron&#34;,&#34;morons&#34;,&#34;move&#34;,&#34;movies&#34;,&#34;multiple&#34;,&#34;munchies&#34;,&#34;municipal&#34;,&#34;muted&#34;,&#34;namao&#34;,&#34;nationwide&#34;,&#34;naturals&#34;,&#34;nb&#39;s&#34;,&#34;neat&#34;,&#34;needed.edit&#34;,&#34;negative&#34;,&#34;neighbourhood&#34;,&#34;nelson&#34;,&#34;net&#34;,&#34;network&#34;,&#34;newfoundland&#34;,&#34;newsroom&#34;,&#34;nexus&#34;,&#34;niagara&#34;,&#34;nicer&#34;,&#34;noobs&#34;,&#34;noon&#34;,&#34;nose&#34;,&#34;note&#34;,&#34;noticing&#34;,&#34;novelty&#34;,&#34;now.canada&#34;,&#34;nudes&#34;,&#34;nug&#34;,&#34;numo&#34;,&#34;nunavut&#34;,&#34;nurse&#34;,&#34;nutella&#34;,&#34;nuts&#34;,&#34;obtain&#34;,&#34;of.i&#39;ll&#34;,&#34;off.edit&#34;,&#34;office.money&#34;,&#34;oil‘s&#34;,&#34;omg&#34;,&#34;ontarios&#34;,&#34;oof&#34;,&#34;optimistic&#34;,&#34;option.some&#34;,&#34;original&#34;,&#34;orwch9u&#34;,&#34;ouest&#34;,&#34;out.edit&#34;,&#34;outcome&#34;,&#34;outsell&#34;,&#34;outstanding&#34;,&#34;outta&#34;,&#34;owners&#34;,&#34;packages&#34;,&#34;paids&#34;,&#34;pain&#34;,&#34;paranoid&#34;,&#34;parent&#34;,&#34;part.all&#34;,&#34;partake&#34;,&#34;partakeoh&#34;,&#34;parties&#34;,&#34;pass&#34;,&#34;pass.going&#34;,&#34;passed&#34;,&#34;passenger&#34;,&#34;passes&#34;,&#34;passwordbrick&#34;,&#34;path&#34;,&#34;patience&#34;,&#34;patient&#34;,&#34;paused&#34;,&#34;peace&#34;,&#34;pei&#34;,&#34;pennies&#34;,&#34;penny&#34;,&#34;percentage&#34;,&#34;perfect&#34;,&#34;permission&#34;,&#34;permits&#34;,&#34;permitting&#34;,&#34;pervasive&#34;,&#34;phemonia&#34;,&#34;photos&#34;,&#34;pic&#34;,&#34;pieces&#34;,&#34;pill&#34;,&#34;pills&#34;,&#34;pinene&#34;,&#34;pizza&#34;,&#34;placing&#34;,&#34;plaingreen&#34;,&#34;plan&#34;,&#34;planes&#34;,&#34;plant.guess&#34;,&#34;platform&#34;,&#34;playing&#34;,&#34;plenty&#34;,&#34;plz&#34;,&#34;pm.no&#34;,&#34;political&#34;,&#34;pooping&#34;,&#34;pop&#34;,&#34;portalocs&#34;,&#34;posed&#34;,&#34;possibility&#34;,&#34;posted&#34;,&#34;posting&#34;,&#34;pot.students&#34;,&#34;pot&#39;s&#34;,&#34;potbyprovince.ca&#34;,&#34;potent&#34;,&#34;potential&#34;,&#34;potentially&#34;,&#34;poti&#34;,&#34;potpriced&#34;,&#34;pounds&#34;,&#34;preclearance&#34;,&#34;premier&#34;,&#34;premium&#34;,&#34;prepaids&#34;,&#34;prepared&#34;,&#34;prerolls&#34;,&#34;president&#39;s&#34;,&#34;pressing&#34;,&#34;price.the&#34;,&#34;pricing:i&#34;,&#34;pricing.https&#34;,&#34;primo&#34;,&#34;prize&#34;,&#34;process&#34;,&#34;processed&#34;,&#34;producers&#34;,&#34;prohibition&#34;,&#34;proliferation&#34;,&#34;promise&#34;,&#34;promised&#34;,&#34;prongs&#34;,&#34;proof&#34;,&#34;properly&#34;,&#34;prov&#34;,&#34;provide&#34;,&#34;provided&#34;,&#34;provider&#34;,&#34;providing&#34;,&#34;provincefeel&#34;,&#34;provincially&#34;,&#34;pumped&#34;,&#34;punch&#34;,&#34;purely&#34;,&#34;purolator&#34;,&#34;quality.people&#34;,&#34;quantity&#34;,&#34;quebecers&#34;,&#34;queues&#34;,&#34;quickly&#34;,&#34;rambling&#34;,&#34;random&#34;,&#34;rate&#34;,&#34;rates&#34;,&#34;ratio&#34;,&#34;reasoned&#34;,&#34;reasoning&#34;,&#34;rebuy&#34;,&#34;rec&#34;,&#34;receipt&#34;,&#34;reckon&#34;,&#34;recommending&#34;,&#34;redditors&#34;,&#34;reduce&#34;,&#34;referring&#34;,&#34;refrain&#34;,&#34;register&#34;,&#34;regular&#34;,&#34;regulation&#34;,&#34;relating&#34;,&#34;release&#34;,&#34;released&#34;,&#34;relevant&#34;,&#34;remain&#34;,&#34;remember&#34;,&#34;remembered&#34;,&#34;remove&#34;,&#34;replies&#34;,&#34;reply&#34;,&#34;reporter&#34;,&#34;represented&#34;,&#34;represents&#34;,&#34;requests&#34;,&#34;residences&#34;,&#34;resistance.obviously&#34;,&#34;resource&#34;,&#34;restaurant&#34;,&#34;restock&#34;,&#34;restocked&#34;,&#34;restocking&#34;,&#34;restocks&#34;,&#34;restricting&#34;,&#34;retail.battleford&#34;,&#34;ridiculous&#34;,&#34;rioted&#34;,&#34;rips&#34;,&#34;risk&#34;,&#34;road.i&#34;,&#34;robbed&#34;,&#34;rough&#34;,&#34;route&#34;,&#34;rqjjbcr&#34;,&#34;rugles&#34;,&#34;rules&#34;,&#34;rules.the&#34;,&#34;running&#34;,&#34;runs&#34;,&#34;s.provide&#34;,&#34;s0g&#34;,&#34;s0k&#34;,&#34;s0m&#34;,&#34;s4a&#34;,&#34;sad&#34;,&#34;saint&#34;,&#34;sainte&#34;,&#34;sake&#34;,&#34;same.maybe&#34;,&#34;sample&#34;,&#34;sampling&#34;,&#34;saskatchewan&#34;,&#34;saskatchewanfire&#34;,&#34;saskatchewannova&#34;,&#34;sat&#34;,&#34;sativas&#34;,&#34;save&#34;,&#34;savings&#34;,&#34;scan&#34;,&#34;scandal&#34;,&#34;scanning&#34;,&#34;scarab138&#34;,&#34;scare&#34;,&#34;schedule&#34;,&#34;scheer&#34;,&#34;scotia&#34;,&#34;scotiahttps&#34;,&#34;screen&#34;,&#34;screw&#34;,&#34;screwing&#34;,&#34;seal&#34;,&#34;sealed&#34;,&#34;searchable&#34;,&#34;searches&#34;,&#34;season&#34;,&#34;security.product&#34;,&#34;sedative&#34;,&#34;sellare&#34;,&#34;sensitive&#34;,&#34;seriously.shipping&#34;,&#34;servers.it&#34;,&#34;session&#34;,&#34;sessions.all&#34;,&#34;set&#34;,&#34;setting&#34;,&#34;shame&#34;,&#34;share&#34;,&#34;shared&#34;,&#34;shipping.catching&#34;,&#34;shippingpre&#34;,&#34;shishkaberry&#34;,&#34;shocked&#34;,&#34;shop.if&#34;,&#34;shopcannabisnl.com&#34;,&#34;shopify&#34;,&#34;short&#34;,&#34;shorted&#34;,&#34;shots&#34;,&#34;should&#39;ve&#34;,&#34;shown&#34;,&#34;significant&#34;,&#34;silly&#34;,&#34;situations&#34;,&#34;size&#34;,&#34;sizes&#34;,&#34;slowly&#34;,&#34;slows&#34;,&#34;smallish&#34;,&#34;smart&#34;,&#34;smashing&#34;,&#34;smelled&#34;,&#34;smith&#34;,&#34;smokable&#34;,&#34;smoke.the&#34;,&#34;smoked.good&#34;,&#34;smooth&#34;,&#34;snapped&#34;,&#34;soften&#34;,&#34;solar&#34;,&#34;solei&#39;s&#34;,&#34;solid&#34;,&#34;solve&#34;,&#34;solventless&#34;,&#34;someday&#34;,&#34;sooooooooooooooo&#34;,&#34;sound&#34;,&#34;southfort&#34;,&#34;southpointe&#34;,&#34;space&#34;,&#34;speeches&#34;,&#34;spending&#34;,&#34;spoiled&#34;,&#34;spoke&#34;,&#34;spoken&#34;,&#34;spoonful&#34;,&#34;spouse&#34;,&#34;stability&#34;,&#34;staff&#34;,&#34;stag&#34;,&#34;standards&#34;,&#34;starting&#34;,&#34;starts&#34;,&#34;stash&#34;,&#34;stated&#34;,&#34;states.i&#34;,&#34;stay&#34;,&#34;step&#34;,&#34;stepping&#34;,&#34;stern&#34;,&#34;stigma&#34;,&#34;stinky&#34;,&#34;stocked&#34;,&#34;stoked&#34;,&#34;stone&#34;,&#34;stoners&#34;,&#34;stony&#34;,&#34;storefront&#34;,&#34;stories&#34;,&#34;storz&#34;,&#34;straightened&#34;,&#34;streams&#34;,&#34;streets&#34;,&#34;strict&#34;,&#34;strikes&#34;,&#34;struggling&#34;,&#34;stuck&#34;,&#34;stuffing&#34;,&#34;subject&#34;,&#34;subreddit&#34;,&#34;substantially&#34;,&#34;success.was&#34;,&#34;successful&#34;,&#34;sucks&#34;,&#34;suggest&#34;,&#34;suggesting.in&#34;,&#34;suggestion&#34;,&#34;suggestions&#34;,&#34;suh&#34;,&#34;sunday&#34;,&#34;sunflower&#34;,&#34;sunk&#34;,&#34;suppliers&#34;,&#34;support&#34;,&#34;support_the_breeders_lp_strain_overview&#34;,&#34;supporting&#34;,&#34;surely&#34;,&#34;surreal&#34;,&#34;suspicion&#34;,&#34;swallow&#34;,&#34;swap&#34;,&#34;sweat&#34;,&#34;sweedy&#34;,&#34;sweeps&#34;,&#34;swim&#34;,&#34;swimming&#34;,&#34;system.i&#34;,&#34;tactic&#34;,&#34;tag&#34;,&#34;talked&#34;,&#34;talks&#34;,&#34;taste&#34;,&#34;tasty&#34;,&#34;taxi&#34;,&#34;tea&#34;,&#34;tempered&#34;,&#34;ten&#34;,&#34;terpenes&#34;,&#34;terpinolene&#34;,&#34;terrorism&#34;,&#34;that.no&#34;,&#34;that.the&#34;,&#34;thc:cbd&#34;,&#34;them.police&#34;,&#34;themed&#34;,&#34;there.the&#34;,&#34;thermometer&#34;,&#34;they’ll&#34;,&#34;they’ve&#34;,&#34;thier&#34;,&#34;this.make&#34;,&#34;thread.british&#34;,&#34;thrive&#34;,&#34;thriving&#34;,&#34;thrown&#34;,&#34;thurs&#34;,&#34;thursday&#34;,&#34;thx&#34;,&#34;ticket&#34;,&#34;tight&#34;,&#34;tilray&#39;s&#34;,&#34;timer&#34;,&#34;timezones&#34;,&#34;tinctures&#34;,&#34;tire&#34;,&#34;today.fyi&#34;,&#34;today&#39;s&#34;,&#34;tolerance&#34;,&#34;tonight&#34;,&#34;tool&#34;,&#34;toss&#34;,&#34;total&#34;,&#34;touch&#34;,&#34;tough&#34;,&#34;tourism&#34;,&#34;tourists&#34;,&#34;traceable&#34;,&#34;tradeitforweed.club&#34;,&#34;traveler&#34;,&#34;tray&#34;,&#34;trend&#34;,&#34;trichomes&#34;,&#34;tried.i&#34;,&#34;tub&#34;,&#34;turnaround&#34;,&#34;tweed&#34;,&#34;tweeds&#34;,&#34;types&#34;,&#34;uhm&#34;,&#34;unable&#34;,&#34;unavailable&#34;,&#34;unbelievable&#34;,&#34;uncertainty&#34;,&#34;understand&#34;,&#34;understanding&#34;,&#34;understood&#34;,&#34;unexpected.i&#34;,&#34;unfold.can&#39;t&#34;,&#34;unfortunately.edit&#34;,&#34;unique&#34;,&#34;unlicensed&#34;,&#34;unrelated&#34;,&#34;up.i&#39;m&#34;,&#34;updating&#34;,&#34;uplifting&#34;,&#34;upload&#34;,&#34;ups.personally&#34;,&#34;upset&#34;,&#34;us.they&#34;,&#34;usa&#34;,&#34;usabilitypoor&#34;,&#34;uscbp&#34;,&#34;vancouver&#34;,&#34;vancouver.holding&#34;,&#34;vaporized&#34;,&#34;vegging&#34;,&#34;vehicle&#34;,&#34;vehicle.out&#34;,&#34;viable&#34;,&#34;vicinity&#34;,&#34;video&#34;,&#34;view&#34;,&#34;violate&#34;,&#34;violation&#34;,&#34;viscous&#34;,&#34;visit&#34;,&#34;voila&#34;,&#34;volume&#34;,&#34;volumes&#34;,&#34;wack&#34;,&#34;waited&#34;,&#34;waking&#34;,&#34;walking&#34;,&#34;walmart&#34;,&#34;war&#34;,&#34;waste&#34;,&#34;wax&#34;,&#34;we’ll&#34;,&#34;weaknesses&#34;,&#34;weary&#34;,&#34;web&#34;,&#34;website:a&#34;,&#34;weds&#34;,&#34;weed.police&#34;,&#34;weed.then&#34;,&#34;weed.you&#34;,&#34;weedless&#34;,&#34;weedmd&#39;s&#34;,&#34;weekly&#34;,&#34;weigh&#34;,&#34;weighed&#34;,&#34;whats&#34;,&#34;whelp&#34;,&#34;whiff&#34;,&#34;whim&#34;,&#34;white.the&#34;,&#34;widow&#34;,&#34;wife&#34;,&#34;willow&#34;,&#34;winnipeg&#34;,&#34;witb&#34;,&#34;wits&#34;,&#34;woman&#34;,&#34;wont&#34;,&#34;worker&#34;,&#34;worried&#34;,&#34;worse&#34;,&#34;would&#39;ve&#34;,&#34;wow&#34;,&#34;wrong&#34;,&#34;wrote&#34;,&#34;wth&#34;,&#34;wut&#34;,&#34;www.albertacannabis.org&#34;,&#34;www.bccannabisstores.com&#34;,&#34;www.cbc.ca&#34;,&#34;www.cbp.gov&#34;,&#34;www.cp24&#34;,&#34;www.delta9&#34;,&#34;www.reddit.com&#34;,&#34;www.shopcannabisnl.comnew&#34;,&#34;www.sqdc.ca&#34;,&#34;yah&#34;,&#34;yea&#34;,&#34;yesterday&#39;s&#34;,&#34;yet.edit&#34;,&#34;yetquebecmontreal&#34;,&#34;yikes&#34;,&#34;you.i&#34;,&#34;you.take&#34;,&#34;you’ll&#34;,&#34;yup&#34;,&#34;zip&#34;,&#34;zombiejesus1987no&#34;,&#34;zone&#34;,&#34;zoom&#34;,&#34;zzz&#34;,&#34;ಠ_ಠ&#34;],&#34;freq&#34;:[52,42,40,38,35,30,29,26,25,24,24,22,20,20,20,20,19,19,18,17,17,17,17,17,16,16,16,16,15,15,15,14,14,14,14,13,13,13,13,13,12,12,12,12,12,12,12,12,11,11,11,11,11,11,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],&#34;fontFamily&#34;:&#34;Segoe UI&#34;,&#34;fontWeight&#34;:&#34;bold&#34;,&#34;color&#34;:&#34;green&#34;,&#34;minSize&#34;:0,&#34;weightFactor&#34;:3.46153846153846,&#34;backgroundColor&#34;:&#34;black&#34;,&#34;gridSize&#34;:0,&#34;minRotation&#34;:-0.785398163397448,&#34;maxRotation&#34;:0.785398163397448,&#34;shuffle&#34;:true,&#34;rotateRatio&#34;:0.4,&#34;shape&#34;:&#34;circle&#34;,&#34;ellipticity&#34;:0.65,&#34;figBase64&#34;:null,&#34;hover&#34;:null},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;While words like cannabis, legal, weed, etc. would be expected to appear in the discussion, there are other frequent words which may arouse your interest. For example, there are a lot of words associated with online shopping. Among them, ocs (Ontario Cannabis Store), shipping, buy, visa, credit, debit, and card appear most frequently. Such observations suggest that a substantial number of comments in the discussion is about the cannabis business.&lt;/p&gt;
&lt;p&gt;Let’s examine some of the comments with those - cannabis business related - words!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_data %&amp;gt;%
  filter(!is.na(likes),word %in% c(&amp;quot;ocs&amp;quot;, &amp;quot;shipping&amp;quot;, &amp;quot;credit&amp;quot;, &amp;quot;store&amp;quot;, &amp;quot;visa&amp;quot;, &amp;quot;debit&amp;quot;,&amp;quot;buy&amp;quot;))%&amp;gt;%
  group_by(id)%&amp;gt;%
  summarize(n=n())%&amp;gt;%
  arrange(desc(n))%&amp;gt;%
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##      id     n
##   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
## 1   457     4
## 2   147     3
## 3   163     3
## 4   211     3
## 5   450     3&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/figure/458_iLoveyoumissmary.png&#34; /&gt;

&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/figure/147_EatPastaSkateFasta.png&#34; /&gt;

&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/figure/163_thedommer.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;These comments confirm my hypothesis that those words are closely related to the cannabis business. In addition, the author of these comments are concerned about their privacy while making a purchase. It seems like, they are not certain about the privacy involved in buying cannabis with their credit cards.&lt;/p&gt;
&lt;p&gt;Who were the most frequent contributors to the discussion? In addition to a “tibble”, as the output of &lt;code&gt;dplyr&lt;/code&gt; functions, &lt;code&gt;ggplot2&lt;/code&gt; allows another visual inspection. The following bar plot exhibits the count of comments that the five most frequent contributors to this discussion have posted. However, &lt;code&gt;[deleted&lt;/code&gt;] is rather for every unknown author than a specific one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset %&amp;gt;%
  group_by(author) %&amp;gt;%
  summarise(Comments=n())%&amp;gt;%
  arrange(desc(Comments))%&amp;gt;%
  mutate(author = reorder(author, Comments)) %&amp;gt;%
  head(5) %&amp;gt;%
  
  ggplot(aes(author,Comments))+
  geom_col(show.legend = F)+
  coord_flip()+
  geom_text(aes(label = Comments))+
  ggtitle(&amp;quot;The Most Frequently Contributing Authors&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Most%20Frequent%20contributors%20PLOT-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because I was assuming these contributors have a substantial impact on positive and negative sentiments, I have decided to focus on the words they posted.&lt;/p&gt;
&lt;p&gt;The plot below outlines the specific and most frequently used words by their authors. Bear in mind these words come from &lt;code&gt;nrc&lt;/code&gt; lexicon. Therefore, the words that were not contained in the lexicon were filtered out. However, the remaining words create the sentiments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_sentiment %&amp;gt;%
    # Count by word and author
    count(word,author) %&amp;gt;%
    # Group by author
    group_by(author) %&amp;gt;%
    # Take the top 10 words for each author
    top_n(10) %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(word = reorder(paste(word, author, sep = &amp;quot;__&amp;quot;), n)) %&amp;gt;%
    filter(author %in% c(&amp;quot;BioSector&amp;quot;, &amp;quot;ruglescdn&amp;quot;,  &amp;quot;terrencemckenna&amp;quot;, &amp;quot;frowawe&amp;quot; )) %&amp;gt;%
    # Set up the plot with aes()
    ggplot(aes(word,n)) +
    geom_col(show.legend = FALSE) +
    scale_x_discrete(labels = function(x) gsub(&amp;quot;__.+$&amp;quot;, &amp;quot;&amp;quot;, x)) +
    facet_wrap(~ author, nrow = 2, scales = &amp;quot;free&amp;quot;) +
    coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Word_Choice-1.png&#34; width=&#34;672&#34; /&gt; The output of the most frequently used words posted by individuals may be interesting. But to what extent is it insightful? One should know the specific sentiment these words annotate. The following chart displays counts of all the words from the previous scheme. Most importantly, these words are categorized within the sentiments from &lt;code&gt;nrc&lt;/code&gt; lexicon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_sentiment %&amp;gt;%
    # Count by word and author
    group_by(word,author)%&amp;gt;%
    mutate(n=n())%&amp;gt;%
    # Group by author
    filter(author %in% c(&amp;quot;BioSector&amp;quot;, &amp;quot;ruglescdn&amp;quot;,  &amp;quot;terrencemckenna&amp;quot;, &amp;quot;frowawe&amp;quot; ))%&amp;gt;%
    group_by(author) %&amp;gt;%
    top_n(30) %&amp;gt;%
    ungroup() %&amp;gt;% 

    # Set up the plot with aes()
    ggplot(aes(word,n)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ sentiment, scales = &amp;quot;free&amp;quot;) +
    coord_flip()+
    ylab(&amp;quot;Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Which_words_contribute_to_scores-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that BioSector, ruglescdn, terrencemckenna, and frowawe express in a number of ways. Even though they do not express disgust or surprise too often, I could not determine what is their most common sentiment. Since a table is not as space demanding as any rigorous plot, the one mentioned below summarizes and counts sentiment related words by their authors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_sentiment %&amp;gt;%
    filter(author %in% c(&amp;quot;BioSector&amp;quot;, &amp;quot;ruglescdn&amp;quot;,  &amp;quot;terrencemckenna&amp;quot;, &amp;quot;frowawe&amp;quot; ))%&amp;gt;%
    group_by(sentiment) %&amp;gt;%
    summarise(count=n())%&amp;gt;%
    arrange(desc(count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    sentiment    count
##    &amp;lt;chr&amp;gt;        &amp;lt;int&amp;gt;
##  1 positive       122
##  2 trust           83
##  3 negative        77
##  4 anticipation    70
##  5 fear            45
##  6 sadness         35
##  7 anger           32
##  8 joy             30
##  9 surprise        26
## 10 disgust         20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Amazing! BioSector, ruglescdn, terrencemckenna, and frowawe create mostly positive sentiment!&lt;/p&gt;
&lt;p&gt;My next goal was to allocate the most positive and negative authors. For this purpose, I created a new variable &lt;code&gt;percent&lt;/code&gt;. &lt;code&gt;count()&lt;/code&gt; groups author, sentiment, and author_total . Then, it creates a new variable &lt;code&gt;n&lt;/code&gt;, and calls &lt;code&gt;ungroup()&lt;/code&gt; afterwards. Consequently, &lt;code&gt;n&lt;/code&gt; is nothing but the number of words by author within a particular sentiment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Which authors use the most negative words?
data_sentiment %&amp;gt;% 
    count(author, sentiment, author_total) %&amp;gt;%
    # Define a new column percent
    mutate(percent=n/author_total) %&amp;gt;%
    # Filter only for negative words
    filter(sentiment %in% c(&amp;quot;negative&amp;quot;,&amp;quot;positive&amp;quot;)) %&amp;gt;%
    # Arrange by percent
    arrange(desc(percent))%&amp;gt;%
    group_by(sentiment)%&amp;gt;%
    top_n(5) %&amp;gt;%
    ungroup()%&amp;gt;%
  
  ggplot(aes(author,percent)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ sentiment, nrow = 2, scales = &amp;quot;free&amp;quot;) +
    coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above discloses the information of authors, responsible for the greater portion of positive and negative comments. There are two things worthy of mention: i.) the two sets do not intersect so these authors are not ambiguous. ii.) there appears to be “no author” from the group of most frequent contributors.&lt;/p&gt;
&lt;p&gt;I also assumed that BioSector, ruglescdn, terrencemckenna, and frowawe are the most popular ones or at least have posted the most popular comments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#the Most Popular Comments
dataset %&amp;gt;%
  select(id,author,likes)%&amp;gt;%
  arrange(desc(likes))%&amp;gt;%
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id           author likes
## 1 436        [deleted]    20
## 2 444 captain_deadfoot    19
## 3 265   AgentChimendez    18
## 4 172        [deleted]    16
## 5 184          jamagut    15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To my surprise, no one appears in these two groups simultaneously.&lt;/p&gt;
&lt;p&gt;To have an idea about what drives the participants to like someone’s comment, one should read them! Below are a few screenshots of the top 3 ranked comments: &lt;img src=&#34;/post/figure/437_SirWinnipegger.png&#34; alt=&#34;center&#34; /&gt; SirWinnipegge complains about new prices in the local dispensary. Has he been buying weed on the black market for lower prices? If so, are the other cannabis users having the same experience? Then is legalizing marihuana an effective strategy to uproot black market cannabis trade? &lt;img src=&#34;/post/figure/265_AgentChimendez.png&#34; alt=&#34;center&#34; /&gt; On the other hand, AgentChimendez celebrates the date: 10/17/2018 as the new milestone in Canadian history. He points out to the 20th of April, the World Cannabis Day, mostly referred to as 420. &lt;img src=&#34;/post/figure/445_captain_deadfoot.png&#34; alt=&#34;center&#34; /&gt; Lastly, captain_deadfoot’s comment was liked because he reacted to someone else outside of Canada. He emphasizes how friendly Canadian black market prices are in contrast to other countries. Again, it seems like the black market has already been supplying cannabis for a friendly price. Should the government introduce price ceiling to effectively compete with the black market?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Least Popular Comments
dataset %&amp;gt;%
  select(id, author,likes) %&amp;gt;%
  arrange(likes) %&amp;gt;%
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id         author likes
## 1 297       dubs2112   -10
## 2 351         Justos    -5
## 3  25     Hendrix194    -2
## 4  60      trekrlife    -2
## 5 348 Happyradish532    -2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/figure/297_dubs2112.png&#34; alt=&#34;center&#34; /&gt; I particularly enjoyed dubs2112’s comments! He/she was constantly bullied (disliked) by others in the discussion because he/she basically calls people stupid for buying legal cannabis products.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/figure/352_Justos.png&#34; alt=&#34;center&#34; /&gt; The other posts with dislikes appear in the same thread. Happyradish532 and Justos celebrate the shock that the black market might incur. They seem comfortable with paying more for legal cannabis since it is supposed to weaken the black market.&lt;/p&gt;
&lt;p&gt;Finally, let’s see how the sentiments - positive and negative - were developing during the first few weeks. This step was the most difficult one. First of all, I created a new data set &lt;code&gt;sentiment_by_time&lt;/code&gt; from &lt;code&gt;tidy_data&lt;/code&gt;. Since the time span for which the discussion was truly alive is really short, I rounded &lt;code&gt;date&lt;/code&gt; to the unit of days and called the new variable &lt;code&gt;date_floor&lt;/code&gt;. After grouping by &lt;code&gt;date_floor&lt;/code&gt;, I created another new variable &lt;code&gt;total_words&lt;/code&gt;, i.e. the number of total words per day. Again, I merged the data with &lt;code&gt;nrc&lt;/code&gt; lexicon.&lt;/p&gt;
&lt;p&gt;In the next step, I filtered the positive and negative sentiments in &lt;code&gt;sentiment_by_time&lt;/code&gt;. Most importantly, I counted entries of the group: &lt;code&gt;date_floor&lt;/code&gt;, &lt;code&gt;sentiment&lt;/code&gt;, and &lt;code&gt;total_words&lt;/code&gt;, and created new variable &lt;code&gt;percent&lt;/code&gt; which is the ratio of counts per group and total words submitted in a day.&lt;/p&gt;
&lt;p&gt;The purpose of this plot is to demonstrate the long-term development of positive vs. negative sentiment. Additionally, &lt;code&gt;method = &amp;quot;lm&amp;quot;&lt;/code&gt; regresses percent on time and plots slopes of the development curves. Unfortunately, the time span in my model is very short and I do not yet mak any conlusions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentiment_by_time &amp;lt;- tidy_data %&amp;gt;%
    # Define a new column using floor_date()
    mutate(date_floor = floor_date(date, unit = &amp;quot;1 day&amp;quot;)) %&amp;gt;%
    # Group by date_floor
    group_by(date_floor) %&amp;gt;%
    mutate(total_words = n()) %&amp;gt;%
    ungroup() %&amp;gt;%
    # Implement sentiment analysis using the NRC lexicon
    inner_join(get_sentiments(&amp;quot;nrc&amp;quot;), by=&amp;quot;word&amp;quot;)

sentiment_by_time %&amp;gt;%
    # Filter for positive and negative words
    filter(sentiment %in% c(&amp;quot;positive&amp;quot;,&amp;quot;negative&amp;quot;)) %&amp;gt;%
    filter(date_floor &amp;lt; ymd(&amp;quot;2018-11-10&amp;quot;)) %&amp;gt;%
    # Count by date, sentiment, and total_words
    count(date_floor, sentiment, total_words) %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(percent = n / total_words) %&amp;gt;%
    
    # Set up the plot with aes()
    ggplot(aes(date_floor,percent,col=sentiment)) +
    geom_line(size = 1.5) +
    geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE, lty = 2) +
    expand_limits(y = 0)+
    ggtitle(&amp;quot;Sentiment Over Time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/web-scraping/2019-03-27-web-scraping-reddit-text-sentiment-analysis_files/figure-html/Sentiment%20over%20time%20Visual-1.png&#34; width=&#34;672&#34; /&gt; Nevertheless, it seems like positive and negative sentiments exhibit the same mild but positive slopes. Secondly, positive sentiment has much greater variation. Finally, both sentiments achieve their peaks shortly after October 22nd and decline later.&lt;/p&gt;
&lt;p&gt;To see what stands behind the steep rise of positive sentiment, I found a few explanatory comments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentiment_by_time %&amp;gt;%
  filter(sentiment %in% c(&amp;quot;positive&amp;quot;,&amp;quot;negative&amp;quot;))%&amp;gt;%
  filter( ymd(&amp;quot;2018-10-22&amp;quot;) &amp;lt; date_floor &amp;amp; date_floor &amp;lt; ymd(&amp;quot;2018-10-29&amp;quot;)) %&amp;gt;%
  group_by(date_floor, sentiment, total_words) %&amp;gt;%
  mutate(n=n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(percent = n / total_words) %&amp;gt;%
  group_by(id)%&amp;gt;%
  mutate(max=max(percent)) %&amp;gt;%
  arrange(desc(max))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 11
## # Groups:   id [5]
##   author likes date                   id word  date_floor         
##   &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;dttm&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dttm&amp;gt;             
## 1 Inter…     0 2018-10-25 04:40:29    13 ques… 2018-10-25 00:00:00
## 2 Inter…     0 2018-10-25 04:40:29    13 word  2018-10-25 00:00:00
## 3 Inter…     0 2018-10-25 04:40:29    13 legal 2018-10-25 00:00:00
## 4 Inter…     0 2018-10-25 04:40:29    13 ille… 2018-10-25 00:00:00
## 5 Diabl…     1 2018-10-27 23:24:02     6 child 2018-10-27 00:00:00
## 6 h3xad…     2 2018-10-23 04:59:38     7 legal 2018-10-23 00:00:00
## 7 h3xad…     2 2018-10-23 04:59:38     7 money 2018-10-23 00:00:00
## 8 Alypi…     0 2018-10-23 12:32:16    14 legal 2018-10-23 00:00:00
## 9 Hendr…     0 2018-10-23 00:01:04    20 eat   2018-10-23 00:00:00
## # … with 5 more variables: total_words &amp;lt;int&amp;gt;, sentiment &amp;lt;chr&amp;gt;, n &amp;lt;int&amp;gt;,
## #   percent &amp;lt;dbl&amp;gt;, max &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First of all, we can see that the positive sentiment achieved its peak on October 25th and was caused by a post written by IntermolecularButter (comment id 13). Another important observation is the fact that even though the time span was 1 week, there were only 7 new comments during that time! Less comments stand for less number words, therefore &lt;code&gt;mutate(percent = n / total_words)&lt;/code&gt; may fluctuate a lot. Here, the peak is caused by the fact that the denominator was higher before October 22nd, resulting in a lower value of &lt;code&gt;percent&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Nonetheless, let’s see what IntermolecularButter posted. &lt;img src=&#34;/post/figure/13_IntermolecularButter.png&#34; alt=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is not very insightful, is it? The main reason why this post creates a positive sentiment, in addition to the explanation of high variation, is because the twelve words from the comment are associated with positive sentiment! Only one word - illegal - is considered negative. On top of that, &lt;code&gt;sentiment&lt;/code&gt; was joined to the data set by &lt;code&gt;inner_join()&lt;/code&gt;. By default, the result is that the data frame filtered words that were not present in the lexicon. For example, I noticed the word “thanks”&amp;quot; is not in &lt;code&gt;nrc&lt;/code&gt; lexicon even though its connotation is definitely positive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;Conclusion&lt;/h6&gt;
&lt;p&gt;The analysis covered three topics; i.) web scraping using &lt;code&gt;rvest&lt;/code&gt; package, ii.) cannabis legalization in Canada, and iii.) sentiment analysis. First of all, &lt;code&gt;rvest&lt;/code&gt; provides enough flexibility to extract an HTML code from static web page and is therefore ideal for web scraping with R. However, I would suggest using API’s whenever possible to ease your job. Secondly, the discussion on cannabis legalization was not alive as one would expect. It seems that concerns about cannabis users’ privacy were not clearly communicated from the government as is evident from the comments. Additionally, during the time of this discussion posting, the consumers seem to be concerned about the price of legal cannabis being higher than the black market pricing. Lastly, sentiment analysis may not be a reliable approach when it comes to analyzing new policies, especially when something was decriminalized. One can see that more than legalization itself, the cannabis community was rather concerned about prices and privacy.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
