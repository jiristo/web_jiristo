<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>4 on Jiri Stodulka</title>
    <link>/publication_types/4/</link>
    <description>Recent content in 4 on Jiri Stodulka</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Jiri Stodulka</copyright>
    <lastBuildDate>Thu, 01 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/publication_types/4/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>First Hand-on with DL</title>
      <link>/deeplearning/20th_aug2019/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/deeplearning/20th_aug2019/</guid>
      <description>&lt;p&gt;Starting with DL which I want to implement for a recommender system. This time I will need to use external GPU. Will probably go for Azure or AWS. I am exploring codes for embeddings and ReLU, Leaky ReLU, Parametric ReLU, Exponential Linear (ELU, SELU) activation functions. The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive. As always, the challenge is to find the starting point. Figured it out it would be best to start with fast.ai.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
