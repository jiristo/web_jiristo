[{"authors":["admin"],"categories":null,"content":"I am a data scientist and research associate with a master\u0026rsquo;s degree in Applied Economics. Consequently, I am passionate about technology and data that generate economic value. Currently, I work on deep learning based recommender systems with A.I. Socratic Circles (AISC) research team. I solve business problems through the broad-based application of data science and creativity. For example, I co-developed a launch sequence (genetic algorithm) for innovative drugs. The solution helps a European pharmaceutic company to maximize product revenue by 17%.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jiristodulka.com/%60/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/`/authors/admin/","section":"authors","summary":"I am a data scientist and research associate with a master\u0026rsquo;s degree in Applied Economics. Consequently, I am passionate about technology and data that generate economic value. Currently, I work on deep learning based recommender systems with A.I. Socratic Circles (AISC) research team. I solve business problems through the broad-based application of data science and creativity. For example, I co-developed a launch sequence (genetic algorithm) for innovative drugs. The solution helps a European pharmaceutic company to maximize product revenue by 17%.","tags":null,"title":"Jiri Stodulka","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://jiristodulka.com/%60/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/`/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://jiristodulka.com/%60/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/`/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://jiristodulka.com/%60/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/`/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["Python"],"content":"This article targets anyone with previous exposure to machine learning but with little to no knowledge of the recommendation systems. However, it is highly probable that anyone interested in this work interacts with a recommender system regularly. Anyone who listens to Spotify or watches movies on Netflix benefits from the rigorous algorithms (recommendation systems) developed by teams of data scientists and software engineers. The theoretical part of the article explains the fundamentals of various recommendation systems. The practical section emphasizes practical usage of collaborative filtering while utilizing Surprise package and movie-lense data set. Specifically, the author creates two recommender systems utilizing i.) Singular Value Decomposition (SVD), ii.) Non-negative Matrix Factorization (NMF). Both models evaluate/determine users' preferences based on principles of linear algebra.\nTheoretical Part Basics of The Recommender System The reason why consumers need reliable recommendations is straightforward: Given the availability of almost unlimited number of choices (e.g. different movie genres of various quality), and the natural tendency of human is to maximize his/her utility, the user needs guidance to the next best item that accommodates his/her needs or preferences. Specifically, Netflix has become so popular because of its capability to suggest the user a next movie (from thousands of other options) that matches to the best of user's taste.\nTypes of the Systems There are many ways and complex algorithms used to build a recommender system. The following are fundamental approaches. While reading, the reader should think which one may be the most effective method when it comes to a movie recommendation.\n The Most Popular Item: It is the simplest strategy and requires no coding skills. It works based on the assumption that the most popular item attracts most consumers or most users. For example, any consumer shopping on Amazon would see the most frequently bought items. Conversely, Netflix would recommend every user the most popular movie in its list.\n Association \u0026amp; Market Based Model: The system makes recommendations based on the items in the consumer's basket. For instance, if the system detected that the buyer is purchasing ground coffee it would also suggest her to buy filters as well (observed association coffee - filters).\n Content Filtering: Uses metadata to determine the user's taste. For example, the system recommends the user movies based on their preferences of genres, actors, themes, etc. Such a system matches the user and the item based on similarity. For example, if the user watched and liked Terminator and Predator (both action movies with Arnold Schwarzenegger in the main role), it would probably recommend them to watch Commando.\n Collaborative Filtering (CF): It is an algorithmic architecture that recommends consumers items based on their observed behavior. There are two types of Collaborative Filtering frameworks: Model-Based Approach and Memory-Based Approach:\n User-based (UBCF): It is a predecessor of Item-based CF. UBCF makes recommendations based on the user's characteristics that are similar to other users in the system. For example, if the end-user positively rates a movie, the algorithm finds other users who have previously rated the movie too, i.e. these users are similar to one another. In the next step, the system recommends the user an unseen movie but highly rated by other - referenced - users. See Figure 1.  Item-based (IBCF): IBCF was originally developed by Amazon and is currently adopted by most online corporations (e.g. Netflix, YouTube, etc.).\n Hybrid Models: As the name suggests, the Hybrid Models combine two or more recommendation strategies. For instance, a Hybrid Content-Collaborative System can recommend the user a movie based on their gender but still focuses on the movie features the user exhibits to prefer.\n  While Hybrid Models logically appear to be the most effective ones, Netflix's recommendation engine is based on the assumption that similar users like and dislike similar items; i.e. Collaborative Filtering is the key to Netflix's success.\nBoth the SVD and NMF models trained in the article are classified as IBCF. Note that the performance of these models do not meet industry standards. With recent advances in deep learning, online users currently encounter recommendations trained with various types of (hybrid) neural networks (e.g. MLP, CN, RNN, etc.). If the reader is interested in a new perspective and the most advanced models used by the giant online corporations, Deep Learning based Recommender System: A Survey and New Perspectives by Zhang et al. provides a complex overview of up-to-date development in this field.\nUser-based (UBCF) \nFigure 1 demonstrates how the UBCF works when the system identifies similar users (the reference group) to the end-user. Again, to explain exactly how the recommender system evaluates/determines the user's taste, one should recall the assumption that similar users exhibit similar preferences. Precisely, that is how the reference group is determined; i.e. both the user and group share a history of rating similar items. The author of this article describes the similarity as \u0026quot;empirical\u0026quot; because the similarity is observable in data. In the next step, the system allocates the only items the reference group had previously been exposed to and recommends the end-user items with the highest predicted rating.\nIssues with UBCF Even though CF is powerful, there are few challenges. Like other researchers, Sarwar et al. (2001) state that scalability and sparsity are the primary issues.\n Scalability: In a system where there are many new users, i.e. users with low records of ratings, it is computationally expensive to train the model. A typical example would be a web-based application with a recommender system incorporating millions of users and items. Such problems arise with Nearest Neighbour algorithms in UBCF where such algorithms require computations that grow simultaneously with the increasing numbers of users and items.\n Sparsity arises in a system when even the most active users have rated or purchased only a marginal number of available items. For example, these users may have experience with even less than 1% of available items.\n  If a recommender system experiences either one or both of the described issues, the algorithm's performance decreases; i.e. the system does not recommend the user relevant items (Sarwar et al.,2001). Consequently, the user does not trust the recommendations.\nItem-based (IBCF) To tackle the issues with UBCF, item-based collaborative techniques analyze the user-item matrix and identify relationships between different items (Sarwar et al.,2001). The item-based recommendation system then makes recommendations based on the discovered linear relationships (similarities) amongst the items.\nCollaborative Filtering: Model-Based Approach Once again, this article discusses Collaborative Item-based Filtering and focuses on the Model-Based Approach which tackles the two challenges imposed by CF. Unlike Memory-Based Approach, Model-Based procedure facilitates machine learning techniques such as Singular Value Decomposition (SVD) and Matrix Factorization models to predict the end user's rating on unrated items. In the context of a movie-to-movie recommender, a collaborative filter answers the question: â€œWhat movies have a similar user-rating profile?\u0026quot;(Lineberry \u0026amp; Longo, 2018).\nImporting Packages and Data import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from collections import defaultdict #data colector #Surprise: https://surprise.readthedocs.io/en/stable/ import surprise from surprise.reader import Reader from surprise import Dataset from surprise.model_selection import GridSearchCV ##CrossValidation from surprise.model_selection import cross_validate ##Matrix Factorization Algorithms from surprise import SVD from surprise import NMF np.random.seed(42) # replicating results  Importing Online Data MovieLens provides available rating datasets from the MovieLens web site (F. M. Harper and J. A. Konstan, 2015). Any machine learning practitioner may use several different rating files with a number of rated movies and the time of release. For demonstrative purposes and limited computation power, the author worked with 100,836 ratings and 3,683 tag applications across 9,742 movies. The full description of the particular dataset can be found here. According to the documentation, the selected users in data rated at least 20 movies on the scale from 0.5 to 5. The dataset was last updated on 9/2018\nThe work considers only tidy data in ratings.csv and movies.csv. Specifically, ratings_df records userId, movieId, and rating consecutively. On the other hand, movies_df stores values in movieId and genres. movieId is, therefore, the mutual variable.\nNote that Surprise enables one to upload data, e.g. csv files, for predictions through its own methods. On the other hand, as it is discussed below, Surprise also allows the user to use pandas' DataFrames. The author works with pd.DataFrame objects for convenience.\nfrom io import BytesIO from zipfile import ZipFile from urllib.request import urlopen r = urlopen(\u0026quot;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\u0026quot;) zipfile = ZipFile(BytesIO(r.read())) #print the content of zipfile zipfile.namelist()  ['ml-latest-small/', 'ml-latest-small/links.csv', 'ml-latest-small/tags.csv', 'ml-latest-small/ratings.csv', 'ml-latest-small/README.txt', 'ml-latest-small/movies.csv']  # tidy df ratings (movieId,) ratings_df = pd.read_csv(zipfile.open('ml-latest-small/ratings.csv')) print('Columns of ratings_df: {0}'.format(ratings_df.columns)) #movies df (tidy data) movies_df = pd.read_csv(zipfile.open('ml-latest-small/movies.csv')) print('Columns of movies_df: {0}'.format(movies_df.columns))  Columns of ratings_df: Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object') Columns of movies_df: Index(['movieId', 'title', 'genres'], dtype='object')  Inspecting the Data One of the advantages of training on the selected dataset is its purity. Unlike in the real world, one does not need to spend extra time on data cleansing. The following chunk's output demonstrates how the data is stored. The results are in line with the disclosed data description.\n#ratings print(ratings_df.head()) print(ratings_df.info()) print(ratings_df.describe())   userId movieId rating timestamp 0 1 1 4.0 964982703 1 1 3 4.0 964981247 2 1 6 4.0 964982224 3 1 47 5.0 964983815 4 1 50 5.0 964982931 \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 100836 entries, 0 to 100835 Data columns (total 4 columns): userId 100836 non-null int64 movieId 100836 non-null int64 rating 100836 non-null float64 timestamp 100836 non-null int64 dtypes: float64(1), int64(3) memory usage: 3.1 MB None userId movieId rating timestamp count 100836.000000 100836.000000 100836.000000 1.008360e+05 mean 326.127564 19435.295718 3.501557 1.205946e+09 std 182.618491 35530.987199 1.042529 2.162610e+08 min 1.000000 1.000000 0.500000 8.281246e+08 25% 177.000000 1199.000000 3.000000 1.019124e+09 50% 325.000000 2991.000000 3.500000 1.186087e+09 75% 477.000000 8122.000000 4.000000 1.435994e+09 max 610.000000 193609.000000 5.000000 1.537799e+09  #movies print(movies_df.head())   movieId ... genres 0 1 ... Adventure|Animation|Children|Comedy|Fantasy 1 2 ... Adventure|Children|Fantasy 2 3 ... Comedy|Romance 3 4 ... Comedy|Drama|Romance 4 5 ... Comedy [5 rows x 3 columns]  Note that movies_df contains only movieId and genres variables which store even multiple genres separated by the vertical bar in one cell.\nData Pre-Processing Filtering Data Set Firstly, it is essential to filter out movies and users with low exposure to remove some of the noise from outliers. According to the official MovieLens documentation, all selected users have rated at least 20 movies in the data set. However, the following code filters out the movies and users based on an arbitrary threshold and creates a new data frame ratings_flrd_df. Moreover, the chunk also prints the value of deleted movies with new and old dimensions.\nmin_movie_ratings = 2 #a movie has was rated at least min_user_ratings = 5 #a user rated movies at least ratings_flrd_df = ratings_df.groupby(\u0026quot;movieId\u0026quot;).filter(lambda x: x['movieId'].count() \u0026gt;= min_movie_ratings) ratings_flrd_df = ratings_flrd_df.groupby(\u0026quot;userId\u0026quot;).filter(lambda x: x['userId'].count() \u0026gt;= min_user_ratings) \u0026quot;{0} movies deleted; all movies are now rated at least: {1} times. Old dimensions: {2}; New dimensions: {3}\u0026quot;\\ .format(len(ratings_df.movieId.value_counts()) - len(ratings_flrd_df.movieId.value_counts())\\ ,min_movie_ratings,ratings_df.shape, ratings_flrd_df.shape )   '3446 movies deleted; all movies are now rated at least: 2 times. Old dimensions: (100836, 4); New dimensions: (97390, 4)'  Data Loading While using Surprise, one can use a bunch of built-in datasets (e.g. Jeseter or even the movielens) parsed by Dataset module. However, it is usually required to build a customized recommender system. In a case as such, it is necessary to upload your own rating dataset either from a file (e.g. csv) or from a pandas' dataframe. In both cases, you need to define a Reader object to parse the file or the dataframe by Surprise. See the reference here.\nIn the next step, one must load the data set through the call of a particular method of surprise.Dataset. Specifically, load_from_file() loads a csv file. Surprise also allows to upload pandas' DataFrame. This time, it is required to upload the data frame with ratings by user per movie (i.e. in the tidy format) with Dataset.load_from_df and specify reader as the argument.\nLastly, build_full_trainset() method builds the training set from the entire data set. As demonstrated later, training on the whole data while using the best hyper tuning parameters is useful for the prediction of top arbitrary number of movies for each userId.\nreader = Reader(rating_scale=(0.5, 5)) #line_format by default order of the fields data = Dataset.load_from_df(ratings_flrd_df[[\u0026quot;userId\u0026quot;,\t\u0026quot;movieId\u0026quot;,\t\u0026quot;rating\u0026quot;]], reader=reader) trainset = data.build_full_trainset() testset = trainset.build_anti_testset()  The following sections aim to explain particular methods of matrix factorization. Since SVD is the first model to be examined, the scope differs a little. To avoid copy-pasting long chunks of code, the author defines and explains two useful generic functions for a.) performance evaluation and b.) predictions.\nMatrix Factorization Hopcroft and Kannan (2012), explains the whole concept of matrix factorization on customer data where m customers buy n products. The authors explain collaborative filtering in a comprehensive language. For demonstrative purposes, the author of this article demonstrates the concept on a specific case.\nLet matrix \\(R_{m*n}\\) represent the ratings on movies assigned by each user, also called the utility matrix. Specifically, the value $r_{ij}=5$ represents the rating of user i assigned to movie j. However, the individual's preference is determined by k factors. For example, the user's age, sex, income, education, etc. are likely to affect the user's behavior. Accordingly, the individual's rating of a movie ($r_{ij}$) is determined by some weighted combinations of the hidden factors. In practice, customer's behavior can be characterized by a k-dimensional vector with much lower dimensions than the original matrix $R$ with $m * n$ dimensions. The vector's components, also called the latent factors, represent the weight of each factor. For example, given a vector $v_2 = [0.2 , 0.8]$ it can be hypothesized that there are only two (unknown) latent factors with subsequent weights describing the rating (behavior).\nMatrix factorization is an effective CF technique because it benefits from the properties of linear algebra. Specifically, consider matrix $R$ as a record of various elements. As it is possible to decompose any integer into the product of its prime factor, matrix factorization also enables humans to explore information about matrices and their functional properties an array of elements (Goodfellow, Bengio, 2016)\nSingular Value Decomposition (SVD) SVD decomposes any matrix into singular vectors and singular values. If the reader has previous experience with machine learning, particularly with dimensionality reduction, they would find traditional use of SVD in Principal Component Analysis (PCA). Simply put, SVD is equivalent to PCA after mean centering, i.e. shifting all data points so that their mean is on the origin (Gillis, 2014).\nFormally, SVD is decomposition of a matrix R into the product of three matrices: \\(R_{m*n} = U_{m*m} D_{m*n} V_{n*n}^{t}\\).\nWhere \\(R_{m*n}\\) denotes the utility matrix with n equal to the number of e.g. users and m number exposed items (movies). $U_{m*m}$ is a left singular orthogonal matrix, representing the relationship between users and latent factors (Hopcroft \u0026amp; Kannan, 2012). \\(D_{m*n}\\) is a diagonal matrix (with positive real values) describing the strength of each latent factor. \\(V_{n*n}^{t}\\) (transpose) is a right singular orthogonal matrix, indicating the similarity between items and latent factors.\nThe general goal of SVD (and other matrix factorization methods) is to decompose the matrix R with all missing $r_{ij}$ and multiply its components \\(U_{m*m} D_{m*n} V_{n*n}^{t}\\) once again. As a result, there are no missing values $r_{ij}$ and it is possible to recommend each user movies (items) they have not seen or purchased yet. To better understand linear algebra behind SVD, one can watch Gilbert Strang's lecture on SVD for MIT OpenCourseWare on YouTube channel or visit refer to NIT Singular Value Decomposition (SVD) tutorial.\n    Number of Factors and RMSE For the demonstrative purpose, let's examine the effect of number of latent factors k on the model's performance. Specifically, it is possible to visually observe the effect of multiple factors on error measurement. As in supervised machine learning, cross_validate computes the error rate for each fold. The following function computes the average of RMSE given by the five folds and append the empty list rmse_svd. Consequently, the list contains 100 measures of min RMSE given 100 consecutive values of k in each test set, and by five folds in every iteration.\ndef rmse_vs_factors(algorithm, data): \u0026quot;\u0026quot;\u0026quot;Returns: rmse_algorithm i.e. a list of mean RMSE of CV = 5 in cross_validate() for each factor k in range(1, 101, 1) 100 values Arg: i.) algorithm = Matrix factoization algorithm, e.g SVD/NMF/PMF, ii.) data = surprise.dataset.DatasetAutoFolds \u0026quot;\u0026quot;\u0026quot; rmse_algorithm = [] for k in range(1, 101, 1): algo = algorithm(n_factors = k) #[\u0026quot;test_rmse\u0026quot;] is a numpy array with min accuracy value for each testset loss_fce = cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=False)[\u0026quot;test_rmse\u0026quot;].mean() rmse_algorithm.append(loss_fce) return rmse_algorithm   0.0015829191458780407  rmse_svd = rmse_vs_factors(SVD,data)  To replicate the plot of performance for each subsequent model, the following chunk defines the function plot_rmse() with two arguments where rmse is a list of float values and algorithm is an instantiated matrix factorization model. The function returns a plot with two line subplots that display performance vs. numbers of factors. The second subplot only zooms in and marks k with the best performance, i.e. the minimum RMSE.\ndef plot_rmse(rmse, algorithm): \u0026quot;\u0026quot;\u0026quot;Returns: sub plots (2x1) of rmse against number of factors. Vertical line in the second subplot identifies the arg for minimum RMSE Arg: i.) rmse = list of mean RMSE returned by rmse_vs_factors(), ii.) algorithm = STRING! of algo \u0026quot;\u0026quot;\u0026quot; plt.figure(num=None, figsize=(11, 5), dpi=80, facecolor='w', edgecolor='k') plt.subplot(2,1,1) plt.plot(rmse) plt.xlim(0,100) plt.title(\u0026quot;{0} Performance: RMSE Against Number of Factors\u0026quot;.format(algorithm), size = 20 ) plt.ylabel(\u0026quot;Mean RMSE (cv=5)\u0026quot;) plt.subplot(2,1,2) plt.plot(rmse) plt.xlim(0,50) plt.xticks(np.arange(0, 52, step=2)) plt.xlabel(\u0026quot;{0}(n_factor = k)\u0026quot;.format(algorithm)) plt.ylabel(\u0026quot;Mean RMSE (cv=5)\u0026quot;) plt.axvline(np.argmin(rmse), color = \u0026quot;r\u0026quot;)  plot_rmse(rmse_svd,\u0026quot;SVD\u0026quot;)  \nAccording to the figure, there is an increasing trend of worse performance with higher k. The lowest RMSE is achieved when $k=4$. However, it is worth mentioning that $k=14$ is also very close to the RMSE achieved with only 4 latent factors. Besides, the author argues that it is not probable that the user's taste (rating) is determined by such a low number of factors. On the other hand, the result suggests a range of values which can be used in GridSearchCV()for parameter tunning.\nGridsearchCV (Sample) param_grid = {'n_factors': [4,6,9,11,14,18,29]} gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5) gs.fit(data) # best RMSE score print(gs.best_score['rmse']) # combination of parameters that gave the best RMSE score print(gs.best_params['rmse'])  0.8639552777419859 {'n_factors': 11}  To make the model generalizable, i.e. avoid over and underfitting, the grid algorithm finds n_factors = 11 optimal.\nTraining SVD Algorithm and Predictions Next, SVD(n_factors = 11) fits the model on trainset. To predict values, i.e. ratings, for each empty element $a_{ij}$ in the utility matrix, it is essential to specify: a.) the users and b.) particular movies that are not in the trainset. build_anti_testset() method of trainset accomplishes the goal. It returns a list of ratings (testset) that are not in the trainset or in the entire utility matrix $R$. Consequently, it is possible to use the fitted model and predict ratings for movies in testset. algo_SVD.test(testset)returns the list with predictions.\nalgo_SVD = SVD(n_factors = 11) algo_SVD.fit(trainset) # Predict ratings for all pairs (i,j) that are NOT in the training set. testset = trainset.build_anti_testset() predictions = algo_SVD.test(testset) # subset of the list predictions predictions[0:2]  [Prediction(uid=1, iid=318, r_ui=3.5110432282575212, est=5, details={'was_impossible': False}), Prediction(uid=1, iid=1704, r_ui=3.5110432282575212, est=4.953822490465707, details={'was_impossible': False})]  Prediction and Historical Reference As is mentioned above, the raw predictions are stored in a list. It would also be difficult to search for any userId and predictions with meaningful information. For example, let's assume there are two goals: i.) suggest any userId the top 10 unseen movies the person is likely to enjoy and ii.) recommend the user movies with titles and genres. Overall, it was the author's objective to define a function with the following properties:\n Map the predictions to each user.\n Return: i.) recommendations for any given userId and ii.) the user's historical ratings\n Return the above objects with specific reference to the movie and its genre in a readable format (i.e. tidy DataFrame)\n  The below function get_top_n() accomplishes the goals. The function takes five arguments. Specifically, predictions is the list with predictions (predictions = algo_SVD.test(testset) ) , userId is an arbitrary user's id, movies_df is DataFrame with title and genre to each movieId, ratings_df contains historical ratings, and n specifies how many movies should be recommended to the user. By default, n is set to 10. get_top_n() consists of two main parts: Part I. comes from the official Surprise documentation. It maps the prediction to each user, sorts them in descending order, and returns the top n (by default 10) recommended movies for the userId specified as the argument of the function. Part II. was inspired by an article published by IVIDIA data scientist Nick Becker on his blog. It prints the total number of movies rated by the user. Then, it merges the DataFrame objects, i.e. history and predictions, on movieID in movies_df. Therefore, besides movieId both objects consequently contain title and genres. One can then holistically evaluate the model's performance on an individual level because the function's output (data frames) allows to observing both the highest rated predictions and the highest rated movies in the past. It is assumed that high rated genres should correspond to the user's taste and are therefore expected to appear in the recommended movies as well.\ndef get_top_n(predictions, userId, movies_df, ratings_df, n = 10): '''Return the top N (default) movieId for a user,.i.e. userID and history for comparisom Args: Returns: ''' #Peart I.: Surprise docomuntation #1. First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) #2. Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key = lambda x: x[1], reverse = True) top_n[uid] = user_ratings[: n ] #Part II.: inspired by: https://beckernick.github.io/matrix-factorization-recommender/ #3. Tells how many movies the user has already rated user_data = ratings_df[ratings_df.userId == (userId)] print('User {0} has already rated {1} movies.'.format(userId, user_data.shape[0])) #4. Data Frame with predictions. preds_df = pd.DataFrame([(id, pair[0],pair[1]) for id, row in top_n.items() for pair in row], columns=[\u0026quot;userId\u0026quot; ,\u0026quot;movieId\u0026quot;,\u0026quot;rat_pred\u0026quot;]) #5. Return pred_usr, i.e. top N recommended movies with (merged) titles and genres. pred_usr = preds_df[preds_df[\u0026quot;userId\u0026quot;] == (userId)].merge(movies_df, how = 'left', left_on = 'movieId', right_on = 'movieId') #6. Return hist_usr, i.e. top N historically rated movies with (merged) titles and genres for holistic evaluation hist_usr = ratings_df[ratings_df.userId == (userId) ].sort_values(\u0026quot;rating\u0026quot;, ascending = False).merge\\ (movies_df, how = 'left', left_on = 'movieId', right_on = 'movieId') return hist_usr, pred_usr  SVD Recommendations Since the model was properly trained, it is already possible to suggest any userID n movies at this stage. Additionally, based on the acquired predictions and defined get_top_n() function it is reasonable to visually inspect the recommended movies and the user's highest rated movies in the past. For instance, let's assume the userId 124 wants to watch a movie at their earliest convenience but has no specific title in mind. On top of that, the movie database contains over 6,000 titles across multiple genres so the user would spend a lot of time researching for what movies are in line with respect to their specific preferences. The following code makes the user's choice much easier. After calling get_top_n() function, it is immediately obvious the user 124 has already rated 50 movies.\nhist_SVD_124, pred_SVD_124 = get_top_n(predictions, movies_df = movies_df, userId = 124, ratings_df = ratings_df)  To see the user's history, let's examine their top 15 highest rated movies. The ratings are in the range from 5 to 4.5. As the table below shows, the user 124 enjoys a wide range of genres. Specifically, the highest rated movies (rating 5) are mostly dramas. Additionally, the user has mostly rated comedies, thrillers, and action/adventure movies.\nhist_SVD_124.head(15)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    userId movieId rating title genres     0 124 1358 5.0 Sling Blade (1996) Drama   1 124 3949 5.0 Requiem for a Dream (2000) Drama   2 124 7361 5.0 Eternal Sunshine of the Spotless Mind (2004) Drama|Romance|Sci-Fi   3 124 6377 5.0 Finding Nemo (2003) Adventure|Animation|Children|Comedy   4 124 2858 5.0 American Beauty (1999) Drama|Romance   5 124 356 5.0 Forrest Gump (1994) Comedy|Drama|Romance|War   6 124 608 5.0 Fargo (1996) Comedy|Crime|Drama|Thriller   7 124 3252 4.5 Scent of a Woman (1992) Drama   8 124 1210 4.5 Star Wars: Episode VI - Return of the Jedi (1983) Action|Adventure|Sci-Fi   9 124 1196 4.5 Star Wars: Episode V - The Empire Strikes Back... Action|Adventure|Sci-Fi   10 124 3328 4.5 Ghost Dog: The Way of the Samurai (1999) Crime|Drama   11 124 1884 4.5 Fear and Loathing in Las Vegas (1998) Adventure|Comedy|Drama   12 124 4226 4.5 Memento (2000) Mystery|Thriller   13 124 3147 4.5 Green Mile, The (1999) Crime|Drama   14 124 5608 4.5 Das Experiment (Experiment, The) (2001) Drama|Thriller     When it comes to predictions, the outcome is stored in the same order and format as the user's history.\npred_SVD_124   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    userId movieId rat_pred title genres     0 124 750 4.610126 Dr. Strangelove or: How I Learned to Stop Worr... Comedy|War   1 124 1136 4.545477 Monty Python and the Holy Grail (1975) Adventure|Comedy|Fantasy   2 124 904 4.541237 Rear Window (1954) Mystery|Thriller   3 124 1208 4.537614 Apocalypse Now (1979) Action|Drama|War   4 124 1204 4.535593 Lawrence of Arabia (1962) Adventure|Drama|War   5 124 56782 4.517568 There Will Be Blood (2007) Drama|Western   6 124 1225 4.513478 Amadeus (1984) Drama   7 124 898 4.512611 Philadelphia Story, The (1940) Comedy|Drama|Romance   8 124 2959 4.510696 Fight Club (1999) Action|Crime|Drama|Thriller   9 124 2160 4.499703 Rosemary's Baby (1968) Drama|Horror|Thriller     Comparing the predictions with history, one can observe that the genres are in line with the user's taste.\nNon-Negative Matrix Factorization (NMF) NMF is another method used for matrix factorization. Contrary to SVD, NMF decomposes the non-negative utility matrix R into the product of matrices W and H: \\(R_{n*d} = W_{n*r} H_{r*d}\\)\nWhere columns in matrix \\(W_{n*r}\\) represent components, while matrix \\(H_{r*d}\\) stores the corresponding weights. More importantly, NMF introduces constraints under which: $W \\geq 0$ and \\(H \\geq 0\\). The component-wise nonnegativity is a substantial difference from SVD (Gillis, 2017). Additionally to collaborative filtering, one can find use cases of NMF in clustering, image processing, or music analysis.\nrmse_nmf = rmse_vs_factors(NMF, data)  plot_rmse(rmse_nmf, \u0026quot;NMF\u0026quot;)  \nparam_grid = {'n_factors': [11,14,15,16,17,18,20]} gs = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=5) gs.fit(data) # best RMSE score print(gs.best_score['rmse']) # combination of parameters that gave the best RMSE score print(gs.best_params['rmse'])  0.8861525979842921 {'n_factors': 17}  algo_NMF = NMF(n_factors = 16) algo_NMF.fit(trainset) # Predict ratings for all pairs (u, i) that are NOT in the training set. testset = trainset.build_anti_testset() predictions = algo_NMF.test(testset)  hist_NMF_124, pred_NMF_124 = get_top_n(predictions, movies_df = movies_df, userId = 124, original_ratings_df = ratings_df)  User 124 has already rated 50 movies.  pred_NMF_124   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    userId movieId rat_pred title genres     0 124 2324 4.604147 Life Is Beautiful (La Vita Ã¨ bella) (1997) Comedy|Drama|Romance|War   1 124 1089 4.558471 Reservoir Dogs (1992) Crime|Mystery|Thriller   2 124 1213 4.548465 Goodfellas (1990) Crime|Drama   3 124 1248 4.544143 Touch of Evil (1958) Crime|Film-Noir|Thriller   4 124 1104 4.541697 Streetcar Named Desire, A (1951) Drama   5 124 750 4.536357 Dr. Strangelove or: How I Learned to Stop Worr... Comedy|War   6 124 904 4.530793 Rear Window (1954) Mystery|Thriller   7 124 1235 4.526960 Harold and Maude (1971) Comedy|Drama|Romance   8 124 1242 4.517884 Glory (1989) Drama|War   9 124 898 4.512130 Philadelphia Story, The (1940) Comedy|Drama|Romance     Conclusion The article discussed the â€œfundamentals of recommender systems and their classificationâ€. Moreover, the author showed how to use Surprise package for two matrix factorization approaches. Among them, SVD achieved slightly lower RMSE (0.864), and therefore performed better, compared to measured RMSE (0.886) by NMF. Both models were hyper parametrized for several latent factors used in training of the algorithms. In addition, the authors showed how to evaluate the models on an individual level by observing the predicted movies and the user's historical rating.\nCitation Lineberry, A., \u0026amp; Longo, C. (2018, September 11). Creating a hybrid content-collaborative movie recommender using deep learning. Retrieved from https://towardsdatascience.com/creating-a-hybrid-content-collaborative-movie-recommender-using-deep-learning-cc8b431618af\nMaxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\nGillis, N. (2014). The why and how of nonnegative matrix factorization (2). Retrieved from https://arxiv.org/abs/1401.5226v2\nGuruswami, V., \u0026amp; Kannan, R. (2012). Singular value decomposition (SVD). In Computer science theory for the information age (pp. 111-135).\nSarwar, Badrul \u0026amp; Badrul, \u0026amp; Karypis, George \u0026amp; Cybenko, George \u0026amp; Konstan, \u0026amp; Joseph, \u0026amp; Reidl, \u0026amp; Tsibouklis, John. (2001). Item-based collaborative filtering recommendation algorithmus.\n","date":1571008357,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571008357,"objectID":"b937605e93ed8e0740438ea740b3b2e3","permalink":"https://jiristodulka.com/%60/post/recsys_cf/","publishdate":"2019-10-13T19:12:37-04:00","relpermalink":"/`/post/recsys_cf/","section":"post","summary":"This article targets anyone with previous exposure to machine learning but with little to no knowledge of the recommendation systems. However, it is highly probable that anyone interested in this work interacts with a recommender system regularly. Anyone who listens to Spotify or watches movies on Netflix benefits from the rigorous algorithms (recommendation systems) developed by teams of data scientists and software engineers. The theoretical part of the article explains the fundamentals of various recommendation systems.","tags":["recommender","system","CF","SVD","NMF","matrix","factorization"],"title":"Collaborative Filtering: Matrix Factorization Recommender System ","type":"post"},{"authors":null,"categories":null,"content":"Research article available eher: https://arxiv.org/abs/1905.09275 ","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569801600,"objectID":"739cb9cceecd58f482162451fb36d841","permalink":"https://jiristodulka.com/%60/project/30th-sep-2019/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/`/project/30th-sep-2019/","section":"project","summary":"Research article available eher: https://arxiv.org/abs/1905.09275 ","tags":["Deep Learning","Reinforcement Learning"],"title":"COBRA (DRL)","type":"project"},{"authors":null,"categories":null,"content":"Returned to Toronto from Europe on Tuesday. I have been recently studying amazing review of recent efforts on deep learning based recommender systems: Deep Learning based Recommender System: A Surve and New Perspectives published by Shuai Zhang et al. So why DL based recsys? Firstly, DL models are end-to-end differentiable, secondly they provide suitable inductive biases. Therefore, DL models can exploit inherent structures if there are any. I particularly like MLP for its simplicity and capability to learn hierarchical features. AE are oslo used in DL based recsys as unsupervised model reducing dimensionality in a similar manner as PCA. AE can be used both for for item-based and user-based models.\nAE is also powerful in learning feature representation, so it is possible to learn feature representations for user/item content features. Another interesting type of AE is Collaborative Deep Learning (CDL). It is a hierarchical model Bayesian model with stacked denoising AE (SDEAE). CDL has two components: i.) perception component (deep neural net.: probabilistic interpretation of of ordinal SDAE ) and ii.) task-specific component (PMF). Collaborative Deep Learning is a pairwise framework for top-n recommendations. Research shows that it can even outperform CDL when it comes to ranking predictions. It outputs a confidence value $C^{-1}_{uij}$ of how much a user u preferes item i over j.\nRNN also get my attention becase I see its usage in time-series because of variants such as LSTM. More interestingly, DRL is exciting because of a possibility to make recommendations based on on trial-and-error paradigm. For sequential modeling, RNN (interval memory) and CNN (filters sliding along with time, i.e. kernel). Sequential signals are important for mining temporal dynamics of user behaviour and time evolution.\nWhen it comes to activation functions, ReLu should be one to go for! Unlike in sigmoid where vanishing gradient may occur and its output is not zero centered, ReLu is always positive with the slope of 1. If the neurons die, one may use leaky ReLu which output may be even negative (but close to 0!). Tanh gives a zero centered output but again, vanishing gradient may arise as well.\n","date":1568332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568332800,"objectID":"59adcbf22faa033fc33b5ae6aa0856d8","permalink":"https://jiristodulka.com/%60/project/13th-sep-2019/","publishdate":"2019-09-13T00:00:00Z","relpermalink":"/`/project/13th-sep-2019/","section":"project","summary":"Returned to Toronto from Europe on Tuesday. I have been recently studying amazing review of recent efforts on deep learning based recommender systems: Deep Learning based Recommender System: A Surve and New Perspectives published by Shuai Zhang et al. So why DL based recsys? Firstly, DL models are end-to-end differentiable, secondly they provide suitable inductive biases. Therefore, DL models can exploit inherent structures if there are any. I particularly like MLP for its simplicity and capability to learn hierarchical features.","tags":["Deep Learning","Recommender System"],"title":"MLP, AE, RNN","type":"project"},{"authors":null,"categories":null,"content":"Called with Omar yesterday. He will finish editing my post about matrix factorization recommender system in a week. I really need to publish it. It\u0026rsquo;s been already three months from my last post. Definitely, I will follow up with smth. regarding DL. There is good repository on Wikipedia: List of datasets for machine-learning research. Now it\u0026rsquo;s time to focus on \u0026ldquo;Deep Learning based Recommender System: A Survey and New Perspectives\u0026rdquo; by Zhang et al. Wikipedia: RNN (recurrent neural network) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition[1] or speech recognition. Such network has over-performed standard recsys. I more and more think I should focus on techniques to build a recommender system based on DL and RL.\n","date":1567900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567900800,"objectID":"55e3e4c588e1f682ac959da7d5f72ec5","permalink":"https://jiristodulka.com/%60/project/8th-sep-2019/","publishdate":"2019-09-08T00:00:00Z","relpermalink":"/`/project/8th-sep-2019/","section":"project","summary":"Called with Omar yesterday. He will finish editing my post about matrix factorization recommender system in a week. I really need to publish it. It\u0026rsquo;s been already three months from my last post. Definitely, I will follow up with smth. regarding DL. There is good repository on Wikipedia: List of datasets for machine-learning research. Now it\u0026rsquo;s time to focus on \u0026ldquo;Deep Learning based Recommender System: A Survey and New Perspectives\u0026rdquo; by Zhang et al.","tags":["Deep Learning","Recommender System"],"title":"Deep Learning based Recommender System","type":"project"},{"authors":null,"categories":null,"content":"Keras models can have both trainable and fixed parameters. Fewer trainable parameters are less flexibel but at the same time less likely to overfit. Trainable parameters are usually in dense layers. Due to an embedding layer in the model (input) and dense layer (with e.g. 4 parameters), the model has much more trainable parameters. Embedding layers often add a large number of trainable parameters into the model (be cautious of overfitting). Embedding layer maps integers to floats: each unique value of of the embedding input gets a parameter for its output. Shared models work the same way as shared layers, i.e. I can put together a sequence of layers to define a custom model. Then, it\u0026rsquo;s possible to share the entire model in exactly the same way I would share a layer. Model stacking is using model\u0026rsquo;s prediction as input to another model. It\u0026rsquo;s one of the most sophisticated way of combining models. When stacking it\u0026rsquo;s important to use different datasets for each model. It\u0026rsquo;s also possible to create single model for classification and regression with Keras. In ordet to do that, I use activation = \u0026quot;sigmoid\u0026quot; in output layer. With two outputs model\u0026rsquo;s each output requires its own loss function passed as a list (e.g. different loss functions for a classification and regression models). Optimzer, e.g. \u0026quot;adam\u0026quot;, can be passed only once for the both model\n","date":1567382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567382400,"objectID":"0c0dd544ab47db671a3452b831228d73","permalink":"https://jiristodulka.com/%60/project/2nd-sep-2019/","publishdate":"2019-09-02T00:00:00Z","relpermalink":"/`/project/2nd-sep-2019/","section":"project","summary":"Keras models can have both trainable and fixed parameters. Fewer trainable parameters are less flexibel but at the same time less likely to overfit. Trainable parameters are usually in dense layers. Due to an embedding layer in the model (input) and dense layer (with e.g. 4 parameters), the model has much more trainable parameters. Embedding layers often add a large number of trainable parameters into the model (be cautious of overfitting).","tags":["Deep Learning"],"title":"Embedding Layers \u0026 Shared Models (Keras)","type":"project"},{"authors":null,"categories":null,"content":"Learned about shared layers allowed by Keras API. They allow me to define an operation and then apply the exact same operation, with the exact same weights on different inputs. Can by used for time-series as well. I understand, defining multiple input layers for multiple entities (e.g. customer IDs) allows me to specify how the data from each entity will be used differently later in this model. Now, I see an applied case in UFC. In data with UFC result for every match and fighter, I can teach the model to learn strength of every single fighter. Such that if any pair of fighters plays each other, I could predict the score, even if those two fighters have never fought before.\n","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"8e242ca6a34fd3116c339dd709adff10","permalink":"https://jiristodulka.com/%60/project/1st-sep-2019/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/`/project/1st-sep-2019/","section":"project","summary":"Learned about shared layers allowed by Keras API. They allow me to define an operation and then apply the exact same operation, with the exact same weights on different inputs. Can by used for time-series as well. I understand, defining multiple input layers for multiple entities (e.g. customer IDs) allows me to specify how the data from each entity will be used differently later in this model. Now, I see an applied case in UFC.","tags":["Deep Learning","Inspiration"],"title":"Shared Layers (Keras)","type":"project"},{"authors":null,"categories":null,"content":"Continuing with DL. Both theory and practice. When it comes to DL network: dense layers learn a weight matrix, where the first dimension of the matrix is the dimension of the input data, and the second dimension is the dimension of the output data. Defining tensor output in one line\nfrom keras.layers import Input, Dense input_tensor = Input(shape=(1,)) output_tensor = Dense(1)(input_tensor)  When it comes to loss function, mean absolute error is a good general function for a keras model. It\u0026rsquo;s less sensitive function to outliers. however, one can use mse which would be equivalent to OLS. In DL, $y^{\\hat} = mx + b$ where $m$ is the weight of the dense layer and $b$ is the bias of the dense layer. Mean squared error is a common loss function and will optimize for predicting the mean, as is done in OLS. Mean absolute error optimizes for the median and is used in quantile regression.\n","date":1567036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567036800,"objectID":"ee008e8f60e1942d3162fafe1078616f","permalink":"https://jiristodulka.com/%60/project/29th-aug-2019/","publishdate":"2019-08-29T00:00:00Z","relpermalink":"/`/project/29th-aug-2019/","section":"project","summary":"Continuing with DL. Both theory and practice. When it comes to DL network: dense layers learn a weight matrix, where the first dimension of the matrix is the dimension of the input data, and the second dimension is the dimension of the output data. Defining tensor output in one line\nfrom keras.layers import Input, Dense input_tensor = Input(shape=(1,)) output_tensor = Dense(1)(input_tensor)  When it comes to loss function, mean absolute error is a good general function for a keras model.","tags":["Deep Learning"],"title":"Dense Layer (Keras)","type":"project"},{"authors":null,"categories":null,"content":"Could not sleep because of a jetlag. So I started playing with compiling DL model. In binary classification I use softmax function for probability distribution. However, softmax is not zero centered! The neuron always fires. I am still continuing with Keras and plying with different parameters in fine-tuning my medel. I.e. for optimizer I use \u0026ldquo;adam\u0026rdquo; and loss function compute through \u0026ldquo;class entropy\u0026rdquo;. I am actually thinking I should start using PyTorch, instead NumPy, for running computations on GPU and PySpark (as once Dwight Gunning from FINRA has recommended me while having a meeting).e even hundereds or thousand nodes in a layer (Tensorflow can take care of that).\n","date":1566950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566950400,"objectID":"83cce58f0e3cf16de87f229cede9d524","permalink":"https://jiristodulka.com/%60/project/28th-aug-2019/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/`/project/28th-aug-2019/","section":"project","summary":"Could not sleep because of a jetlag. So I started playing with compiling DL model. In binary classification I use softmax function for probability distribution. However, softmax is not zero centered! The neuron always fires. I am still continuing with Keras and plying with different parameters in fine-tuning my medel. I.e. for optimizer I use \u0026ldquo;adam\u0026rdquo; and loss function compute through \u0026ldquo;class entropy\u0026rdquo;. I am actually thinking I should start using PyTorch, instead NumPy, for running computations on GPU and PySpark (as once Dwight Gunning from FINRA has recommended me while having a meeting).","tags":["Deep Learning"],"title":"Softmax","type":"project"},{"authors":null,"categories":null,"content":"Activation function: Inputs (variables or outputs of neurons) get weights. If the dot product of these weights is higher than certain value, the activation function simulata if the neuros fire or not. Activation function is close to a sigmoid function (e.g. square root function), so the output is between 0 and 1. Activation function captures non-linearities. Classification: Dog (0.71) or cat(0.29), i.e. 71% confidence the object is a dog. So why to use DL instead of linear regression? DL models allow interactions (no assumption of no multicollinearity). Cool thing about NNs is that I don\u0026rsquo;t need to specify interactions. They are captured by the model itself. ReLu is actually very simple:\ndef relu(input): '''Defines ReLu activation function: Input node dot product''' # Calculate the value for the output of the relu function: output output = max(0, input) # Return the value just calculated return(output)  e.g.\nprint(relu(5)) #5 print(relu(-10)) #0  Deep networks internally build representations of patterns in the data. Partially replace the need for feature engineering. Subsequent layers build increasingly sophisticated representations of raw data. Gradient descent is loss vs weight (I want to minimize the loss). Learned about back propagation. I need to recap it once (or probably several times) more. At least, I understand the analogy between a neural network and brain neuron. It fires back and forward! DC has a great tutorial by Dan Becker (Data Scientist at at Google).\nKeras: Dense layer means all the nodes in the current layer connect to all the nodes in the previous layer. There may be even hundereds or thousand nodes in a layer (Tensorflow can take care of that).\n","date":1566864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566864000,"objectID":"5f053520f11b50418e6b7477c1969597","permalink":"https://jiristodulka.com/%60/project/27th-aug-2019/","publishdate":"2019-08-27T00:00:00Z","relpermalink":"/`/project/27th-aug-2019/","section":"project","summary":"Activation function: Inputs (variables or outputs of neurons) get weights. If the dot product of these weights is higher than certain value, the activation function simulata if the neuros fire or not. Activation function is close to a sigmoid function (e.g. square root function), so the output is between 0 and 1. Activation function captures non-linearities. Classification: Dog (0.71) or cat(0.29), i.e. 71% confidence the object is a dog. So why to use DL instead of linear regression?","tags":["Deep Learning"],"title":"Activation Function ReLu","type":"project"},{"authors":null,"categories":null,"content":"I am geting back to DataCamp. I\u0026rsquo;ve figured out that following relevant tracks (e.g. DL for now) with combination of reading articles and watching videos is one of the most effective learning paths when the starting point and direction are unknown. I also connected and had a call with two other data scientist. Daniel\u0026rsquo;s from Ottawa, has a PhD in Computer Sciences. It turned out he knows the same folks in Toronto, mostly through following AISC. He knows Amir and Toronto ML Channel on Slack. On top of that, he is an expert in DL. So great connection! Diven is Omar\u0026rsquo;s apprentice as I am. He works on the same recsys utilizing Surprise and is moving to Toronto soon. We agreed on some strategy how to start building DL recsys together. Finally, I bought the ticket for reinforcement learning workshop. Reinforcement deep learning recommender system?! Hell, I am going for that!\n","date":1566518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566518400,"objectID":"8d16c36c0cf30709fd2dfe765c05155d","permalink":"https://jiristodulka.com/%60/project/23rd-aug-2019/","publishdate":"2019-08-23T00:00:00Z","relpermalink":"/`/project/23rd-aug-2019/","section":"project","summary":"I am geting back to DataCamp. I\u0026rsquo;ve figured out that following relevant tracks (e.g. DL for now) with combination of reading articles and watching videos is one of the most effective learning paths when the starting point and direction are unknown. I also connected and had a call with two other data scientist. Daniel\u0026rsquo;s from Ottawa, has a PhD in Computer Sciences. It turned out he knows the same folks in Toronto, mostly through following AISC.","tags":["Deep Learning"],"title":"Advanced Deep Learning with Keras in Python","type":"project"},{"authors":null,"categories":null,"content":"Called with a colleague, Mirek. Mirek uses Azur, he showed me to run it. Pretty simple compared to AWS. Anyway, Mirek has built a TS model (with ARIMAX) predicting gold price. He is trying to challange his model with DL; specifically LSTM. We discussed parameters like batch size and number of samples. Quite exciting! I\u0026rsquo;ve once read the best performance on TS data is usually achieved with combination of TS and DL models. On top of that, Mirek shared with me Bokeh library for dashboards. He says it\u0026rsquo;s simpler than Pyviz. I need to give it a try\u0026hellip;\n","date":1566432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566432000,"objectID":"6c8d4f6214ae982af0254151990d54be","permalink":"https://jiristodulka.com/%60/project/22st-aug-2019/","publishdate":"2019-08-22T00:00:00Z","relpermalink":"/`/project/22st-aug-2019/","section":"project","summary":"Called with a colleague, Mirek. Mirek uses Azur, he showed me to run it. Pretty simple compared to AWS. Anyway, Mirek has built a TS model (with ARIMAX) predicting gold price. He is trying to challange his model with DL; specifically LSTM. We discussed parameters like batch size and number of samples. Quite exciting! I\u0026rsquo;ve once read the best performance on TS data is usually achieved with combination of TS and DL models.","tags":["Deep Learning","Inspiration"],"title":"Dashborads and LSTM","type":"project"},{"authors":null,"categories":null,"content":"I am going for AWS because of its popularity (motivation: industry expectation). I regret it\u0026hellip; I lost all my day while trying to figure out everything. I closed AWS documentation and opened Deep Learning. I sweared I will read the entire book chapter by chapter. Application is what does matter but first I need to build on a solid \u0026ldquo;skeleton\u0026rdquo;, i.e. theory. I learned about different distribution functions and their application. E.g. mass and marginal distribution functions.\n","date":1566345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566345600,"objectID":"6f69e832d2b907ec254f3f3faa2abce4","permalink":"https://jiristodulka.com/%60/project/21st-aug-2019/","publishdate":"2019-08-21T00:00:00Z","relpermalink":"/`/project/21st-aug-2019/","section":"project","summary":"I am going for AWS because of its popularity (motivation: industry expectation). I regret it\u0026hellip; I lost all my day while trying to figure out everything. I closed AWS documentation and opened Deep Learning. I sweared I will read the entire book chapter by chapter. Application is what does matter but first I need to build on a solid \u0026ldquo;skeleton\u0026rdquo;, i.e. theory. I learned about different distribution functions and their application.","tags":["Deep Learning"],"title":"Amazon Web Services","type":"project"},{"authors":null,"categories":null,"content":"Starting with DL which I want to implement for a recommender system. This time I will need to use external GPU. Will probably go for Azure or AWS. I am exploring codes for embeddings and ReLU, Leaky ReLU, Parametric ReLU, Exponential Linear (ELU, SELU) activation functions. The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive. As always, the challenge is to find the starting point. To figure it out, it would be best to start with fast.ai\n","date":1566259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566259200,"objectID":"ebe06f93044c2865d5a835d66d89ac03","permalink":"https://jiristodulka.com/%60/project/20thaug2019/","publishdate":"2019-08-20T00:00:00Z","relpermalink":"/`/project/20thaug2019/","section":"project","summary":"Starting with DL which I want to implement for a recommender system. This time I will need to use external GPU. Will probably go for Azure or AWS. I am exploring codes for embeddings and ReLU, Leaky ReLU, Parametric ReLU, Exponential Linear (ELU, SELU) activation functions. The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.","tags":["Deep Learning"],"title":"Starting with Deep Learning","type":"project"},{"authors":[],"categories":["Python"],"content":" In this post, I will analyze major Crime Indicators in Toronto in years from 2014 to 2018. I obtained the publicly available data set from the Toronto Police Service. First, I will visually inspect the crime scene in the City. Specifically, I will use the Matplot library to demonstrate the composition of assaults. Additionally, I will mark the most criminal neighborhoods on the map while utilizing both the MarkerCluster and HeatMap as plugins of folium package. Examining the criminal behavior on the map, shows the downtown as the area with the highest concentration of crime. On top of that, criminal neighborhoods can be easily clustered. Then, one can see that areas alongside the major routes exhibit high criminal activity as well.\n#GitHub Repository git clone https://github.com/jiristo/toronto-crime-folium.git  \n\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import re #regular expression matching operations import folium #maping crime on the map from folium.plugins import HeatMap, MarkerCluster #making maping visually appealing %matplotlib inline  Loading Data I downloaded the original csv file MCI_2014_to_2018.csvfrom http://data.torontopolice.on.ca/datasets/mci-2014-to-2018. I preprocessed the data and selected only the variables of my interest and saved the file as toronto_crime.csv.\ncrime = pd.read_csv('toronto_crime.csv') crime = crime.drop(columns = \u0026quot;Unnamed: 0\u0026quot;)  The original data set already breaks down the occurrencedate (yyy-MM-dd HHâ€Tâ€:mm:ss.SSSâ€™Zâ€™) into marginal time categories, i.e. year, month, hour, etc. However, I wanted to show how easily Python and pandas can generate these variables from the string format.\ncrime[\u0026quot;datetime\u0026quot;] = pd.to_datetime(crime[\u0026quot;occurrencedate\u0026quot;]) crime['year'] = crime['datetime'].dt.year crime['month'] = crime['datetime'].dt.month crime['dayofweek'] = crime['datetime'].dt.dayofweek crime['hour'] = crime['datetime'].dt.hour  crime[\u0026quot;MCI\u0026quot;] = crime[\u0026quot;MCI\u0026quot;].astype('category')  I am not presenting EDA of the date in this post. If you wish to see some of the methods I have used, I recommend you read my previous post on â€œMachine Learning and Diabetesâ€, where I disclose some of the frequently used commands.\nCrime Composition in Toronto Technically, visualizing the crime composition in the City if Toronto was the most difficult part of my work. Initially, my aim was to make the following visualization of every MCI. However, as you can see in the following code, this would take a lot of space. Therefore, I have decided to focus only on: * Crime in general * Different categories of assault in offence.\nAdditionally, I wanted to share an appealing visualization rather than pie charts (I originally started with). I admit, I had a lot of fun while playing with the following code. The whole objective with preprocessing data was to store values and labels separately in the same order.\nTherefore, the following section has three parts: 1. Generating values and labels for Crimes 2. Generating values and labels for Assaluts 3. Visualizing Crime and Assaults\nSince I am obsessed with writing neat and clean codes, or at least I always try my best, I wonder if you can come up with a more efficient solution? If so, can you kindly share it with me?\n1. Values and Labels: Crime Obtaining values and labels from crime was the easiest step and I do not think I need to explain the logic behind it. The process is as simple as \u0026ldquo;ask for values\u0026rdquo; and \u0026ldquo;ask for labels\u0026rdquo;.\nvalues_crime = crime[\u0026quot;MCI\u0026quot;].value_counts() labels_crime = crime[\u0026quot;MCI\u0026quot;].value_counts().keys()  2. Values and Labels: Assault However, coming up with the values and labels for assaults was a different story and it took me a while before I came up with and realized the Step 3. .\n Step 1: I filtered for rows with any form of \u0026ldquo;ASSAULT\u0026rdquo; in offence variable. I called the filtered df crime_assault. You can notice that I specified the selection to be case insensitive by flags=re.IGNORECASE.\n Step 2: I counted the values in crime_assault, i.e. \u0026ldquo;How many times each criminal act classified as assault appears in the data\u0026rdquo;.\n Step 3: This step was actually not necessary. However, when you look at the chart below, you should notice the category other. In fact, it consolidates the other three types of assaults: *Peace Officer Wpn/Cbh, Force/Thrt/Impede, and Aggravated Assault Avails Pros in the range from 251 to 12. As you can see, the values are marginal (compared to Assaults of 62194). Since the categories were overlaying in the initial plot, I have decided to consolidate them. I iterated over the rows in values_assault with the aim to rename the key of the value in offence smaller than 1500. I had to save it as pd.DataFrame object because of the following step.\n Step 4: Since the purpose of the whole procedure was to plot the data with labels and values, it was essential to store the values in the exact index order as the labels: sort_values(\u0026quot;offence\u0026quot;,ascending=False). In this step, I was grouping the data according to index. It is because values_assault was originally a Series object. However, after I stored it as pd.DataFrame object, I had to use index because key is strictly associated with Series objects.\n Step 5: I saved the index values, i.e. the categories of assault, as strings.\n  To wrap it up, the goal of these 5 steps was to store the values and labels separately in the same order to facilitate visualization.\ncrime_assault = crime[crime[\u0026quot;offence\u0026quot;].str.contains('ASSAULT', flags=re.IGNORECASE, regex=True)] #Step 1. values_assault = crime_assault[\u0026quot;offence\u0026quot;].value_counts() #Step 2. for key,value in values_assault.iteritems(): #Step 3. if value \u0026lt; 1500: values_assault= pd.DataFrame(values_assault.rename({key: \u0026quot;other\u0026quot;})) values_assault=values_assault.groupby(values_assault.index).sum().sort_values(\u0026quot;offence\u0026quot;,ascending=False) #Step 4. labels_assault = values_assault.index #Step 5.  Visualization As I already said, I wanted to generate an appealing visualization. I took inspiration from Kevin Amipara and his article called â€œA better visualization of Pie charts by MatPlotLibâ€.\nThe only changes I have made was to i.) plotting the two plots in subplots, and ii.) adding the legends.\nplt.figure(num=None, figsize=(15, 12)) ############################## Crime ################################ plt.subplot(1,2,1) plt.pie(values_crime, autopct='%1.1f%%', pctdistance=0.85, startangle=90, explode = [0.05]*labels_crime.shape[0]) #draw circle centre_circle = plt.Circle((0,0),0.70,fc='white') fig = plt.gcf() fig.gca().add_artist(centre_circle) #title+legend plt.title(\u0026quot;Share of Criminal Offences in Toronto\u0026quot;,size=20) plt.legend(labels_crime,loc=2) # Equal aspect ratio ensures that pie is drawn as a circle plt.axis('equal') plt.tight_layout() ############################## Assault ################################ plt.subplot(1,2,2) plt.pie(values_assault, autopct='%1.1f%%',pctdistance=0.85, startangle=90, explode = [0.05]*labels_assault.shape[0]) #draw circle centre_circle = plt.Circle((0,0),0.70,fc='white') fig = plt.gcf() fig.gca().add_artist(centre_circle) #title+legend plt.title(\u0026quot;Share of Assaults in Toronto\u0026quot;,size=20) plt.legend(labels_assault,loc=1) # Equal aspect ratio ensures that pie is drawn as a circle plt.axis('equal') plt.tight_layout() plt.show()  As you can see from the left chart (Criminal Offences), Torontians experience assaults most of the time. Assaults represent over 50% of the criminal activity in the city. Furthermore, â€œBreak and Enterâ€ represents 21% of the activity.\nLooking at the right plot (Assaults), you can infer more interesting facts. Firstly, 17% of the assaults are conducted with a weapon. Specifically, if you happened to be assaulted in Toronto, there is a 17% chance you were threaten by weapon. Unfortunately, I cannot infer the share of fire guns regarding these assaults. And according to â€œAssault Bodily Harmâ€, a person assaulted is 5% more likely to be bodily harmed.\nMapping Crime Since my family is visiting me soon from Europe, I was already concerned for their safety. I thought it would be effective to find places with high criminal density, so I can avoid them with my family. There are two ways in which you can do it: * 1. Utilize longitude and latitude coordinates as axis * 2. Use a model (e.g.ffolium) and plot crime on a map\nLongitude and Latitude on x and y Axis This is the easiest method of how to inspect criminality in the map. Simply put, Long and Lat are nothing but the coordinates. Given the high density of crime in the last four years, plotting crimes on a scatter plot should form a coherent map of the City of Toronto. Notice that I played with the arguments in plt.scatter(). The density was so high that I had to reduce dot sizes and decrease transparency as much as possible. The process required some trial and error method. Consequentially, one can infer several things from such simple mapping.\nplt.figure(num=None, figsize=(10, 8)) plt.scatter(\u0026quot;Long\u0026quot;, \u0026quot;Lat\u0026quot;, data = crime, c = 'y',alpha = 0.1, edgecolor = 'black', s=2) plt.grid() plt.xlabel('long') plt.ylabel('lat') plt.title('Toronto Crime') plt.tight_layout() plt.axis('tight') plt.show()  Firstly, there are a few high-density areas on the map. The most evident one is in the south of the City â€“ downtown. The neighborhoods around also exhibit high criminal activity and there are also other spots suggesting criminal neighborhoods can be clustered. Contrary, the white spots are green - park - areas. You can also infer that crime appears mostly alongside the major roads. Technically, you can observe every major street, avenue, and road in the City. Additionally, you can clearly see, the Young Street heading North from Downtown. Did you also know that Young St. is said to be the longest street in the world? Remember, these are just dots in the scatter plot.\nNeighborhoods You Should avoid in Toronto The following section supports my previous statement that criminal neighborhoods can be clustered. In the following code, I grouped the data by Neighbourhood (with 141 discrete values), and counted MCI in each district, i.e. top_N.\nThen, I dropped the duplicates in Neighbourhood in crime, and got Long and Lat coordinates. I joined the data with top_N. As a result, I obtained the map_data DataFrame object, where each row (with Neighbourhood index) records Lat, Lon, and MCI accordingly.\n# Top N Criminal Neighbourhoods in Toronto top_N = crime.groupby('Neighbourhood')[['MCI']].count().sort_values(by=['MCI']) # Coordinates Criminal Neighbourhoods map_data = crime[['Neighbourhood', 'Lat', 'Long']].drop_duplicates('Neighbourhood').set_index('Neighbourhood') \\ .join(top_N, how='inner')  See the most criminal neighborhoods in Toronto bellow:\nmap_data.sort_values(by=['MCI'], ascending=False).head(10)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Lat Long MCI   Neighbourhood        Church-Yonge Corridor (75) 43.663906 -79.384155 6301   Waterfront Communities-The Island (77) 43.644955 -79.397644 5674   West Humber-Clairville (1) 43.743992 -79.598869 4338   Moss Park (73) 43.657307 -79.373459 3609   Bay Street Corridor (76) 43.658077 -79.384712 3564   Kensington-Chinatown (78) 43.650070 -79.396881 3263   Woburn (137) 43.777592 -79.226578 3158   York University Heights (27) 43.774353 -79.499802 3141   Downsview-Roding-CFB (26) 43.733581 -79.483727 2974   Annex (95) 43.665916 -79.407471 2908     Finally, I could use map_data DataFrame alongside with folium and visualize criminality in neighborhoods as clusters with the heatmap.\nThe process follows a certain logic: 1. create a Map (m) with using longitude and latitude of the place of your interest. 2. if you want to add a marker or other feature from folium.plugins, generate the object and use add_to(M) where M usually represents folium.Map() object or other module, e.g. HeatMap()\n Step 1: Creating \u0026amp; adding clustering functionality, i.e. MarkerCluster(), to the map (m). Step 2: Creating \u0026amp; adding Marker for every row, i.e. neighborhood, based on Long and Lat to the cluster (cluster). Step 3: Creating \u0026amp; adding HeatMapto the map (m).  Finally, I could use map_data DataFrame along with folium and visualize criminality in the neighborhoods as clusters within the heatmap.\nThe process follows a certain logic: 1. Create a Map (m) with using longitude and latitude of the place of your interest. 2. If you want to add a marker or another feature from folium.plugins, generate the object and use add_to(M) where M usually represents folium.Map() object or other module, e.g. HeatMap()\n Step 1: Creating \u0026amp; adding clustering functionality, i.e. MarkerCluster(), to the map (m). Step 2: Creating \u0026amp; adding Marker for every row, i.e. neighborhood, based on Long and Lat to the cluster (cluster). Step 3: Creating \u0026amp; adding HeatMap to the map (m).\n# Mapping Criminal Neighbourhoods m = folium.Map( location=[43.702270, -79.366074], zoom_start=11 ) #Step 1: Clusters cluster = MarkerCluster().add_to(m) #Step 2: Clusters breaking into Markers for x in map_data.iterrows(): folium.Marker([x[1].Lat, x[1].Long]).add_to(cluster) #Step 3: Heat max_crime = map_data['MCI'].max() # max value as reference for the darkets shade heat = HeatMap(map_data.values, min_opacity=0.2, max_val=max_crime, radius=30, blur=20, max_zoom=11) heat.add_to(m) m # call m to see the heat map with clusters    Consequently, I created both visually appealing and interactive map of the most dense criminal places in the city. It is possible to click on any cluster and zoom in and out to see places that you might want to avoid.\n","date":1561764392,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561764392,"objectID":"27e310c725f69d1fff2f1d0f61fffad8","permalink":"https://jiristodulka.com/%60/post/toronto-crime/","publishdate":"2019-06-28T19:26:32-04:00","relpermalink":"/`/post/toronto-crime/","section":"post","summary":"In this post, I will analyze major Crime Indicators in Toronto in years from 2014 to 2018. I obtained the publicly available data set from the Toronto Police Service. First, I will visually inspect the crime scene in the City. Specifically, I will use the Matplot library to demonstrate the composition of assaults. Additionally, I will mark the most criminal neighborhoods on the map while utilizing both the MarkerCluster and HeatMap as plugins of folium package.","tags":["crime","python","toronto","clustering","visualization","folium","heatmap"],"title":"Toronto Crime and Folium","type":"post"},{"authors":[],"categories":["Python"],"content":" In this post, I will explore and configure a few classification algorithms (supervised machine learniIn this post, I will explore and configure a few classification algorithms (supervised machine learning). I originally wanted to apply ML and fit some models on diabetes data. The diabetes dataset is one of the most well known data available to ML enthusiasts for their learning purposes. After I had started writing my script for the post, I found an article written by Lahiru Liyanapathirana more than an year ago. It turned out that Lahiru wrote his article with the same intention; i.e. to fit the best suitable model. He did a great job and exhausted a wide range of suitable algorithms from Scikit-learn. However, this did not discourage me! Unlike Lahiru, I will focus only on four classifiers consecutively. Specifically, i.) k-nearest neighbor (k-NN), ii.) logistic regression, iii.) decision tree and iv.) random forest. However, and in addition to Lahiruâ€™s article, I will show and introduce a few â€œnewâ€ techniques. First, I will share how and why I handled the missing data differently (this turned out to have a negative impact on performance). Secondly, I will be fitting the model using Pipeline and SimpleImputer as the arguments of GridSearchCV. To achieve the best performance and generality, I will be also tuning the hyperparameters for every model individually. Last but not least, I maintain that accuracy is not and should not be the only criteria when evaluating the modelâ€™s performance!\ngit clone https://github.com/jiristo/MLdiabetes_inpectplot.git  Loading the necessary packages I loaded the following packages according to their consecutive roles in the process.\n# data handeling import pandas as pd import numpy as np # Data preprocessing for ML from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score # model traning and testing faciliators from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler # Overfiitting/underfitting guide from sklearn.model_selection import GridSearchCV # ML models to be explored from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier # Performace measurements from sklearn.metrics import classification_report # Visualization import scikitplot as skplt import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  from warnings import simplefilter # ignore all future warnings simplefilter(action='ignore', category= FutureWarning)  Exploratory Data Analysis (EDA) df = pd.read_csv(\u0026quot;diabetes_data.csv\u0026quot;)  df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    pregnancies glucose diastolic triceps insulin bmi dpf age diabetes     0 6 148 72 35 0 33.6 0.627 50 1   1 1 85 66 29 0 26.6 0.351 31 0   2 8 183 64 0 0 23.3 0.672 32 1   3 1 89 66 23 94 28.1 0.167 21 0   4 0 137 40 35 168 43.1 2.288 33 1     Looking at the head of the data frame, there are several 0 values. Aside from the label (binominal diabetes), the observation suggests some missing data! I bet if your insulin level was 0 and you would be still reading these lines, on a funny note you probably would be the first undead person interested in ML.\ndf.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    pregnancies glucose diastolic triceps insulin bmi dpf age diabetes     count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000   mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958   std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951   min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000   25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000   50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000   75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000   max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000     Based on the summary statistics, min indicates other features with missing values: i.) glucose ii.) diastolic (blood pressure), iii.) triceps, iv.) bmi, i.e. $bmi = \\frac{body weight}{height}$. Frankly, anyone with 0 blood pressure or bmi (body-mass-index) must be dead. Therefore, it is necessary to clean the data.\ndf.info() df.isnull().sum()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null int64 insulin 768 non-null int64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 diabetes 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB pregnancies 0 glucose 0 diastolic 0 triceps 0 insulin 0 bmi 0 dpf 0 age 0 diabetes 0 dtype: int64  There are 768 all numeric records with no NaN. Therefore, 0 values in the data represent missing records. How many â€œzerosâ€ are in each column of the data frame (df)? I answered the question with a for loop and count_nonzero() method of NumPy array.\nfor colname in df.columns[:8]: print('0s in \u0026quot;{variable}\u0026quot;: {count}'.format( variable=colname, count=np.count_nonzero(df[colname] == 0)))  0s in \u0026quot;pregnancies\u0026quot;: 111 0s in \u0026quot;glucose\u0026quot;: 5 0s in \u0026quot;diastolic\u0026quot;: 35 0s in \u0026quot;triceps\u0026quot;: 227 0s in \u0026quot;insulin\u0026quot;: 374 0s in \u0026quot;bmi\u0026quot;: 11 0s in \u0026quot;dpf\u0026quot;: 0 0s in \u0026quot;age\u0026quot;: 0  In general, there are three approaches in which a data scientist could handle so many missing values and they are as below: 1. Delete any observation with missing values, which would result in substantial loss of data. 2. Substitute missing values with either mean, median, or mode; which can be a great trade between regression to the mean but keeping the data. However, it is not the best solution when it comes to the substitution of more than half of values in a variable. 3. Give up; hell no.\nBefore dealing with the missing values, letâ€™s examine the data visually. Since Lahiru already presented the histograms of the variables, I have decided to expand the insight by examining the box plots. Unlike the histogram, a box plot is a great tool to identify outliers in the data.\ndf.drop(\u0026quot;diabetes\u0026quot;,axis=1).boxplot(figsize = [10, 7]) plt.show()  Obviously, the variables exhibit various distribution patterns. For example, insulin reaches both the greatest maxima and exhibits many overlapping outliers. Its distribution is right-skewed, and I expect the variable is a very strong predictor for the diagnosis of diabetes. You do not have to be an expert in ML but I do not think the data is the best sample for ML. Firstly, there are a lot of missing values. Secondly, ML is not very effective when it comes to handling outliers.\nSince, I will be using RandomForestClassifier, there is a way to quickly and visually investigate the importance of these features. This is the tool: skplt.estimators.plot_feature_importances. It plots the classifierâ€™s feature importance. You can visually inspect how much the variable, relative to other features, correlates to the occurrence of diabetes.\ndf2 = df[(df.insulin !=0) \u0026amp; (df.bmi !=0) \u0026amp; (df.pregnancies !=0) \u0026amp; (df.glucose !=0)] feature_names = df2.columns[:-1] randfor = RandomForestClassifier() randfor.fit(df2.drop(columns = \u0026quot;diabetes\u0026quot;, axis=1),df2[\u0026quot;diabetes\u0026quot;]) skplt.estimators.plot_feature_importances(randfor, feature_names=feature_names, figsize=(9, 5)) plt.show()  The result supports my assumption that glucose is a very strong predictor for the presence of diabetes.\nData Wrangling I see nothing wrong in deleting a few observations with missing values, however, in my case the approach would result in a loss of too many observations. Lahirru has probably realized the same and therefore deleted only the rows with missing data in glucose, diastolic, and bmi. Therefore, he deleted only a small fraction of all observations. To deal with the issue in my own way, I have decided to combine 1.) and 2.) approaches discussed above. I was curious whether I would achieve higher accuracy and precision and what recall would be. For the sake of learning, I even decided to share my experience regardless of the result. I designed my approach as follows: * Firstly, I decided to delete every row where the number of 0s was greater than 1. On the other hand, I made the assumption which probably does not hold, i.e., pregnancies do not exhibit any missing values and observed 0s are valid records indicating that the person is not pregnant. * Secondly, I replaced the remaining missing values with a `mode. * I will handle the operation using SimpleImputer in the Pipeline while fitting the model to the training data set and predicting.\nThe following chunk of code summarizes the first step of my approach:\nfor index,row in df.iterrows(): zero_count = row[1:6] if np.count_nonzero(zero_count==0) \u0026gt; 1: df.drop(index, inplace=True) df.shape  (534, 9)  As a result, I deleted only 234 rows. Deleting every observation with a missing value would result in loss of at least 374 rows.\nfor colname in df.columns[:8]: print('Ex-post0s in \u0026quot;{variable}\u0026quot;: {count}'.format( variable=colname, count=np.count_nonzero(df[colname] == 0)))  Ex-post0s in \u0026quot;pregnancies\u0026quot;: 78 Ex-post0s in \u0026quot;glucose\u0026quot;: 1 Ex-post0s in \u0026quot;diastolic\u0026quot;: 0 Ex-post0s in \u0026quot;triceps\u0026quot;: 0 Ex-post0s in \u0026quot;insulin\u0026quot;: 140 Ex-post0s in \u0026quot;bmi\u0026quot;: 1 Ex-post0s in \u0026quot;dpf\u0026quot;: 0 Ex-post0s in \u0026quot;age\u0026quot;: 0  Lastly, one should examine correlations between the variables and label. There are many ways to do so, but I particularly prefer the heat map.\nplt.figure(figsize = [8, 5.5]) sns.heatmap(df2.corr(), square=False, cmap='RdYlGn') plt.show()  I evaluated the correlations from three perspectives; i.e.,1.) General pattern, 2.) Correlation between diabetes and other variables, 3.) Correlation amongst all variables. 1. The data set contains only variables positively correlated with presence of diabetes.\n Most of the times, the variables exhibit low correlations between themselves and diabetes. However, glucose and age seem to be much more correlated with the target than other variables.\n All the features are mostly uncorrelated. However, you can notice correlations above 0.6 between:\n age and pregnancies glucose and insulin triceps and bmi    I consulted a relative of mine who studies medicine to take a look at the observations. She confirmed the correlations are in line with her expectancy. Finally, I substituted every 0 in the data frame (except in the label) with NaN:\nFinally, I substitued every 0 in the data frame (except in the label) with NaN :\ndf.glucose.replace(0, np.nan, inplace = True) df.insulin.replace(0, np.nan, inplace = True) df.bmi.replace(0, np.nan, inplace = True)  Pre-setting and Modeling Strategy Last but not least, I split the data into labels and 2D arrays for training and testing as is the standard (and not rocket science) approach in ML.\nX = df.drop(columns = \u0026quot;diabetes\u0026quot;, axis=1) y = df[\u0026quot;diabetes\u0026quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)  Imputer Referring back to the second step in my proposed approach, I utilized SimpleImputer and I passed it into Pipeline. The function substitutes NaN with a chosen value relative to the variable, e.g. mean, mode, etc.\nimp = SimpleImputer(strategy = \u0026quot;most_frequent\u0026quot;, missing_values = np.nan)  The following steps summarize my approach to fit a particular model. Moreover, I designed the logic to be perfectly replicable:\n create the list steps_model containing functions:\n imp StandardScaler model   create pipeline_model by calling Pipeline\n pass steps_model as the argument   Create a list parameters_model\n the object contains the parameters of a particular model with ranges of values   Create cv_model by calling GridSearchCV (cross validation)\n Specify the arguments as follows: pipeline _model , param_grid = parameters_model, scoring = \u0026quot;roc_auc\u0026quot;, cv = 5   Call cv_model.fit(X_train, y_train) to train a model\n Predict the label pred_model by calling cv_model.predict(X_test)\n Evaluate performance by:\n cv_model.best_params_(y_test, pred_model) accuracy_score(y_test, pred_model) classification_report(y_test, pred_model)   The strategy is identical to all the models that I will attempt to fit, i.e.: i.) k-nearest neighbor (k-NN), ii.) logistic regression, iii.) decision tree and iv.) random forest. Therefore, , I will discuss the key issues only in I.)k - Nearest Neighbour. I am intentionally avoiding commenting on other models because the logic remains the same. However, the list** parameters_model should be unique to each classifier because the parameters differ from model to model. I will comment on the performance and draw a conclusion in the last part of the article.\nI.) K - Nearest Neighbour # Step 1. steps_knn = [(\u0026quot;imputation\u0026quot;, imp), (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;knn\u0026quot;, KNeighborsClassifier())]  # Step 2. pipeline_knn = Pipeline(steps_knn)  # Step 3. For KNN, I only specified the range of paramters from 1 to 51. parameters_knn = {\u0026quot;knn__n_neighbors\u0026quot;:np.arange(1,51)}  # Step 4. cv_knn = GridSearchCV(pipeline_knn, param_grid = parameters_knn, scoring = \u0026quot;roc_auc\u0026quot;, cv = 5 )  Training the Model (Step 5.) By calling cv_model on the label and 2D data frame, the function utilizes all the steps from steps_modelat first. Specifically, only and only the data is: i.) cleaned (NaNs substituted with the mode), ii.) standardized, iii.) it can be classified by the classifier. Additionally, the process goes hand in hand with GridSearchCV what splits X_train and y_train into 5 identical data sets and performs cross-validation on each of them. However, each data set is split into different test and train sections. While cross-validating, GridSearchCV searches for the best parameters specified in parameters_model.\n# Step 5. cv_knn.fit(X_train, y_train)  GridSearchCV(cv=5, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('imputation', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='most_frequent', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform'))]), fit_params=None, iid='warn', n_jobs=None, param_grid={'knn__n_neighbors': array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])}, pre_dispatch='2*n_jobs', refit=True, return_train_score='warn', scoring='roc_auc', verbose=0)  # Step 6. pred_knn = cv_knn.predict(X_test)  Measuring Performance (Step 7.) classification_report is one of the methods to see a bigger picture of the modelâ€™s performance. Since I classified the binary variable relevant to human health, I want to focus on the precision and recall.\n precision is also called positive predictive value (PPV). Mathematically: $PPV = \\frac{TP}{TP+FP}$ It is similar to accuracy, but focuses only on data the model has predicted to be positive, i.e. diabetes = 1. Referring to a confusion matrix, precision of 1 means that there were no false positives. recall, also called sensitivity OR True Positive Rate (TPR), answers the question on how complete the results are, i.e. did the model miss any positive classes and to what extent? Mathematically expressed:$TPR = \\frac{TP}{TP+FN}$ In our case, low recall would mean that the model has incorrectly classified a lot of individuals with diabetes as healthy ones.  One should ask, what is the superior metric from the two? A friend of mine, a data scientist, told me he was asked the same at his interview. In fact, it really depends! For example, imagine cancer diagnostics, would you rather classify few more patients as false positive and after more precise examination conclude they had no cancer or would you rather let escape the ones with cancer as healthy individuals? In this particular case, the model should minimize $FN$ in the confusion matrix. Consequently, recall,i.e. TPR, which should be closer to 1. Lastly, there is always a trade-off between the two negatively correlated metrics.\n# Step 7. print(\u0026quot;KNN Parameters are: {}\u0026quot;.format(cv_knn.best_params_)) print(\u0026quot;KNN Accuracy is: {}\u0026quot;.format(accuracy_score(y_test, pred_knn))) print(classification_report(y_test, pred_knn))  KNN Parameters are: {'knn__n_neighbors': 47} KNN Accuracy is: 0.7142857142857143 precision recall f1-score support 0 0.70 0.96 0.81 100 1 0.83 0.31 0.45 61 micro avg 0.71 0.71 0.71 161 macro avg 0.76 0.64 0.63 161 weighted avg 0.75 0.71 0.67 161  II.) Logistic Reg # Steps 1. \u0026amp; 2. pipeline_logreg = Pipeline([(\u0026quot;imputation\u0026quot;, imp), (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;logreg\u0026quot;, LogisticRegression())])  # Step 3. parameters_logreg = {\u0026quot;logreg__C\u0026quot;:np.arange(0.1, 1.1, 0.1), \u0026quot;logreg__penalty\u0026quot;:(\u0026quot;l1\u0026quot;, \u0026quot;l2\u0026quot;)}  # Step 4. cv_logreg = GridSearchCV(pipeline_logreg, param_grid = parameters_logreg, cv = 5 )  # Step 5. cv_logreg.fit(X_train,y_train)  GridSearchCV(cv=5, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('imputation', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='most_frequent', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logreg', LogisticRegression(C=1.0, class_weight=None, dual=Fa...enalty='l2', random_state=None, solver='warn', tol=0.0001, verbose=0, warm_start=False))]), fit_params=None, iid='warn', n_jobs=None, param_grid={'logreg__C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), 'logreg__penalty': ('l1', 'l2')}, pre_dispatch='2*n_jobs', refit=True, return_train_score='warn', scoring=None, verbose=0)  #Â Step 6. pred_logreg = cv_logreg.predict(X_test)  III.) Decision Tree # Steps 1. \u0026amp; 2. pipeline_dectree = Pipeline([(\u0026quot;imputation\u0026quot;, imp), (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;dectree\u0026quot;, DecisionTreeClassifier())])  # Step 3. parameters_dectree = {'dectree__max_features': ['auto', 'sqrt', 'log2', None], #'dectree__min_samples_split': np.arange(2, 15), #'dectree__min_samples_leaf':np.arange(2, 15), 'dectree__random_state':[42]}  # Step 4. cv_dectree = GridSearchCV(pipeline_dectree, param_grid = parameters_dectree, cv = 5 )  # Step 5. cv_dectree.fit(X_train,y_train)  GridSearchCV(cv=5, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('imputation', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='most_frequent', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('dectree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, ... min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best'))]), fit_params=None, iid='warn', n_jobs=None, param_grid={'dectree__max_features': ['auto', 'sqrt', 'log2', None], 'dectree__random_state': [42]}, pre_dispatch='2*n_jobs', refit=True, return_train_score='warn', scoring=None, verbose=0)  # Step 6. pred_dectree = cv_dectree.predict(X_test)  IV.) Random Forest pipeline_randfst = Pipeline([(\u0026quot;imputation\u0026quot;, imp), (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;randfst\u0026quot;, RandomForestClassifier())])  paraparameters_randfst = {\u0026quot;randfst__n_estimators\u0026quot;: np.arange(5,200,5), \u0026quot;randfst__criterion\u0026quot;:['gini','entropy'], #\u0026quot;randfst__n_jobs\u0026quot;:[-1], \u0026quot;randfst__random_state\u0026quot;:[42]}  cv_randfst = GridSearchCV(pipeline_randfst, param_grid = paraparameters_randfst, cv = 5)  cv_randfst.fit(X_train, y_train)  GridSearchCV(cv=5, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('imputation', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='most_frequent', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randfst', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', ...obs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))]), fit_params=None, iid='warn', n_jobs=None, param_grid={'randfst__n_estimators': array([ 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195]), 'randfst__criterion': ['gini', 'entropy'], 'randfst__random_state': [42]}, pre_dispatch='2*n_jobs', refit=True, return_train_score='warn', scoring=None, verbose=0)  pred_randfst = cv_randfst.predict(X_test)  Step 7. \u0026amp; Best Model Selection I already mentioned precision and recall but did explain accuracy. In fact, accuracy may not be the best parameter for choosing the right model. Consider test data with 100 individuals from which 99 subjects are healthy ones and only 1 individual has diabetes. Also, assume the model successfully classified 99 healthy people but completely failed to classify the one individual with diabetes.\nGived that $accuracy = \\frac{TP + TN} {TP + TN + FP + FN}$, than the accuracy would be 99%. However, the algorithm missed 100% individuals in the positive class.\nThe following code displays the best parameters and accuracy of every classifier I fit. Moreover, classification_report displays the table with recall and precision, so you can effectively evaluate each model.\nprint(\u0026quot;KNN Parameters are: {}\u0026quot;.format(cv_knn.best_params_)) print(\u0026quot;KNN Accuracy is: {}\u0026quot;.format(accuracy_score(y_test, pred_knn))) print(classification_report(y_test, pred_knn)) print(\u0026quot;Log. reg Parameters are: {}\u0026quot;.format(cv_logreg.best_params_)) print(\u0026quot;Log. reg accuracy is: {}\u0026quot;.format(accuracy_score(y_test, pred_logreg))) print(classification_report(y_test, pred_logreg)) print(\u0026quot;Dec. tree Parameters are: {}\u0026quot;.format(cv_dectree.best_params_)) print(\u0026quot;Dec. tree accuracy is: {}\u0026quot;.format(accuracy_score(y_test, pred_dectree))) print(classification_report(y_test, pred_dectree)) print(\u0026quot;Rand. forest Parameters are: {}\u0026quot;.format(cv_randfst.best_params_)) print(\u0026quot;Rand. forest accuracy is: {}\u0026quot;.format(accuracy_score(y_test, pred_randfst))) print(classification_report(y_test, pred_randfst))  KNN Parameters are: {'knn__n_neighbors': 47} KNN Accuracy is: 0.7142857142857143 precision recall f1-score support 0 0.70 0.96 0.81 100 1 0.83 0.31 0.45 61 micro avg 0.71 0.71 0.71 161 macro avg 0.76 0.64 0.63 161 weighted avg 0.75 0.71 0.67 161 Log. reg Parameters are: {'logreg__C': 0.1, 'logreg__penalty': 'l2'} Log. reg accuracy is: 0.7639751552795031 precision recall f1-score support 0 0.76 0.91 0.83 100 1 0.78 0.52 0.63 61 micro avg 0.76 0.76 0.76 161 macro avg 0.77 0.72 0.73 161 weighted avg 0.77 0.76 0.75 161 Dec. tree Parameters are: {'dectree__max_features': None, 'dectree__random_state': 42} Dec. tree accuracy is: 0.6708074534161491 precision recall f1-score support 0 0.74 0.73 0.73 100 1 0.56 0.57 0.57 61 micro avg 0.67 0.67 0.67 161 macro avg 0.65 0.65 0.65 161 weighted avg 0.67 0.67 0.67 161 Rand. forest Parameters are: {'randfst__criterion': 'entropy', 'randfst__n_estimators': 70, 'randfst__random_state': 42} Rand. forest accuracy is: 0.7453416149068323 precision recall f1-score support 0 0.74 0.91 0.82 100 1 0.76 0.48 0.59 61 micro avg 0.75 0.75 0.75 161 macro avg 0.75 0.69 0.70 161 weighted avg 0.75 0.75 0.73 161  As you can see, the logistic model performs with the highest accuracy of 0.77 meaning the model predicted 77% of cases correctly. However, you do not want to send any patient home with diabetes as if she was healthy. From this perspective, you should choose the model with recall close to 1. Since, the logistic model has recall = 0.57, you may consider to decide for the decision tree classifier. Considering overall accuracy, the model performs by 10 p.p. worse. However, it has the largest recall from all the considered classifiers. Administering this model, you would have sent home the lowest possible number of patients with diabetes at the cost of reexamining greater number of healthy individuals.\nI am not arguing decision tree is the best possible model for such type of data. Actually, I do not assume someone uses this particular branch of machine learning when it comes to health. I am aware of cases where deep learning and neural network models are used instead. For example, a visual algorithm can detect cancer when it is trained on pictures of human cells.\nThe point I am trying to make is that accuracy is not the best and only one parameter for model selection. There are many factors one must consider when a model needs to be selected.\nConclusion I exI explained and demonstrated how important is it to explore your data before modeling. Specifically, there are more possibilities of how missing values can be recorded. To see a bigger picture of your data, it is important to ask yourself some basic questions, e.g. â€œcan someone have 0 blood pressure?â€. Once you identify missing values in your data, you should decide on how to deal with them. Would you delete every record with a missing value or would you substitute it with a mean, or would you try to be efficient and keep as much data as possible? I also put forth my point of view on how to find the feature\u0026rsquo;s importance on the final outcome. Finally, I evaluated the models based on recall rather than on accuracy because recall close to 1 minimizes the number of cases when a patient with either diabetes or cancer would be classified and treated incorrectly.\n","date":1559085992,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559085992,"objectID":"03446ad46ea7106ab5e5025df59560c4","permalink":"https://jiristodulka.com/%60/post/supervised-ml/","publishdate":"2019-05-28T19:26:32-04:00","relpermalink":"/`/post/supervised-ml/","section":"post","summary":"In this post, I will explore and configure a few classification algorithms (supervised machine learniIn this post, I will explore and configure a few classification algorithms (supervised machine learning). I originally wanted to apply ML and fit some models on diabetes data. The diabetes dataset is one of the most well known data available to ML enthusiasts for their learning purposes. After I had started writing my script for the post, I found an article written by Lahiru Liyanapathirana more than an year ago.","tags":["python","knn","regression","randomforest","ML","supervised","sklearn","classifier"],"title":"Supervised Machine Learning With Scikit-learn and Diabetes","type":"post"},{"authors":[],"categories":["R"],"content":"     Canada recently joined few other countries that have completely decriminalized cannabis consumption, hence making it entirely legal. While many argued the Country has infringed a number of international agreements, others sang chorales. Whatever your attitude towards legalization of cannabis may be, there are one set of people that are definitely happy and they are the â€œCanadian Cannabis consumersâ€. However, only provincial, i.e.Â governmental, legal entities are allowed to sell and distribute marihuana products until April 2019. Marihuana consumers may not break the law while smoking or passing a joint any more. What were the sentiments tied to people in the early days of Cannabis legalization and when the consumers were allowed to make online orders? Did the new policies meet the usersâ€™ expectations? I am sure, some future and official studies will answer the question very soon. However, I attempted to find the answer on my own. In this post, I will explain how I scraped one of the biggest website forums - Reddit. Additionally, I will demonstrate how I performed a simple sentiment analysis on a tidy dataset I had created.\nGitHub Repository:\ngit clone https://github.com/jiristo/webscraping_inspectplot.git Reading HTML Code Reddit is a static website. At the time, when I was scraping the forum, I did not know about its public API. However, the main purpose of my effort was to i.) learn web scraping and create my own data set, and ii.) understand web structure. To conduct the analysis, I was mostly interested in pure text, i.e.Â reviews, and their authors. Any contributor can also assign points (in the form of likes or dislikes) to the most appealing comments posted by other users. Each post also displays a time frame when the comment was posted. rvest library, developed by Hadley Wickham, is powerful enough to secure any accessible data within an HTML. Moreover, the package supports the syntax of tidyverse.\nFirst of all, I connected to the webpage.\nurl \u0026lt;- (\u0026quot;https://old.reddit.com/r/canadients/comments/9ovapz/legal_cannabis_in_canada_megathread/?limit=500\u0026quot;) I struggled a little while selecting all the nodes with comments and relevant information (my variables). However, you need to read HTML code, call read_html(), only once. Basically, every comment on Reddit is displayed in a â€œbubbleâ€. And most importantly, the â€œbubblesâ€ contain all the required information! Being new to HTML and web scraping, I experienced difficulties to specify the correct arguments inside html_nodes(). Examining HTML code and conducting some research, I realized I had to use CSS selectors to style the elements in HTML. To my understanding . as the argument in quotation marks means: â€œselect classes so .entry selector selects all the objects with the entry class. The output assigned to reviews is a list (of nodes) of length 484. Again, every node contains relevant information about each comment. Therefore, I called read_html() and html_nodes() only once.\n# library(rvest) reviews \u0026lt;- url %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026#39;.entry\u0026#39;) Generating the Variables I started with the authors. html_node(\u0026quot;.author\u0026quot;) selects all the objects within the author class from reviews. The output is a vector of a length of the reviews. Each element in the list is a node with the class author. For example: \u0026lt;a href=\u0026quot;https://old.reddit.com/user/Hendrix194\u0026quot; class=\u0026quot;author may-blank id-t2_n4hdv\u0026quot;\u0026gt;Hendrix194\u0026lt;/a\u0026gt;. This would be useless unless you call other functions from rvest, i.e. html_text(). It extracts the selector content that is, in this case, the authorâ€™s name: Hendrix194!\n Author author \u0026lt;- reviews %\u0026gt;% html_node(\u0026quot;.author\u0026quot;) %\u0026gt;% html_text()%\u0026gt;% str_trim() author \u0026lt;- as.factor(author) After obtaining and examining the value author, I found several [deleted] values. Initially, I thought these authors and their comments were deleted. However, for whatever reason, the comments were still visible. Therefore, I decided to not dispose them. Finally, I stored the vector as a factor because each author is an individual entity.\n Comment Naturally, each comment is the most important variable for my analysis. The approach was identical to scraping author. Once again, the function selects all the objects with the class. In addition to solely extracting the selector content by html_text(), I specified an additional argument trim = TRUE. trim eliminates any space character (which is invisible to human eye) before and after a string. Additionally, I dispose of newline separators \\n by calling the gsub() function and specifying the pattern as the functionâ€™s argument.\ncomment \u0026lt;- reviews %\u0026gt;% html_node(\u0026quot;.md\u0026quot;) %\u0026gt;% html_text(trim = TRUE ) %\u0026gt;% gsub(\u0026quot;\\n\u0026quot;,\u0026quot;\u0026quot;,.) After I extracted the content of .score, I ended up with a string vector where each value was composed of two words. Additionally, the first word was supposed to be numeric, e.g. \u0026quot;3 points\u0026quot;. To get it, I simply supplied 1; i.e.Â specified the position of my word of interest, as the argument inside stringr::word() and piped the result into as.integer().\n# library(stringr) likes \u0026lt;- reviews %\u0026gt;% html_node(\u0026quot;.score\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% word(1) %\u0026gt;% as.integer()  Initially, I thought I would only be able to scrape the time variable as it is displayed on Reddit, e.g. \u0026quot;3 months ago\u0026quot;. Fortunately, such format is the output of java, which together with the HTML and CSS, makes any websiteâ€™s content readable to a human. Note, that I also did not extract selectorâ€™s content as I did with the previous variables.\nFinally, I formatted time with the base striptime() function and transformed it by ymd_hms() into POSIXct. Such formatting may have major benefits when analyzing sentiments in units of minutes, seconds but also at lower frequencies!\n# lubridate date \u0026lt;- reviews %\u0026gt;% html_node(\u0026quot;time\u0026quot;)%\u0026gt;% html_attr(\u0026quot;title\u0026quot;)%\u0026gt;% strptime(format = \u0026quot;%a %b %d %H:%M:%S %Y\u0026quot;,tz = \u0026quot;UTC\u0026quot;)%\u0026gt;% ymd_hms()  Data Frames I ended up with 4 vectors. Firstly, I merged all of them into one data frame dataset. I also called filter and mutate to simultaneously clean the data a little bit and create id - a unique value to every comment. dataset is my entry level data frame for two additional tidy datasets. Additionally, it is easy to search for any id and examine the associated comment.\n# Data frame from vectors dataset \u0026lt;- data.frame(author, likes, comment, date, stringsAsFactors = FALSE) %\u0026gt;% filter(!is.na(author))%\u0026gt;% # Filtering comments of few words filter(str_count(comment)\u0026gt;=5) %\u0026gt;% # creating ID for each comment so I can refer to it later mutate(id = row_number()) Secondly, it was necessary to clean dataset$comment. Specifically, any stop words, single and meaningless characters, and numbers that are redundant for the majority of text analysis. My goal was to create a new tidy data frame where each observation would be a single word from the comment it appears in. I achieved that through tidytext::unnest_tokens(), tidytext::stop_words, dplyr::anti_join(). unnest_token() takes three arguments: i.) dataset, ii.) new variable (word), and iii.) name of a string column (comment). The output is the tidy data frame where number of rows per entity (comment or id) is equal to number of words in comment. It is also worthy of mention that the function sets all the letters to lower case by default. I also called anti_join() three times to filter: i.) stop words (stop_words is a vector of the stop words from tidytext), ii.) URL links (url_words), iii.) and numeric strings. Note that the argument in anti_join() must not be a vector! Finally, I filtered any words of lower length or equal to 3 characters.\n#tidytext url_words \u0026lt;- tibble( word = c(\u0026quot;https\u0026quot;,\u0026quot;http\u0026quot;)) #filternig weblinks tidy_data \u0026lt;- dataset %\u0026gt;% unnest_tokens(word, comment) %\u0026gt;% anti_join(stop_words, by=\u0026quot;word\u0026quot;) %\u0026gt;% anti_join(url_words, by=\u0026quot;word\u0026quot;) %\u0026gt;% # filtering \u0026quot;numeric\u0026quot; words anti_join(tibble(word = as.character (c(1:10000000))), by=\u0026quot;word\u0026quot;) %\u0026gt;% # stop word already filter but there were still redundent words, e.g. nchar(word) \u0026lt; 3 filter(nchar(word)\u0026gt;=3) rm(url_words) Lastly, I created data_sentiment data frame from tidy_data. I achieved that by filtering and joining the data from nrc lexicon. I had chosen nrc because it contains a variety of emotions associated with the word. I also created author_total, i.e.Â how many words in total has each author posted to the forum. inner_join(get_sentiments(\u0026quot;nrc\u0026quot;), by=\u0026quot;word\u0026quot;) joins nrc lexicon with sentiments to tidy_data. Note that inner_join() filters out any words out of the intersection of any two data sets.\ndata_sentiment \u0026lt;- tidy_data %\u0026gt;% # Group by author group_by(author) %\u0026gt;% # Define a new column author_total; i.e. how many words an author has posted mutate(author_total=n()) %\u0026gt;% ungroup() %\u0026gt;% # Implement sentiment analysis with the NRC lexicon inner_join(get_sentiments(\u0026quot;nrc\u0026quot;), by=\u0026quot;word\u0026quot;) These steps above summarize my preprocessing and data cleaning strategies. Having three different data sets, there was nothing else impeding the analysis. From this step, it was easy to explore and analyze the data more precisely.\n Exploratory Data Analysis (EDA) Collecting any time varying variable, one should always know the time span.\nrange(dataset$date) #time span ## [1] \u0026quot;2018-10-17 04:17:53 UTC\u0026quot; \u0026quot;2019-03-10 04:45:13 UTC\u0026quot; range(dataset$date)[2]-range(dataset$date)[1] ## Time difference of 144.019 days Interesting! The very first comment on the forum was posted on the day when legalization happened, i.e. 2018-10-17. On the other hand, the last comment was submitted (in time of writing this post) in March 2019. This tells us the discussion has been alive for 145 days, i.e.Â almost 5 months. Initially, I thought that the time frame would be sufficient for interesting observations and conclusions. However, the frequency of new comments matters as well. Is there a representative number of submitted comments for each month? One way to answer the question would be a visual examination.\nThe following plot displays the count of new comments in time.\ndataset %\u0026gt;% select(date,comment) %\u0026gt;% mutate(date = round_date(date, \u0026quot;1 day\u0026quot;)) %\u0026gt;% group_by(date) %\u0026gt;% mutate(n_comments = n()) %\u0026gt;% # filter(date \u0026lt; ymd(\u0026quot;2018-10-25\u0026quot;)) %\u0026gt;% # Had to round up the date object into \u0026quot;week\u0026quot; units, otherwise grouping a mutating would not work (too narrow interval) ggplot(aes(date,n_comments)) + geom_line(linetype = 1)+ ggtitle(\u0026quot;Number of Comments in Time\u0026quot;) Unfortunately, the discussion was truly â€œaliveâ€ only a few weeks after October 17th. After the end of October 2018, new comments per day or even month were marginal.\nEven though I could not answer my initial questions in the full extent, I could focus on the cannabis consumersâ€™ initial attitude. So how many comments were posted there?\nnrow(dataset) # n comments ## [1] 460 Besides the number of comments, and the frequency of new ones, the total of contributors to the discussion should be represented as well.\nnlevels(dataset$author) #n levels ## [1] 166 Relative to the total number of comments, I maintain that enough authors have contributed into the discussion.\n Text Analysis Did you already examine this postâ€™s cover picture? It is the word cloud from wordcloud2 package and it displays the most frequent words in the discussion. By default, words with the highest frequency are centered in the middle of the plot. I like word clouds because they are great tools for very first inspection of text data. You can easily infer the subject being discussed.\n# library(wordcloud2) tidy_data %\u0026gt;% select(word) %\u0026gt;% count(word,sort=T) %\u0026gt;% wordcloud2(backgroundColor = \u0026quot;black\u0026quot;, color = \u0026quot;green\u0026quot;)  {\"x\":{\"word\":[\"cannabis\",\"legal\",\"ocs\",\"people\",\"time\",\"day\",\"weed\",\"shipping\",\"visa\",\"card\",\"market\",\"buy\",\"alberta\",\"lol\",\"ontario\",\"strains\",\"government\",\"online\",\"debit\",\"black\",\"credit\",\"oil\",\"pretty\",\"update\",\"canada\",\"store\",\"website\",\"yeah\",\"itâ€™s\",\"morning\",\"prices\",\"bought\",\"hours\",\"line\",\"seeds\",\"data\",\"days\",\"donâ€™t\",\"illegal\",\"street\",\"border\",\"flower\",\"home\",\"medical\",\"post\",\"smoke\",\"stuff\",\"trade\",\"allowed\",\"guys\",\"iâ€™m\",\"lot\",\"shipped\",\"wait\",\"bank\",\"canadian\",\"laws\",\"night\",\"price\",\"smoking\",\"sold\",\"stock\",\"thatâ€™s\",\"thinking\",\"week\",\"cbd\",\"guess\",\"happy\",\"legally\",\"mine\",\"policy\",\"pre\",\"retail\",\"stores\",\"strain\",\"address\",\"bud\",\"call\",\"called\",\"canâ€™t\",\"company\",\"delivery\",\"drive\",\"edibles\",\"feel\",\"found\",\"information\",\"issue\",\"locations\",\"mastercard\",\"ocs.ca\",\"packaging\",\"prepaid\",\"province\",\"public\",\"purchase\",\"read\",\"sell\",\"selling\",\"ship\",\"smoked\",\"specifically\",\"start\",\"super\",\"youâ€™re\",\"amount\",\"bit\",\"buds\",\"cost\",\"edit\",\"fee\",\"google\",\"grey\",\"hope\",\"kush\",\"law\",\"legalization\",\"marijuana\",\"medreleaf\",\"money\",\"postal\",\"question\",\"rolling\",\"sativa\",\"service\",\"statement\",\"vanilla\",\"waiting\",\"worth\",\"american\",\"aurora\",\"based\",\"coming\",\"considered\",\"dont\",\"fine\",\"flowr\",\"half\",\"haze\",\"industry\",\"info\",\"live\",\"middle\",\"montreal\",\"news\",\"nice\",\"option\",\"paid\",\"park\",\"pink\",\"plants\",\"pot\",\"products\",\"reason\",\"search\",\"security\",\"site\",\"thc\",\"trading\",\"vape\",\"weedmd\",\"weekend\",\"access\",\"actual\",\"ago\",\"avenue\",\"business\",\"buying\",\"cards\",\"coffee\",\"concentrate\",\"concentrates\",\"cool\",\"country\",\"customer\",\"doubt\",\"edison\",\"eigth\",\"experience\",\"free\",\"fuck\",\"ghost\",\"gift\",\"gram\",\"green\",\"grow\",\"growing\",\"guy\",\"hash\",\"helps\",\"hotel\",\"ill\",\"illicit\",\"makes\",\"oils\",\"page\",\"past\",\"phone\",\"picture\",\"pictures\",\"power\",\"pricing\",\"processing\",\"provinces\",\"purchased\",\"quality\",\"questions\",\"roll\",\"saturday\",\"seed\",\"selection\",\"shatter\",\"shop\",\"similar\",\"smell\",\"sort\",\"sounds\",\"specific\",\"surprised\",\"taxes\",\"thread\",\"times\",\"told\",\"tomorrow\",\"train\",\"travel\",\"type\",\"user\",\"wondering\",\"yesterday\",\"3.5g\",\"act\",\"aged\",\"ahead\",\"amazing\",\"amex\",\"anxiety\",\"assume\",\"bad\",\"ban\",\"banking\",\"broken\",\"burn\",\"calgary\",\"canopy\",\"changed\",\"check\",\"chill\",\"code\",\"complain\",\"completely\",\"container\",\"declined\",\"doesnâ€™t\",\"door\",\"drinking\",\"email\",\"entry\",\"gonna\",\"grabbed\",\"growth\",\"hack\",\"haha\",\"hear\",\"hey\",\"house\",\"indica\",\"issues\",\"l'acadie\",\"left\",\"licenced\",\"limited\",\"local\",\"location\",\"love\",\"luck\",\"main\",\"major\",\"means\",\"minutes\",\"oct\",\"officers\",\"organigram\",\"pay\",\"paying\",\"person\",\"pick\",\"plain\",\"police\",\"privacy\",\"product\",\"quarter\",\"quick\",\"rafael\",\"ready\",\"recently\",\"reddit\",\"redecan\",\"safe\",\"san\",\"sense\",\"shit\",\"sorted\",\"sour\",\"south\",\"started\",\"statements\",\"sticking\",\"tide\",\"tobacco\",\"top\",\"trouble\",\"ultra\",\"vertical\",\"walk\",\"websites\",\"2gs\",\"3.5\",\"admits\",\"advice\",\"age\",\"albertacannabis.org\",\"americans\",\"answer\",\"aphria\",\"apparently\",\"apply\",\"arrive\",\"banned\",\"billing\",\"bong\",\"brands\",\"bulk\",\"campaign\",\"cannafarms\",\"catherine\",\"cbp\",\"centre\",\"charge\",\"christmas\",\"cities\",\"comment\",\"comments\",\"concern\",\"concerned\",\"continue\",\"cops\",\"correct\",\"crazy\",\"criminal\",\"current\",\"damn\",\"deal\",\"dealer\",\"deliver\",\"demographic\",\"destroy\",\"dry\",\"dude\",\"due\",\"earlier\",\"easier\",\"eat\",\"expect\",\"fairly\",\"fear\",\"federal\",\"figured\",\"fixed\",\"food\",\"forgot\",\"fresh\",\"friday\",\"frustrating\",\"fucked\",\"grams\",\"grown\",\"guards\",\"guessing\",\"happen\",\"happening\",\"hard\",\"heads\",\"heard\",\"heavy\",\"hit\",\"hold\",\"holds\",\"hoping\",\"impressed\",\"included\",\"isnâ€™t\",\"job\",\"keeping\",\"kids\",\"laid\",\"lcbo\",\"leafs\",\"level\",\"license\",\"licenses\",\"light\",\"linked\",\"literally\",\"lp's\",\"marketing\",\"matter\",\"meant\",\"medicine\",\"mind\",\"missed\",\"monday\",\"month\",\"mortar\",\"notification\",\"nova\",\"nugs\",\"office\",\"outrageous\",\"overpriced\",\"packaged\",\"password\",\"patriot\",\"pen\",\"permit\",\"personally\",\"picked\",\"pics\",\"pipe\",\"plane\",\"plant\",\"posts\",\"probable\",\"producer\",\"psychedelic\",\"quebec\",\"real\",\"reasons\",\"received\",\"recommend\",\"recommendations\",\"records\",\"reddit's\",\"registered\",\"related\",\"rolled\",\"royal\",\"run\",\"sale\",\"scale\",\"servers\",\"shadows\",\"shot\",\"sit\",\"slip\",\"snoop\",\"social\",\"solei\",\"spam\",\"steal\",\"stick\",\"supply\",\"supposed\",\"talking\",\"telling\",\"terrible\",\"thereâ€™s\",\"theyâ€™re\",\"tilray\",\"toronto\",\"tracking\",\"trip\",\"u.s\",\"updated\",\"version\",\"white\",\"wonâ€™t\",\"word\",\"workers\",\"wouldnâ€™t\",\"yep\",\"18th\",\"20am\",\"25th\",\"31.99\",\"4th\",\"9.24\",\"9.95\",\"abcann\",\"absurd\",\"account\",\"add\",\"added\",\"adding\",\"afraid\",\"aglc.ca\",\"airports\",\"alta\",\"amazed\",\"amounts\",\"amsterdam\",\"anymore\",\"apps\",\"article\",\"ass\",\"avoid\",\"aware\",\"awesome\",\"balance\",\"basically\",\"basis\",\"beans\",\"bed\",\"beleave\",\"birthday\",\"block\",\"blvd\",\"boat\",\"booze\",\"bottle\",\"boutique\",\"breaks\",\"brick\",\"bright\",\"bring\",\"bro\",\"broadway\",\"btw\",\"buddy\",\"bunch\",\"burns\",\"busy\",\"bylaw\",\"cancel\",\"canna\",\"canntrust\",\"cap\",\"capsules\",\"car\",\"carry\",\"cash\",\"casual\",\"casuals\",\"caught\",\"chance\",\"charges\",\"cheaper\",\"checked\",\"checkout\",\"cheers\",\"choices\",\"chopping\",\"claim\",\"clamping\",\"clones\",\"closer\",\"closing\",\"coast\",\"commerce\",\"companies\",\"competition\",\"competitive\",\"confident\",\"confirm\",\"congratulations\",\"consume\",\"consumption\",\"couch\",\"countryside\",\"couple\",\"court.happy\",\"cove\",\"crack\",\"crashed\",\"created\",\"cronos\",\"crossing\",\"dads\",\"danksgiving\",\"date\",\"deals\",\"decent\",\"demand\",\"didnâ€™t\",\"digits\",\"disappear\",\"dna\",\"dollar\",\"dollars\",\"dream\",\"drink\",\"dumb\",\"dunno\",\"east\",\"easy\",\"eaton\",\"edible\",\"edmonton\",\"effect\",\"eighth\",\"eighths\",\"emblem\",\"empty\",\"encrypted\",\"ents\",\"error\",\"evening\",\"exchange\",\"excited\",\"exciting\",\"existing\",\"explicitly\",\"export\",\"eye\",\"facilitate\",\"failed\",\"fair\",\"family\",\"fast\",\"fees\",\"feet\",\"field\",\"figure\",\"fire\",\"fireside\",\"flexdelivery\",\"footer\",\"foreign\",\"fort\",\"front\",\"funny\",\"fyi\",\"game\",\"genetics\",\"giving\",\"goal\",\"god\",\"gotta\",\"govt\",\"grabbing\",\"grove\",\"gst\",\"hahaha\",\"hands\",\"harvest\",\"haven\",\"havenâ€™t\",\"hexo\",\"highly\",\"hits\",\"honest\",\"household\",\"huge\",\"hydropothecary\",\"iâ€™ve\",\"ice\",\"ideally\",\"idiots\",\"images\",\"imagine\",\"imgur.com\",\"import\",\"individual\",\"insulted\",\"interim\",\"internet\",\"involved\",\"irisa\",\"jars\",\"joint\",\"joints\",\"kinda\",\"knowing\",\"label\",\"landed\",\"landing\",\"lands\",\"larger\",\"lazy\",\"lbs\",\"lead\",\"learn\",\"leave\",\"legalized\",\"legislation\",\"licensed\",\"life\",\"lights\",\"liiv\",\"lined\",\"lines\",\"lineup\",\"list\",\"load\",\"loaded\",\"logo\",\"lower\",\"lowest\",\"lps\",\"lungs\",\"macleod\",\"markham\",\"massive\",\"meantime\",\"mention\",\"midnight\",\"mix\",\"model\",\"mom\",\"mynslc.com\",\"nb.com\",\"nervous\",\"newstrike\",\"nope\",\"normal\",\"notice\",\"notices\",\"nslc\",\"oaks\",\"october\",\"offense\",\"offer\",\"offering\",\"opportunity\",\"ounce\",\"oven\",\"packed\",\"pardoning\",\"patrol\",\"personal\",\"physical\",\"planned\",\"planning\",\"plastic\",\"poor\",\"possession\",\"pound\",\"pour\",\"press\",\"priced\",\"prime\",\"private\",\"produce\",\"proper\",\"protected\",\"purchases\",\"purchasing\",\"purulator\",\"putting\",\"quantities\",\"quarters\",\"quÃ©bec\",\"queue\",\"railway\",\"ran\",\"randylaheyjr\",\"reasonable\",\"receiving\",\"rediquette\",\"refuse\",\"registration\",\"regulations\",\"report\",\"reporting\",\"reserve\",\"response\",\"rest\",\"restrictions\",\"retailer\",\"ripped\",\"road\",\"rolls\",\"rosin\",\"rue\",\"rule\",\"sask\",\"school\",\"searching\",\"sells\",\"separate\",\"server\",\"shake\",\"shark\",\"shelf\",\"shift\",\"ships\",\"shitty\",\"shock\",\"shoppers\",\"shopping\",\"shops\",\"shrink\",\"shut\",\"sign\",\"sites\",\"sitting\",\"slightly\",\"smoker\",\"smokes\",\"soil\",\"source\",\"sovereignty\",\"spend\",\"sprays\",\"spruce\",\"sqdc\",\"starseed\",\"sticky\",\"stomach\",\"stoned\",\"stop\",\"stored\",\"story\",\"strategy\",\"strawberry\",\"strong\",\"subs\",\"substance\",\"subtle\",\"successfully\",\"suddenly\",\"suffering\",\"suppose\",\"symbl\",\"system\",\"taking\",\"target\",\"tax\",\"term\",\"terms\",\"terrascend\",\"that'd\",\"ton\",\"tons\",\"tourist\",\"town\",\"trades\",\"traffic\",\"trail\",\"transporting\",\"true\",\"tuesday\",\"united\",\"users\",\"usual\",\"valley\",\"vaping\",\"vapor\",\"verification\",\"vie\",\"violates\",\"visited\",\"visiting\",\"wake\",\"warehouse\",\"warning\",\"warrant\",\"watch\",\"weâ€™re\",\"wednesday\",\"weird\",\"west\",\"whoâ€™ve\",\"wide\",\"wipe\",\"woodstock\",\"world\",\"worry\",\"www.cannabis\",\"yorkton\",\"youre\",\"0.01\",\"0.5g\",\"01am\",\"0a2moosomin\",\"0e0estevan\",\"0v2martensville\",\"1.4136368\",\"1.4867420\",\"100,000\",\"11pm\",\"15am\",\"17some\",\"17th\",\"195c\",\"1ec5c87b\",\"1st\",\"200mg\",\"20celebrate\",\"20pm\",\"21st\",\"262.36\",\"27am\",\"27th\",\"2pm\",\"3.44g\",\"3.50\",\"3.5gs\",\"30,000\",\"30am\",\"30g\",\"39.99\",\"3n0wiid\",\"4.20\",\"4.5\",\"4.85\",\"421a\",\"48.95\",\"49.95\",\"5.50\",\"5.5g\",\"50,000\",\"50k\",\"56.50\",\"56amontariohttps\",\"5mg\",\"6.95\",\"62.99\",\"6am\",\"6pm\",\"7.5g\",\"7.95\",\"77.63\",\"78.70\",\"8th\",\"8ths\",\"9p9ens\",\"aaaa\",\"aah\",\"about.they\",\"abroad\",\"absolutely\",\"absorbed\",\"abuser\",\"accept\",\"acceptable\",\"accepted\",\"access.and\",\"accessories\",\"ace\",\"acmpr\",\"acquire\",\"acquired\",\"act.pot\",\"activation\",\"activities\",\"acts\",\"adam\",\"addict\",\"addressed\",\"adds\",\"admissible\",\"admission\",\"adult\",\"advance\",\"advertising\",\"afaik\",\"agents\",\"aglc\",\"agree\",\"agreed\",\"ahhh\",\"airline\",\"airplane\",\"airspace\",\"airtight\",\"albertacannabis\",\"albertacannabis.orghere\",\"albertahttps\",\"albertfire\",\"alien\",\"alive\",\"amazing.both\",\"amazing.i\",\"amazon\",\"america\",\"america's\",\"analysis\",\"angles\",\"angyfox13\",\"announced\",\"annoying\",\"answered\",\"apologies\",\"appeal\",\"applies\",\"appreciated\",\"approach\",\"approved\",\"area.edit\",\"arenâ€™t\",\"arrange\",\"arriving\",\"arrogant\",\"ash\",\"assessment\",\"asshats\",\"assumed\",\"assuming\",\"assure\",\"athabasca\",\"atleast\",\"atm\",\"attempt\",\"attitude\",\"attracting\",\"august\",\"aussie\",\"australian\",\"authorized\",\"ave\",\"average\",\"awake\",\"aws\",\"b8015\",\"babies\",\"backlogged\",\"backwards\",\"bag\",\"bake\",\"baked\",\"banks\",\"barter\",\"base\",\"bastards\",\"batteries\",\"battleford\",\"bcuz\",\"beatles\",\"beer\",\"beginner.weedmd\",\"bellerose\",\"bench\",\"benefit\",\"benson\",\"bet\",\"bias\",\"bickel\",\"biohazard\",\"biosector\",\"bit.i\",\"bits\",\"blaze\",\"blend\",\"board\",\"booo\",\"booooo\",\"boost\",\"bootleggers\",\"bore\",\"bored\",\"born\",\"bother\",\"boulevard\",\"box\",\"boxes\",\"brain\",\"brand\",\"brandace\",\"branding\",\"brandno\",\"breaking\",\"breeders\",\"bringing\",\"brings\",\"brother\",\"browse\",\"browser\",\"brunswick\",\"brunswickhttps\",\"brutal\",\"bubbler\",\"bucks\",\"buisness\",\"bullshit\",\"bummed\",\"bureaucrat\",\"buried\",\"businesses\",\"butter\",\"buyer\",\"bylaws\",\"c45\",\"cache\",\"cake\",\"calgarynova\",\"calgarysmall\",\"caller\",\"calling\",\"calls\",\"canabis\",\"canadagrows\",\"canadas\",\"canadianmoms\",\"canadians\",\"canadients\",\"cancellation\",\"cancelled\",\"cancelling\",\"cannabisocs\",\"cannabutter\",\"cannaflower\",\"caps\",\"card.edit\",\"card.so\",\"cardholder\",\"care\",\"cart\",\"catching\",\"catsa\",\"cause.you\",\"caused\",\"centennial\",\"cents\",\"cerebral\",\"challenging\",\"chamber\",\"charged\",\"charging\",\"charlottetown\",\"chatted\",\"cheap\",\"checking\",\"checks\",\"child\",\"childproof\",\"children\",\"china\",\"choice\",\"chose\",\"cigarette\",\"circulated\",\"citation\",\"cite\",\"citizen\",\"citizen.i\",\"citizens\",\"city\",\"clandestine\",\"clarify\",\"clearing\",\"click\",\"clinic\",\"clone\",\"close\",\"closet\",\"clue\",\"coconut\",\"coffeeshops\",\"coils\",\"cold\",\"colors\",\"columbia\",\"columbiaonline\",\"comanies\",\"comfortable\",\"comments.hope\",\"comments.thanks\",\"committed\",\"committing\",\"common\",\"communication\",\"community\",\"compare\",\"competitively\",\"competitors.the\",\"complaining\",\"complicated\",\"complies\",\"complies.no\",\"concierge\",\"concrete\",\"condo\",\"confirmation\",\"confusing\",\"congrats\",\"connect\",\"connection\",\"conspiracy\",\"constitute\",\"construction\",\"consultandgrow.cathey\",\"consumers\",\"contact\",\"containers\",\"continent\",\"continuing\",\"controlled\",\"convenient\",\"convert\",\"convicted\",\"cop\",\"corner\",\"corp\",\"corporations\",\"couldnâ€™t\",\"countdowns\",\"countries.any\",\"courtroom\",\"covering\",\"covers\",\"crappy\",\"crash\",\"create\",\"creative\",\"crept\",\"criminally\",\"crippling\",\"critical\",\"cross\",\"crosses\",\"crossing.jeez\",\"crossings\",\"crotch\",\"crowded\",\"crown\",\"curious\",\"customers\",\"customs\",\"d'octobre\",\"d290\",\"dab\",\"dad\",\"daily\",\"dates\",\"day.everyone\",\"day.i\",\"daysdidn't\",\"dead\",\"decarb\",\"decarbed\",\"deceiving\",\"decide\",\"decided\",\"decides\",\"declining\",\"deemed\",\"deep\",\"deeper\",\"def\",\"default.here\",\"defined\",\"defitnely\",\"dehydrate\",\"delahaze\",\"delay\",\"delays\",\"delivered\",\"deliveries\",\"delivers\",\"density\",\"dent\",\"deny\",\"depending\",\"depts\",\"describe\",\"deserve\",\"desired.all\",\"desk\",\"detailed\",\"details\",\"determine\",\"determined\",\"device\",\"devonwaldo's\",\"dick\",\"didnt\",\"difficult\",\"dig\",\"digest\",\"diligent\",\"dipping\",\"direct\",\"directly\",\"disappeared\",\"disappears\",\"disappointed\",\"disappointing\",\"disapproving\",\"discord\",\"discount\",\"discovered\",\"discrete\",\"discuss\",\"discussion\",\"dislike\",\"dispensaries\",\"dispensary\",\"dispensery\",\"disproportionately\",\"dissapointed\",\"distalite.the\",\"distillate\",\"doable\",\"doe\",\"dogged\",\"dominant\",\"doob\",\"dosing\",\"doug\",\"downtown\",\"downvote\",\"downvoted\",\"downvotes\",\"downvoting\",\"dozen\",\"drag\",\"draw\",\"dreamed\",\"dreams\",\"dried\",\"driven\",\"driver\",\"drop\",\"drops\",\"drove\",\"drug\",\"drugs\",\"dum\",\"dynavap\",\"eastnewfoundland\",\"editor\",\"edmonton420\",\"edmontonalternative\",\"edmontoncannabis\",\"edmontonfire\",\"edmontonnova\",\"edmontonwestside\",\"educated\",\"education\",\"elements\",\"emails\",\"employee\",\"energetic\",\"enforceable\",\"enforcing\",\"enjoyable\",\"enter\",\"enters\",\"epic\",\"essential\",\"est\",\"eventually\",\"evidence\",\"excuse\",\"exist\",\"expectations\",\"expensive\",\"expensive.money\",\"experience:order\",\"experience:packaging\",\"experience.website:great\",\"experienced\",\"experiences\",\"experienceunable\",\"expert\",\"expired\",\"explain\",\"explained\",\"extra\",\"extremely\",\"facilitating\",\"factors\",\"fall\",\"familiar\",\"faq\",\"farmers\",\"fat\",\"favorites\",\"favourite\",\"favourites\",\"federally\",\"feds\",\"feedback\",\"feeling\",\"feels\",\"fields\",\"fight\",\"figr\",\"fill\",\"filling\",\"filtered\",\"finagling\",\"finally\",\"financial\",\"finished\",\"firecrackers\",\"flagging\",\"flat\",\"flawed\",\"flex\",\"flight\",\"floods\",\"flowers\",\"flu\",\"fluffy\",\"fly\",\"flying\",\"follow\",\"fool\",\"football\",\"forced\",\"ford\",\"fortunate\",\"forward\",\"fredericton\",\"freedom.i\",\"fri\",\"frostiness\",\"fucked.shipping\",\"fuckers\",\"fucking\",\"fuss\",\"future\",\"gagner\",\"games\",\"generalize\",\"giant\",\"girlfriend\",\"glad\",\"glass\",\"glut\",\"godâ€™s\",\"good.otherwise\",\"got.also\",\"govâ€™t\",\"government's\",\"governmentâ€™s\",\"grab\",\"grade\",\"grail\",\"gramflat\",\"grand\",\"grandchildren\",\"granted\",\"grants\",\"grass\",\"grateful\",\"greenhouses\",\"greens\",\"grind\",\"grocery\",\"ground\",\"grovenova\",\"grower\",\"grower.wasn't\",\"guaranteed\",\"guide\",\"gwill\",\"habit\",\"hah\",\"hairs\",\"halifax\",\"hammer\",\"hand\",\"handed\",\"handle\",\"handling\",\"handy\",\"happened\",\"hardcore\",\"harper\",\"harriman\",\"hassle\",\"hassles\",\"hatmanitobahttps\",\"hatnova\",\"hatnumo\",\"head\",\"headache\",\"headband\",\"headed\",\"hearing\",\"hearsay\",\"heavier\",\"hectic\",\"hell\",\"helloocs.ca\",\"helping\",\"here.either\",\"hereâ€™s\",\"heroin\",\"history\",\"holders\",\"holding\",\"holes\",\"homework\",\"honestly\",\"honey\",\"hops\",\"hotels\",\"hotter\",\"hour\",\"hubert\",\"hung\",\"hybrid\",\"hybridliiv\",\"hydro\",\"hydroponics\",\"iâ€™d\",\"iâ€™ll\",\"id'd\",\"idea\",\"ideal\",\"idiot\",\"idiotsweaksauce\",\"illegally\",\"imgur\",\"immediately\",\"improve\",\"improved\",\"inadmissible\",\"inadmissible.link\",\"incentive\",\"include\",\"incognito\",\"income\",\"increase\",\"increases\",\"indicas\",\"info:canada\",\"information.edit\",\"informationhttps\",\"informed\",\"ingest\",\"ingested\",\"insane\",\"inside\",\"insight\",\"inspection\",\"inspired\",\"intense\",\"intention\",\"intentions\",\"interacting\",\"interchangeably\",\"interior\",\"international\",\"interview\",\"investigated\",\"investigation\",\"invisible\",\"iron\",\"island\",\"issue.i\",\"issued\",\"issuing\",\"it.as\",\"jack\",\"jar\",\"jean\",\"jesus\",\"jet\",\"jill\",\"jive\",\"jnkrbwdk\",\"jobs\",\"joke\",\"joking\",\"journey\",\"judge\",\"jurisdiction\",\"justly\",\"kamloops\",\"kensington\",\"kick\",\"kidding\",\"kiefy\",\"kills\",\"kinky\",\"label.liiv\",\"labelling\",\"labour\",\"labradorhttp\",\"lack\",\"lady\",\"lagged.i\",\"land\",\"landrace\",\"largest\",\"late\",\"laughable\",\"laughter\",\"launch\",\"laying\",\"layout\",\"leading\",\"leaves\",\"leaving\",\"led\",\"leftthe\",\"legalities\",\"legalize\",\"legit\",\"lethbridge\",\"licence\",\"licensee\",\"licensing\",\"lifetime\",\"lift.co\",\"likes\",\"liking\",\"lil\",\"link\",\"links\",\"lipid\",\"liquor\",\"lists\",\"lmao\",\"lmfao\",\"loading\",\"locally\",\"located\",\"locking\",\"logic\",\"login\",\"lol.like\",\"lolâ€™d\",\"lolol\",\"lolwtf\",\"longtime\",\"looked\",\"lots\",\"lounges\",\"lucky\",\"lug\",\"luggage\",\"lumped\",\"lying\",\"macro\",\"magazine\",\"magento\",\"magneto\",\"mail\",\"majour\",\"mangled\",\"mango\",\"manitoba\",\"manitobans.saskatchewanjimmy's\",\"map\",\"mappeel\",\"maprosemont\",\"mapville\",\"marie\",\"maritimes\",\"marked\",\"marketconvenient\",\"marketplace\",\"master\",\"mastercards\",\"match\",\"match.edit\",\"mate\",\"material\",\"materials\",\"math\",\"me.all\",\"me.oh\",\"media\",\"medically\",\"memory\",\"mentioned\",\"mentioning\",\"meridian\",\"mess\",\"message\",\"messing\",\"metal\",\"method\",\"milled\",\"million\",\"min\",\"mines\",\"minimal\",\"minimum\",\"mins\",\"minute\",\"mirabel\",\"miss\",\"missing\",\"misunderstood\",\"mixed\",\"mmpr\",\"moc\",\"mode\",\"modest\",\"moist\",\"mom's.they\",\"moment.throw\",\"moms\",\"monopoly\",\"months\",\"months.thanks\",\"moochers\",\"more.edit\",\"mornings\",\"moron\",\"morons\",\"move\",\"movies\",\"multiple\",\"munchies\",\"municipal\",\"muted\",\"namao\",\"nationwide\",\"naturals\",\"nb's\",\"neat\",\"needed.edit\",\"negative\",\"neighbourhood\",\"nelson\",\"net\",\"network\",\"newfoundland\",\"newsroom\",\"nexus\",\"niagara\",\"nicer\",\"noobs\",\"noon\",\"nose\",\"note\",\"noticing\",\"novelty\",\"now.canada\",\"nudes\",\"nug\",\"numo\",\"nunavut\",\"nurse\",\"nutella\",\"nuts\",\"obtain\",\"of.i'll\",\"off.edit\",\"office.money\",\"oilâ€˜s\",\"omg\",\"ontarios\",\"oof\",\"optimistic\",\"option.some\",\"original\",\"orwch9u\",\"ouest\",\"out.edit\",\"outcome\",\"outsell\",\"outstanding\",\"outta\",\"owners\",\"packages\",\"paids\",\"pain\",\"paranoid\",\"parent\",\"part.all\",\"partake\",\"partakeoh\",\"parties\",\"pass\",\"pass.going\",\"passed\",\"passenger\",\"passes\",\"passwordbrick\",\"path\",\"patience\",\"patient\",\"paused\",\"peace\",\"pei\",\"pennies\",\"penny\",\"percentage\",\"perfect\",\"permission\",\"permits\",\"permitting\",\"pervasive\",\"phemonia\",\"photos\",\"pic\",\"pieces\",\"pill\",\"pills\",\"pinene\",\"pizza\",\"placing\",\"plaingreen\",\"plan\",\"planes\",\"plant.guess\",\"platform\",\"playing\",\"plenty\",\"plz\",\"pm.no\",\"political\",\"pooping\",\"pop\",\"portalocs\",\"posed\",\"possibility\",\"posted\",\"posting\",\"pot.students\",\"pot's\",\"potbyprovince.ca\",\"potent\",\"potential\",\"potentially\",\"poti\",\"potpriced\",\"pounds\",\"preclearance\",\"premier\",\"premium\",\"prepaids\",\"prepared\",\"prerolls\",\"president's\",\"pressing\",\"price.the\",\"pricing:i\",\"pricing.https\",\"primo\",\"prize\",\"process\",\"processed\",\"producers\",\"prohibition\",\"proliferation\",\"promise\",\"promised\",\"prongs\",\"proof\",\"properly\",\"prov\",\"provide\",\"provided\",\"provider\",\"providing\",\"provincefeel\",\"provincially\",\"pumped\",\"punch\",\"purely\",\"purolator\",\"quality.people\",\"quantity\",\"quebecers\",\"queues\",\"quickly\",\"rambling\",\"random\",\"rate\",\"rates\",\"ratio\",\"reasoned\",\"reasoning\",\"rebuy\",\"rec\",\"receipt\",\"reckon\",\"recommending\",\"redditors\",\"reduce\",\"referring\",\"refrain\",\"register\",\"regular\",\"regulation\",\"relating\",\"release\",\"released\",\"relevant\",\"remain\",\"remember\",\"remembered\",\"remove\",\"replies\",\"reply\",\"reporter\",\"represented\",\"represents\",\"requests\",\"residences\",\"resistance.obviously\",\"resource\",\"restaurant\",\"restock\",\"restocked\",\"restocking\",\"restocks\",\"restricting\",\"retail.battleford\",\"ridiculous\",\"rioted\",\"rips\",\"risk\",\"road.i\",\"robbed\",\"rough\",\"route\",\"rqjjbcr\",\"rugles\",\"rules\",\"rules.the\",\"running\",\"runs\",\"s.provide\",\"s0g\",\"s0k\",\"s0m\",\"s4a\",\"sad\",\"saint\",\"sainte\",\"sake\",\"same.maybe\",\"sample\",\"sampling\",\"saskatchewan\",\"saskatchewanfire\",\"saskatchewannova\",\"sat\",\"sativas\",\"save\",\"savings\",\"scan\",\"scandal\",\"scanning\",\"scarab138\",\"scare\",\"schedule\",\"scheer\",\"scotia\",\"scotiahttps\",\"screen\",\"screw\",\"screwing\",\"seal\",\"sealed\",\"searchable\",\"searches\",\"season\",\"security.product\",\"sedative\",\"sellare\",\"sensitive\",\"seriously.shipping\",\"servers.it\",\"session\",\"sessions.all\",\"set\",\"setting\",\"shame\",\"share\",\"shared\",\"shipping.catching\",\"shippingpre\",\"shishkaberry\",\"shocked\",\"shop.if\",\"shopcannabisnl.com\",\"shopify\",\"short\",\"shorted\",\"shots\",\"should've\",\"shown\",\"significant\",\"silly\",\"situations\",\"size\",\"sizes\",\"slowly\",\"slows\",\"smallish\",\"smart\",\"smashing\",\"smelled\",\"smith\",\"smokable\",\"smoke.the\",\"smoked.good\",\"smooth\",\"snapped\",\"soften\",\"solar\",\"solei's\",\"solid\",\"solve\",\"solventless\",\"someday\",\"sooooooooooooooo\",\"sound\",\"southfort\",\"southpointe\",\"space\",\"speeches\",\"spending\",\"spoiled\",\"spoke\",\"spoken\",\"spoonful\",\"spouse\",\"stability\",\"staff\",\"stag\",\"standards\",\"starting\",\"starts\",\"stash\",\"stated\",\"states.i\",\"stay\",\"step\",\"stepping\",\"stern\",\"stigma\",\"stinky\",\"stocked\",\"stoked\",\"stone\",\"stoners\",\"stony\",\"storefront\",\"stories\",\"storz\",\"straightened\",\"streams\",\"streets\",\"strict\",\"strikes\",\"struggling\",\"stuck\",\"stuffing\",\"subject\",\"subreddit\",\"substantially\",\"success.was\",\"successful\",\"sucks\",\"suggest\",\"suggesting.in\",\"suggestion\",\"suggestions\",\"suh\",\"sunday\",\"sunflower\",\"sunk\",\"suppliers\",\"support\",\"support_the_breeders_lp_strain_overview\",\"supporting\",\"surely\",\"surreal\",\"suspicion\",\"swallow\",\"swap\",\"sweat\",\"sweedy\",\"sweeps\",\"swim\",\"swimming\",\"system.i\",\"tactic\",\"tag\",\"talked\",\"talks\",\"taste\",\"tasty\",\"taxi\",\"tea\",\"tempered\",\"ten\",\"terpenes\",\"terpinolene\",\"terrorism\",\"that.no\",\"that.the\",\"thc:cbd\",\"them.police\",\"themed\",\"there.the\",\"thermometer\",\"theyâ€™ll\",\"theyâ€™ve\",\"thier\",\"this.make\",\"thread.british\",\"thrive\",\"thriving\",\"thrown\",\"thurs\",\"thursday\",\"thx\",\"ticket\",\"tight\",\"tilray's\",\"timer\",\"timezones\",\"tinctures\",\"tire\",\"today.fyi\",\"today's\",\"tolerance\",\"tonight\",\"tool\",\"toss\",\"total\",\"touch\",\"tough\",\"tourism\",\"tourists\",\"traceable\",\"tradeitforweed.club\",\"traveler\",\"tray\",\"trend\",\"trichomes\",\"tried.i\",\"tub\",\"turnaround\",\"tweed\",\"tweeds\",\"types\",\"uhm\",\"unable\",\"unavailable\",\"unbelievable\",\"uncertainty\",\"understand\",\"understanding\",\"understood\",\"unexpected.i\",\"unfold.can't\",\"unfortunately.edit\",\"unique\",\"unlicensed\",\"unrelated\",\"up.i'm\",\"updating\",\"uplifting\",\"upload\",\"ups.personally\",\"upset\",\"us.they\",\"usa\",\"usabilitypoor\",\"uscbp\",\"vancouver\",\"vancouver.holding\",\"vaporized\",\"vegging\",\"vehicle\",\"vehicle.out\",\"viable\",\"vicinity\",\"video\",\"view\",\"violate\",\"violation\",\"viscous\",\"visit\",\"voila\",\"volume\",\"volumes\",\"wack\",\"waited\",\"waking\",\"walking\",\"walmart\",\"war\",\"waste\",\"wax\",\"weâ€™ll\",\"weaknesses\",\"weary\",\"web\",\"website:a\",\"weds\",\"weed.police\",\"weed.then\",\"weed.you\",\"weedless\",\"weedmd's\",\"weekly\",\"weigh\",\"weighed\",\"whats\",\"whelp\",\"whiff\",\"whim\",\"white.the\",\"widow\",\"wife\",\"willow\",\"winnipeg\",\"witb\",\"wits\",\"woman\",\"wont\",\"worker\",\"worried\",\"worse\",\"would've\",\"wow\",\"wrong\",\"wrote\",\"wth\",\"wut\",\"www.albertacannabis.org\",\"www.bccannabisstores.com\",\"www.cbc.ca\",\"www.cbp.gov\",\"www.cp24\",\"www.delta9\",\"www.reddit.com\",\"www.shopcannabisnl.comnew\",\"www.sqdc.ca\",\"yah\",\"yea\",\"yesterday's\",\"yet.edit\",\"yetquebecmontreal\",\"yikes\",\"you.i\",\"you.take\",\"youâ€™ll\",\"yup\",\"zip\",\"zombiejesus1987no\",\"zone\",\"zoom\",\"zzz\",\"à² _à² \"],\"freq\":[52,42,40,38,35,30,29,26,25,24,24,22,20,20,20,20,19,19,18,17,17,17,17,17,16,16,16,16,15,15,15,14,14,14,14,13,13,13,13,13,12,12,12,12,12,12,12,12,11,11,11,11,11,11,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"green\",\"minSize\":0,\"weightFactor\":3.46153846153846,\"backgroundColor\":\"black\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":[]} While words like cannabis, legal, weed, etc. would be expected to appear in the discussion, there are other frequent words which may arouse your interest. For example, there are a lot of words associated with online shopping. Among them, ocs (Ontario Cannabis Store), shipping, buy, visa, credit, debit, and card appear most frequently. Such observations suggest that a substantial number of comments in the discussion is about the cannabis business.\nLetâ€™s examine some of the comments with those - cannabis business related - words!\ntidy_data %\u0026gt;% filter(!is.na(likes),word %in% c(\u0026quot;ocs\u0026quot;, \u0026quot;shipping\u0026quot;, \u0026quot;credit\u0026quot;, \u0026quot;store\u0026quot;, \u0026quot;visa\u0026quot;, \u0026quot;debit\u0026quot;,\u0026quot;buy\u0026quot;))%\u0026gt;% group_by(id)%\u0026gt;% summarize(n=n())%\u0026gt;% arrange(desc(n))%\u0026gt;% head(5) ## # A tibble: 5 x 2 ## id n ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 457 4 ## 2 147 3 ## 3 163 3 ## 4 211 3 ## 5 450 3    These comments confirm my hypothesis that those words are closely related to the cannabis business. In addition, the author of these comments are concerned about their privacy while making a purchase. It seems like, they are not certain about the privacy involved in buying cannabis with their credit cards.\nWho were the most frequent contributors to the discussion? In addition to a â€œtibbleâ€, as the output of dplyr functions, ggplot2 allows another visual inspection. The following bar plot exhibits the count of comments that the five most frequent contributors to this discussion have posted. However, [deleted] is rather for every unknown author than a specific one.\ndataset %\u0026gt;% group_by(author) %\u0026gt;% summarise(Comments=n())%\u0026gt;% arrange(desc(Comments))%\u0026gt;% mutate(author = reorder(author, Comments)) %\u0026gt;% head(5) %\u0026gt;% ggplot(aes(author,Comments))+ geom_col(show.legend = F)+ coord_flip()+ geom_text(aes(label = Comments))+ ggtitle(\u0026quot;The Most Frequently Contributing Authors\u0026quot;) Because I was assuming these contributors have a substantial impact on positive and negative sentiments, I have decided to focus on the words they posted.\nThe plot below outlines the specific and most frequently used words by their authors. Bear in mind these words come from nrc lexicon. Therefore, the words that were not contained in the lexicon were filtered out. However, the remaining words create the sentiments.\ndata_sentiment %\u0026gt;% # Count by word and author count(word,author) %\u0026gt;% # Group by author group_by(author) %\u0026gt;% # Take the top 10 words for each author top_n(10) %\u0026gt;% ungroup() %\u0026gt;% mutate(word = reorder(paste(word, author, sep = \u0026quot;__\u0026quot;), n)) %\u0026gt;% filter(author %in% c(\u0026quot;BioSector\u0026quot;, \u0026quot;ruglescdn\u0026quot;, \u0026quot;terrencemckenna\u0026quot;, \u0026quot;frowawe\u0026quot; )) %\u0026gt;% # Set up the plot with aes() ggplot(aes(word,n)) + geom_col(show.legend = FALSE) + scale_x_discrete(labels = function(x) gsub(\u0026quot;__.+$\u0026quot;, \u0026quot;\u0026quot;, x)) + facet_wrap(~ author, nrow = 2, scales = \u0026quot;free\u0026quot;) + coord_flip() The output of the most frequently used words posted by individuals may be interesting. But to what extent is it insightful? One should know the specific sentiment these words annotate. The following chart displays counts of all the words from the previous scheme. Most importantly, these words are categorized within the sentiments from nrc lexicon.\ndata_sentiment %\u0026gt;% # Count by word and author group_by(word,author)%\u0026gt;% mutate(n=n())%\u0026gt;% # Group by author filter(author %in% c(\u0026quot;BioSector\u0026quot;, \u0026quot;ruglescdn\u0026quot;, \u0026quot;terrencemckenna\u0026quot;, \u0026quot;frowawe\u0026quot; ))%\u0026gt;% group_by(author) %\u0026gt;% top_n(30) %\u0026gt;% ungroup() %\u0026gt;% # Set up the plot with aes() ggplot(aes(word,n)) + geom_col(show.legend = FALSE) + facet_wrap(~ sentiment, scales = \u0026quot;free\u0026quot;) + coord_flip()+ ylab(\u0026quot;Count\u0026quot;) You can see that BioSector, ruglescdn, terrencemckenna, and frowawe express in a number of ways. Even though they do not express disgust or surprise too often, I could not determine what is their most common sentiment. Since a table is not as space demanding as any rigorous plot, the one mentioned below summarizes and counts sentiment related words by their authors.\ndata_sentiment %\u0026gt;% filter(author %in% c(\u0026quot;BioSector\u0026quot;, \u0026quot;ruglescdn\u0026quot;, \u0026quot;terrencemckenna\u0026quot;, \u0026quot;frowawe\u0026quot; ))%\u0026gt;% group_by(sentiment) %\u0026gt;% summarise(count=n())%\u0026gt;% arrange(desc(count)) ## # A tibble: 10 x 2 ## sentiment count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 positive 122 ## 2 trust 83 ## 3 negative 77 ## 4 anticipation 70 ## 5 fear 45 ## 6 sadness 35 ## 7 anger 32 ## 8 joy 30 ## 9 surprise 26 ## 10 disgust 20 Amazing! BioSector, ruglescdn, terrencemckenna, and frowawe create mostly positive sentiment!\nMy next goal was to allocate the most positive and negative authors. For this purpose, I created a new variable percent. count() groups author, sentiment, and author_total . Then, it creates a new variable n, and calls ungroup() afterwards. Consequently, n is nothing but the number of words by author within a particular sentiment.\n# Which authors use the most negative words? data_sentiment %\u0026gt;% count(author, sentiment, author_total) %\u0026gt;% # Define a new column percent mutate(percent=n/author_total) %\u0026gt;% # Filter only for negative words filter(sentiment %in% c(\u0026quot;negative\u0026quot;,\u0026quot;positive\u0026quot;)) %\u0026gt;% # Arrange by percent arrange(desc(percent))%\u0026gt;% group_by(sentiment)%\u0026gt;% top_n(5) %\u0026gt;% ungroup()%\u0026gt;% ggplot(aes(author,percent)) + geom_col(show.legend = FALSE) + facet_wrap(~ sentiment, nrow = 2, scales = \u0026quot;free\u0026quot;) + coord_flip() The plot above discloses the information of authors, responsible for the greater portion of positive and negative comments. There are two things worthy of mention: i.) the two sets do not intersect so these authors are not ambiguous. ii.) there appears to be â€œno authorâ€ from the group of most frequent contributors.\nI also assumed that BioSector, ruglescdn, terrencemckenna, and frowawe are the most popular ones or at least have posted the most popular comments.\n#the Most Popular Comments dataset %\u0026gt;% select(id,author,likes)%\u0026gt;% arrange(desc(likes))%\u0026gt;% head(5) ## id author likes ## 1 436 [deleted] 20 ## 2 444 captain_deadfoot 19 ## 3 265 AgentChimendez 18 ## 4 172 [deleted] 16 ## 5 184 jamagut 15 To my surprise, no one appears in these two groups simultaneously.\nTo have an idea about what drives the participants to like someoneâ€™s comment, one should read them! Below are a few screenshots of the top 3 ranked comments: SirWinnipegge complains about new prices in the local dispensary. Has he been buying weed on the black market for lower prices? If so, are the other cannabis users having the same experience? Then is legalizing marihuana an effective strategy to uproot black market cannabis trade? On the other hand, AgentChimendez celebrates the date: 10/17/2018 as the new milestone in Canadian history. He points out to the 20th of April, the World Cannabis Day, mostly referred to as 420. Lastly, captain_deadfootâ€™s comment was liked because he reacted to someone else outside of Canada. He emphasizes how friendly Canadian black market prices are in contrast to other countries. Again, it seems like the black market has already been supplying cannabis for a friendly price. Should the government introduce price ceiling to effectively compete with the black market?\n#Least Popular Comments dataset %\u0026gt;% select(id, author,likes) %\u0026gt;% arrange(likes) %\u0026gt;% head(5) ## id author likes ## 1 297 dubs2112 -10 ## 2 351 Justos -5 ## 3 25 Hendrix194 -2 ## 4 60 trekrlife -2 ## 5 348 Happyradish532 -2 I particularly enjoyed dubs2112â€™s comments! He/she was constantly bullied (disliked) by others in the discussion because he/she basically calls people stupid for buying legal cannabis products.\nThe other posts with dislikes appear in the same thread. Happyradish532 and Justos celebrate the shock that the black market might incur. They seem comfortable with paying more for legal cannabis since it is supposed to weaken the black market.\nFinally, letâ€™s see how the sentiments - positive and negative - were developing during the first few weeks. This step was the most difficult one. First of all, I created a new data set sentiment_by_time from tidy_data. Since the time span for which the discussion was truly alive is really short, I rounded date to the unit of days and called the new variable date_floor. After grouping by date_floor, I created another new variable total_words, i.e.Â the number of total words per day. Again, I merged the data with nrc lexicon.\nIn the next step, I filtered the positive and negative sentiments in sentiment_by_time. Most importantly, I counted entries of the group: date_floor, sentiment, and total_words, and created new variable percent which is the ratio of counts per group and total words submitted in a day.\nThe purpose of this plot is to demonstrate the long-term development of positive vs.Â negative sentiment. Additionally, method = \u0026quot;lm\u0026quot; regresses percent on time and plots slopes of the development curves. Unfortunately, the time span in my model is very short and I do not yet mak any conlusions.\nsentiment_by_time \u0026lt;- tidy_data %\u0026gt;% # Define a new column using floor_date() mutate(date_floor = floor_date(date, unit = \u0026quot;1 day\u0026quot;)) %\u0026gt;% # Group by date_floor group_by(date_floor) %\u0026gt;% mutate(total_words = n()) %\u0026gt;% ungroup() %\u0026gt;% # Implement sentiment analysis using the NRC lexicon inner_join(get_sentiments(\u0026quot;nrc\u0026quot;), by=\u0026quot;word\u0026quot;) sentiment_by_time %\u0026gt;% # Filter for positive and negative words filter(sentiment %in% c(\u0026quot;positive\u0026quot;,\u0026quot;negative\u0026quot;)) %\u0026gt;% filter(date_floor \u0026lt; ymd(\u0026quot;2018-11-10\u0026quot;)) %\u0026gt;% # Count by date, sentiment, and total_words count(date_floor, sentiment, total_words) %\u0026gt;% ungroup() %\u0026gt;% mutate(percent = n / total_words) %\u0026gt;% # Set up the plot with aes() ggplot(aes(date_floor,percent,col=sentiment)) + geom_line(size = 1.5) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE, lty = 2) + expand_limits(y = 0)+ ggtitle(\u0026quot;Sentiment Over Time\u0026quot;) Nevertheless, it seems like positive and negative sentiments exhibit the same mild but positive slopes. Secondly, positive sentiment has much greater variation. Finally, both sentiments achieve their peaks shortly after October 22nd and decline later.\nTo see what stands behind the steep rise of positive sentiment, I found a few explanatory comments.\nsentiment_by_time %\u0026gt;% filter(sentiment %in% c(\u0026quot;positive\u0026quot;,\u0026quot;negative\u0026quot;))%\u0026gt;% filter( ymd(\u0026quot;2018-10-22\u0026quot;) \u0026lt; date_floor \u0026amp; date_floor \u0026lt; ymd(\u0026quot;2018-10-29\u0026quot;)) %\u0026gt;% group_by(date_floor, sentiment, total_words) %\u0026gt;% mutate(n=n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(percent = n / total_words) %\u0026gt;% group_by(id)%\u0026gt;% mutate(max=max(percent)) %\u0026gt;% arrange(desc(max)) ## # A tibble: 9 x 11 ## # Groups: id [5] ## author likes date id word date_floor ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; ## 1 Interâ€¦ 0 2018-10-25 04:40:29 13 quesâ€¦ 2018-10-25 00:00:00 ## 2 Interâ€¦ 0 2018-10-25 04:40:29 13 word 2018-10-25 00:00:00 ## 3 Interâ€¦ 0 2018-10-25 04:40:29 13 legal 2018-10-25 00:00:00 ## 4 Interâ€¦ 0 2018-10-25 04:40:29 13 illeâ€¦ 2018-10-25 00:00:00 ## 5 Diablâ€¦ 1 2018-10-27 23:24:02 6 child 2018-10-27 00:00:00 ## 6 h3xadâ€¦ 2 2018-10-23 04:59:38 7 legal 2018-10-23 00:00:00 ## 7 h3xadâ€¦ 2 2018-10-23 04:59:38 7 money 2018-10-23 00:00:00 ## 8 Alypiâ€¦ 0 2018-10-23 12:32:16 14 legal 2018-10-23 00:00:00 ## 9 Hendrâ€¦ 0 2018-10-23 00:01:04 20 eat 2018-10-23 00:00:00 ## # â€¦ with 5 more variables: total_words \u0026lt;int\u0026gt;, sentiment \u0026lt;chr\u0026gt;, n \u0026lt;int\u0026gt;, ## # percent \u0026lt;dbl\u0026gt;, max \u0026lt;dbl\u0026gt; First of all, we can see that the positive sentiment achieved its peak on October 25th and was caused by a post written by IntermolecularButter (comment id 13). Another important observation is the fact that even though the time span was 1 week, there were only 7 new comments during that time! Less comments stand for less number words, therefore mutate(percent = n / total_words) may fluctuate a lot. Here, the peak is caused by the fact that the denominator was higher before October 22nd, resulting in a lower value of percent.\nNonetheless, letâ€™s see what IntermolecularButter posted. It is not very insightful, is it? The main reason why this post creates a positive sentiment, in addition to the explanation of high variation, is because the twelve words from the comment are associated with positive sentiment! Only one word - illegal - is considered negative. On top of that, sentiment was joined to the data set by inner_join(). By default, the result is that the data frame filtered words that were not present in the lexicon. For example, I noticed the word â€œthanksâ€\u0026quot; is not in nrc lexicon even though its connotation is definitely positive.\n Conclusion The analysis covered three topics; i.) web scraping using rvest package, ii.) cannabis legalization in Canada, and iii.) sentiment analysis. First of all, rvest provides enough flexibility to extract an HTML code from static web page and is therefore ideal for web scraping with R. However, I would suggest using APIâ€™s whenever possible to ease your job. Secondly, the discussion on cannabis legalization was not alive as one would expect. It seems that concerns about cannabis usersâ€™ privacy were not clearly communicated from the government as is evident from the comments. Additionally, during the time of this discussion posting, the consumers seem to be concerned about the price of legal cannabis being higher than the black market pricing. Lastly, sentiment analysis may not be a reliable approach when it comes to analyzing new policies, especially when something was decriminalized. One can see that more than legalization itself, the cannabis community was rather concerned about prices and privacy.\n  ","date":1553644800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553644800,"objectID":"c242b0051d435f9669a2c838218eb6e1","permalink":"https://jiristodulka.com/%60/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/","publishdate":"2019-03-27T00:00:00Z","relpermalink":"/`/post/web-scraping/web-scraping-reddit-text-sentiment-analysis/","section":"post","summary":"Canada recently joined few other countries that have completely decriminalized cannabis consumption, hence making it entirely legal. While many argued the Country has infringed a number of international agreements, others sang chorales. Whatever your attitude towards legalization of cannabis may be, there are one set of people that are definitely happy and they are the â€œCanadian Cannabis consumersâ€. However, only provincial, i.e.Â governmental, legal entities are allowed to sell and distribute marihuana products until April 2019.","tags":["NLP","R","rvest","sentimentanalysis","textmining","webscraping"],"title":"Web Scraping Reddit: Text (Sentiment) Analysis","type":"post"}]