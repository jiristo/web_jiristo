Tue: 20th Aug 2019
Starting with DL which I want to implement for a recommender system. This time I will need to use external GPU. Will probably go for Azure or AWS. I am exploring codes for embeddings and ReLU, Leaky ReLU, Parametric ReLU, Exponential Linear (ELU, SELU) activation functions. The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive. As always, the challenge is to find the starting point. Figured it out it would be best to start with fast.ai


Wed: 21st Aug 2019
I am going for AWS because of its popularity (motivation: industry expectation). I regret it... I lost all my day while trying to figure out everything. I closed AWS documentation and opened Deep Learning. I sweared I will read the entire book chapter by chapter. Application is what does matter but first I need to build on a solid "skeleton", i.e. theory. I learned about different distribution functions and their application. E.g. mass and marginal distribution functions.


Thu: 22nd Aug 2019
Called with a colleague ,Mirek. Mirek uses Azur, he showed me to run it. Pretty simple compared to AWS. Anyway, Mirek has built a TS model (with ARIMAX) predicting gold price. He is trying to challange his model with DL; specifically LSTM. We discussed parameters like batch size and number of samples. Quite exciting! I've once  read the best performance on TS data is usually achieved with combination of TS and DL models. On top of that, Mirek shared with me Bokeh library for dashboards. He says it's simpler than Pyviz. I need to give it a try...

Fri: 23rd Aug 2019
I am geting back to DataCamp. I've figured out that following relevant tracks (e.g. DL for now) with combination of reading articles and watching videos is one of the most effective learning paths when the starting point and direction are unknown. I also connected and had a call with two other data scientist. Daniel's from Ottawa, has a PhD in Computer Sciences. It turned out he knows the same folks in Toronto, mostly through following AISC. He knows Amir and Toronto ML Channel on Slack. On top of that, he is an expert in DL. So great connection! Diven is Omar's apprentice as I am. He works on the same recsys utilizing Surprise and is moving to Toronto soon. We agreed on some strategy how to start building DL recsys together. Finally, I bought the ticket for reinforcement learning workshop. Reinforcement deep learning recommender system?! Hell, I am going for that!


Tue: 27th Aug 2019
Activation function: Inputs (variables or outputs of neurons) get weights. If the **dot product** of these weights is higher than certain value, the activation function simulata if the neuros fire or not. Activation function is close to a sigmoid function (e.g. square root function), so the output is between 0 and 1. **Activation function captures non-linearities**. Classification: Dog (0.71) or cat(0.29), i.e. 71% confidence the object is a dog. So why to use DL instead of linear regression? DL models allow interactions (no assumption of no multicollinearity). Cool thing about NNs is that I don't need to specify interactions. They are captured by the model itself. ReLu is actually very simple:

```
def relu(input):
    '''Defines ReLu activation function: Input node dot product'''
    # Calculate the value for the output of the relu function: output
    output = max(0, input)

    # Return the value just calculated
    return(output)
```
e.g.
```
print(relu(5))
#5
print(relu(-10))
#0
```

Deep networks internally build representations of patterns in the data. Partially replace the need for feature engineering. Subsequent layers build increasingly sophisticated
representations of raw data. Gradient descent is loss vs weight (I want to minimize the loss). Learned about back propagation. I need to recap it once (or probably several times) more. At least, I understand the analogy between a neural network  and brain neuron. It fires back and forward! DC has a great tutorial by Dan Becker (Data Scientist at at Google).


Wed: 28th Aug 2019
I started with Keras! Dense layer means all the nodes in the current layer connect to all the nodes in the previous layer. There may be even hundereds or thousand nodes in a layer (Tensorflow can take care of that).


Thu: 28th Aug 2019
Could not sleep because of a jetlag. So I started playing with compiling DL model. In binary classification I use softmax function for probability distribution. I am still continuing with Keras and plying with different parameters in fine-tuning my medel. I.e. for optimizer I use "adam" and loss function compute through "class entropy". I am actually thinking I should start using PyTorch, instead NumPy, for running computations on GPU and PySpark (as once Dwight Gunning from FINRA has recommended me while having a meeting).

Fri: 29th Aug 2019
Continuing with DL. Both theory and practice. When it comes to DL network: dense layers learn a **weight matrix**, where the **first dimension of the matrix** is the **dimension of the input data**, and the second dimension is the dimension of the output data.
Defining `tensor` output in one line
```
from keras.layers import Input, Dense
input_tensor = Input(shape=(1,))
output_tensor = Dense(1)(input_tensor)
```
When it comes to loss function, **mean absolute error** is a good general function for a *keras* model. It's less sensitive function to outliers. however, one can use `mse` which would be equivalent to OLS. In DL, $y^{\hat} = mx + b$ where $m$ is the weight of the dense layer and $b$ is the bias of the dense layer. Mean squared error is a common loss function and will optimize for predicting the mean, as is done in OLS. Mean absolute error optimizes for the median and is used in quantile regression.

Mon:  1st Sep 2019
Learned about **shared layers** allowed by Keras API. They allow me to define an operation and then apply the exact same operation, with the exact same weights on different inputs. Can by used for time-series as well.
I understand, defining multiple input layers for multiple entities (e.g. customer IDs) **allows me to specify** how the data from each entity will be used differently later in this model. Now, I see an applied case in UFC. In data with UFC result for every match and fighter, I can teach the model to learn strength of every single fighter. Such that if any pair of fighters plays each other, I could predict the score, even if those two fighters have never fought before.

Tue: 2nd Sep 2019
Keras models can have both trainable and fixed parameters. Fewer trainable parameters are less flexibel but at the same time less likely to overfit. Trainable parameters are usually in **dense** layers. Due to an embedding layer in the model (input) and dense layer (with e.g. 4 parameters), the model has much more trainable parameters. **Embedding layers often add a large number of trainable parameters into the model** (be cautious of overfitting). Embedding layer maps integers to floats: each unique value of of the embedding input gets a parameter for its output.
**Shared models** work the same way as shared layers, i.e. I can put together a sequence of layers to define a custom model. Then, it's possible to share the entire model in exactly the same way I would share a layer.
**Model stacking** is using model's prediction as input to another model. It's one of the most sophisticated way of combining models. When stacking it's important to use different datasets for each model.
It's also possible to create single model for classification and regression with Keras. In ordet to do that, I use `activation = "sigmoid"` in output layer. With two outputs model's each output requires its own loss function passed as a list (e.g. different loss functions for a classification and regression models). Optimzer, e.g. `"adam"`, can be passed only once for the both models.


Sun: 8th Sep 2019
Called with Omar yesterday. He will finish editing my post about matrix factorization recommender system in a week. I really need to publish it. It's been already three months from my last post. Definitely, I will follow up with smth. regarding DL. There is good repository on Wikipedia: **List of datasets for machine-learning research**. Now it's time to focus on "Deep Learning based Recommender System: A Survey and New Perspectives" by Zhang et al.
Wikipedia: RNN (recurrent neural network) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition[1] or speech recognition. Such network has over-performed standard recsys.


Fri: 13th Sep 2019
Returned to Toronto from Europe on Tuesday. I have been recently studying amazing review  of recent efforts on deep learning based recommender systems: **Deep Learning based Recommender System: A Surve and New Perspectives** published by Shuai Zhang et al. So why DL based recsys? Firstly, DL models are end-to-end differentiable, secondly they provide suitable inductive biases. Therefore, DL models can exploit inherent structures if there are any.
I particularly like MLP for its simplicity and capability to learn hierarchical features.
AE are oslo used in DL based recsys as unsupervised model reducing dimensionality in a similar manner as PCA. AE can be used both for for item-based and user-based models.

AE is also powerful in learning feature representation, so it is possible to learn feature representations for user/item content features.
Another interesting type of AE is Collaborative Deep Learning (CDL). It is a hierarchical model Bayesian model with stacked denoising AE (SDEAE). CDL has two components: i.) perception component (deep neural net.: probabilistic interpretation of of ordinal SDAE ) and ii.) task-specific component (PMF).
Collaborative Deep Learning is a pairwise framework for top-n recommendations.  Research shows that it can even outperform CDL when it comes to ranking predictions. It outputs a confidence value $C^{-1}_{uij}$ of how much a user *u* preferes item *i* over *j*.

 RNN also get my attention becase I see its usage in time-series because of variants such as LSTM. More interestingly, DRL is exciting because of a possibility to make recommendations based on on trial-and-error paradigm.
For sequential modeling,  RNN (interval memory)  and CNN (filters sliding along with time, i.e. kernel). Sequential signals are important for mining temporal dynamics of user behaviour and time evolution.

When it comes to activation functions, **ReLu should be one to go for!** Unlike in **sigmoid** where vanishing gradient may occur and its output is not zero centered, ReLu is always positive with the slope of 1. If the neurons die, one may use leaky ReLu which output may be even negative (but close to 0!). **Tanh** gives a zero centered output but again, vanishing gradient may arise as well.


Sat 14th Sep 2019
Continued reading Goodfellow et al. When it comes multiple hypothesis explaining known observations equally well, scholars invoke a principle of parsimony, known as Occam's razor, i.e. choose the "simplest" hypothesis. Vapnik-Chervonekis dimension or VC dimension is a mean of quantyfing model capacity, the capacity of a binary classifier. ML models can work but they are rarely used in practice when working with DL algorithms. Parametric models, i.e. linear regression, learn a function described by a parameter vector whose size is finite and fixed before any data is observed (i.e. fixed-length vector of weights). Nonparametric models (e.g. KNN) are not limited in this way.




Sun 15th Sep 2019
Started with PyTorch. Random tensors are very important in neural networks. Parameters of the neural networks typically are initialized with random weights (random tensors).
The gradient is a multi-variable generalization of the derivative. Since neural nets have typically large number of derivatives, the gradient term is used instead.




Tue 17th Sep 2019

Why activation function or ReLU? Because we need non-linearity! Multiple linear layers can be expressed as neural networks with one layer only.

```
# Calculate the first and second hidden layer
hidden_1 = torch.matmul(input_layer, weight_1)
hidden_2 = torch.matmul(hidden_1, weight_2)
```

```
print(torch.matmul(hidden_2_activated, weight_3))
#tensor([[0.2655, 0.1311, 3.8221, 3.0032]])

```

```
# Calculate weight_composed_1 and weight
weight_composed_1 = torch.matmul(weight_1, weight_2)
weight = torch.matmul(weight_composed_1, weight_3)
```
```
# Multiply input_layer with weight
print(torch.matmul(input_layer, weight))
#tensor([[0.2655, 0.1311, 3.8221, 3.0032]])

```

Neural networks don't need to have the same number of units in each layer.

```
# Instantiate ReLU activation function as relu
relu = nn.ReLU()

# Initialize weight_1 and weight_2 with random numbers
weight_1 = torch.rand(4, 6)
weight_2 = torch.rand(6, 2)

# Multiply input_layer with weight_1
hidden_1 = torch.matmul(input_layer, weight_1)

# Apply ReLU activation function over hidden_1 and multiply with weight_2
hidden_1_activated = relu(hidden_1)

```
print(torch.matmul(hidden_1_activated, weight_2))
# tensor([[0., 0]])

```



Wed: 18th Sep 2019
Why to use CNN? In fully connected NN, the model consider relationships between areas in pictures that have no relationship. For example, pixel in upper left corner and pixels in lower right corner. The arising question is: "do wee need to consider all the relations between the features in these two (and other unrelated) areas? Though, fully-connected neural networks with many units and weights are large and computationally inefficient. Additionally, nets with many parameters (weights) can overfit on the training set. There are two main ideas on mitigating these problems: i.) Units are connected with only a few units from the previous layer. ii.) Units share weights. CNN implements these ideas. Convolving means doing a dot product between the image and the filter. In analogy to the fully connected NN, filters: i.e. kernels, are actually the weights of the network. After finishing the procedure - convolution - we obtain an activation map or feature map with reduced size. Convolution layer contains multiple of convolution maps. The goal of CNN is to learn filters which give us good activation maps, i.e. good features.
Padding is adding 0s in the sides of the image. The goal of padding is to make size of the activation map matching the size of the image so every pixel in the original image is encoded.

